<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]  updated1: 2024-03-19 16:22:59"><meta property="og:type" content="article"><meta property="og:title" content="2+3.线性模型与全连接前馈神经网络"><meta property="og:url" content="https://amostian.github.io/posts/3050284380/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]  updated1: 2024-03-19 16:22:59"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240316230758952.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240317110949088.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240317145453732.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240317211949271.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240317212003378.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318094505252.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318101336268.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318212547601.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318204613936.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318205323324.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319124803900.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319124830766.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319124902919.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318222853913.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240318223339988.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319003424239.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319003441350.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319003458955.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240319011936498.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240330151233579.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20231006000634803.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20240320205108341.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20231007113640918.png"><meta property="og:image" content="https://amostian.github.io/posts/3050284380/image-20231007175031669.png"><meta property="article:published_time" content="2024-03-17T01:57:03.000Z"><meta property="article:modified_time" content="2024-03-29T16:44:42.000Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/3050284380/image-20240316230758952.png"><link rel="canonical" href="https://amostian.github.io/posts/3050284380/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>2+3.线性模型与全连接前馈神经网络 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">65</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">82</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">216</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/3050284380/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">2+3.线性模型与全连接前馈神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-03-17 09:57:03" itemprop="dateCreated datePublished" datetime="2024-03-17T09:57:03+08:00">2024-03-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-03-30 00:44:42" itemprop="dateModified" datetime="2024-03-30T00:44:42+08:00">2024-03-30</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">16.5k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>50 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><blockquote><p>updated1: 2024-03-19 16:22:59</p></blockquote><span id="more"></span><h1 id="2-线性模型"><a href="#2-线性模型" class="headerlink" title="2. 线性模型"></a>2. 线性模型</h1><p><strong>线性模型可以看做单层神经网络</strong></p><p><img src="/posts/3050284380/image-20240316230758952.png" alt="image-20240316230758952"></p><p><strong>torch中创建的向量为行向量，所以后续基于行向量进行原理推导</strong></p><p>对于 <strong>线性模型</strong>，$d$ 维行输入向量 $\mathbf{x}=[x^{(1)},x^{(2)},\cdots,x^{(d)}]\in \mathbb{R}^{1\times d}$ ，输出 $\hat{y}$ 是输入的加权和</p><script type="math/tex;mode=display">\hat{y}=\mathbf{x}\cdot\mathbf{w}+b=\mathbf{x}\mathbf{w}^T+b</script><p>$\mathbf{w}=[w_1,w_2,\cdots,w_d]\in \mathbb{R}^{1\times d}$</p><p><strong>衡量预测质量</strong>：比较真实值和预测值，假设 $y$ 是真实值，$\hat{y}$ 是预测值，可以使用平方损失衡量预测质量</p><script type="math/tex;mode=display">\ell(y,\hat{y})=\frac{1}{2}(y-\hat{y})^2</script><p>对于参数的取值，需要收集一些数据点来决定，称为<strong>训练数据</strong>，通常越多越好。假设有 $n$ 个样本，数据集为</p><script type="math/tex;mode=display">\mathbf{X}=\begin{bmatrix}\mathbf{x}_1\\\mathbf{x}_2\\\vdots\\\mathbf{x}_n\end{bmatrix}\in\R^{n\times d},\hat{\mathbf{y}}=\mathbf{X}\cdot\mathbf{w}+b=\begin{bmatrix}\mathbf{x}_1\cdot\mathbf{w}+b\\\mathbf{x}_2\cdot\mathbf{w}+b\\\vdots\\\mathbf{x}_n\cdot\mathbf{w}+b\end{bmatrix}=\begin{bmatrix}\mathbf{x}_1\mathbf{w}^T+b\\\mathbf{x}_2\mathbf{w}^T+b\\\vdots\\\mathbf{x}_n\mathbf{w}^T+b\end{bmatrix}=\begin{bmatrix}\hat{y}_1\\\hat{y}_2\\\vdots\\\hat{y}_n\end{bmatrix}\in \mathbb{R}^{n\times1}</script><p>定义损失函数：将模型对数据的预测结果与真实结果误差作为损失函数，希望这个损失越小越好</p><script type="math/tex;mode=display">\ell\left(\mathbf{X},\mathbf{y},\mathbf{w},b\right)=\frac{1}{2n}\sum\limits_{i=1}^n\left[y_i-\left(\mathbf{x}_i\cdot\mathbf{w}+b\right)\right]^2=\frac{1}{2n}\Vert \mathbf{y}-\mathbf{X}\cdot\mathbf{w}\Vert^2=\frac{1}{2n}\Vert \mathbf{y}-\mathbf{X}\mathbf{w}^T\Vert^2</script><p>所以目标函数为：最小化损失函数来学习参数</p><script type="math/tex;mode=display">w^*,b^*=arg \min\limits_{w,b}\ell\left(\mathbf{X},\mathbf{y},\mathbf{w},b\right)</script><p>使用训练数据进行 <strong>参数学习</strong> ：通过梯度下降法更新参数使损失函数的最小</p><script type="math/tex;mode=display">\mathbf{w}_{t+1}=\mathbf{w}_t-\eta\frac{\partial \ell}{\partial \mathbf{w}}\\
b_{t+1}=b_t-\eta\frac{\partial \ell}{\partial b}</script><hr><p>为简化过程，将偏差加入权重，$\mathbf{w}=[w_1,w_2,\cdots,w_d,b]$ ，对数据进行1扩充，$\mathbf{x}_i=[x_i^{(1)},x^{(2)}_i,\cdots,x^{(d)}_i,1]$</p><script type="math/tex;mode=display">\hat{y}=(\mathbf{x}_i,\mathbf{w})=\mathbf{x}_i\mathbf{w}^T=[x^{(1)}_i,x^{(2)}_i,\cdots,x^{(d)}_i,1]\begin{bmatrix}w_1\\w_2\\\vdots\\w_d\\b
\end{bmatrix}=w_1x^{(1)}_i+w_2x^{(2)}_i+\cdots+w_dx^{(d)}_i+b</script><p>则有，</p><script type="math/tex;mode=display">\mathbf{X}=\left[
\begin{array}{cc}
\mathbf{x}_1&1\\\mathbf{x}_2&1\\\vdots&\vdots\\\mathbf{x}_n&1
\end{array}
\right]=\begin{bmatrix}x_1^{(1)}&x_1^{(2)}&\cdots&x_1^{(d)}&1\\x_2^{(1)}&x_2^{(2)}&\cdots&x_2^{(d)}&1\\\vdots&\vdots&\ddots&\vdots&\vdots\\x_n^{(1)}&x_n^{(2)}&\cdots&x_n^{(d)}&1\end{bmatrix}\in\R^{n\times (d+1)}\quad,\mathbf{w}^T=\begin{bmatrix}w_1\\w_2\\\vdots\\w_d\\b\end{bmatrix}\in \mathbb{R}^{(d+1)\times 1}</script><p>损失函数为</p><script type="math/tex;mode=display">\ell\left(\mathbf{X},\mathbf{y},\mathbf{w}\right)=\frac{1}{2n}\sum\limits_{i=1}^n\left[y_i-\left(\mathbf{x}_i\mathbf{w}^T+b\right)\right]^2=\frac{1}{2n}\Vert \mathbf{y}-\mathbf{X}\mathbf{w}^T\Vert^2</script><p>对参数求梯度</p><script type="math/tex;mode=display">\frac{\partial}{\partial \mathbf{w}}\ell(\mathbf{X},\mathbf{y},\mathbf{w})=\frac{1}{n}\left(\mathbf{y}-\mathbf{X}\mathbf{w}^T\right)^T\mathbf{X}</script><p>由于该函数是凸函数，最优解为</p><script type="math/tex;mode=display">\frac{\partial}{\partial \mathbf{w}}\ell(\mathbf{X},\mathbf{y},\mathbf{w})=0\\
\iff \frac{1}{n}\left(\mathbf{y}-\mathbf{X}\mathbf{w}^T\right)^T\mathbf{X}=0\\
\iff \left(\mathbf{y}^T-\mathbf{w}\mathbf{X}^T\right)\mathbf{X}=0\\
\iff\mathbf{y}^T\mathbf{X}=\mathbf{w}\mathbf{X}^T\mathbf{X}\\
\mathbf{w}^{*}=\left(\mathbf{y}^T\mathbf{X}\right)\cdot\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\\
\mathbf{w}^{*}=\left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\right]^T\cdot \left(\mathbf{y}^T\mathbf{X}\right)^T\\
\mathbf{w}^*=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}\mathbf{y}</script><h2 id="2-1-线性回归实现"><a href="#2-1-线性回归实现" class="headerlink" title="2.1 线性回归实现"></a>2.1 线性回归实现</h2><h3 id="2-1-1-生成数据"><a href="#2-1-1-生成数据" class="headerlink" title="2.1.1 生成数据"></a>2.1.1 生成数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline <span class="comment">#在jupyter notebook中显示图像</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#功能：根据真实的参数，基于线性模型生成带噪音的随机训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#生成训练集矩阵，n*|w|，从正态分布N(0,1)中抽取输入值</span></span><br><span class="line">    <span class="comment">#	n行表示n个数据，|w| 列表示一个数据有|w|个特征</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment"># y是n维真实输出向量</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#在真实输出上加噪音，噪音服从N(0,0.01)</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="comment">#y的行数无所谓，列数指定</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"><span class="comment"># features中的每一行都包含一个二维数据样本</span></span><br><span class="line"><span class="comment"># labels中的每一行都包含一维标签值</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>],<span class="string">&#x27;\nlabel:&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># features: tensor([0.2151, 0.0915])</span></span><br><span class="line"><span class="comment"># label: tensor([4.3277])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成第二个特征features[:, 1]和labels的散点图</span></span><br><span class="line">d2l.set_figsize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(x,y) xxx.detach()，从计算图中分离出来才能转为numpy的ndarray</span></span><br><span class="line">d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="/posts/3050284380/image-20240317110949088.png" alt="image-20240317110949088"></p><h3 id="2-1-2-生成批量"><a href="#2-1-2-生成批量" class="headerlink" title="2.1.2 生成批量"></a>2.1.2 生成批量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#输入：批量大小、整体数据集，</span></span><br><span class="line"><span class="comment">#输出：指定大小的批量集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    <span class="comment">#数据数为特征数</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    <span class="comment">#为输入数据的索引创建列表</span></span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 对索引列表随机排序</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="comment">#功能：数据每隔一个批量封装为可迭代对象的一项，这些批量项被封装为一个可迭代对象作为函数返回值</span></span><br><span class="line">    <span class="comment"># range(start,end,step)从start到end间隔step-1个数取一个数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="comment">#从乱序索引列表中中逐次选择批量</span></span><br><span class="line">        batch_indices = torch.tensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="comment"># yield会将返回值封装为可迭代对象的一个item，item有两个,可用X,y解构可得feature[]与label</span></span><br><span class="line">        <span class="comment">#	yield一次，主调函数接收一次</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line"><span class="comment">#yield一次，主函数就会接收一次，即 data_iter中for循环的 i 会被记录(0,10,20,...990)</span></span><br><span class="line"><span class="comment">#返回100个批量，每个批量10个数据</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/posts/3050284380/image-20240317145453732.png" alt="image-20240317145453732"></p><h3 id="2-1-3-初始化模型"><a href="#2-1-3-初始化模型" class="headerlink" title="2.1.3 初始化模型"></a>2.1.3 初始化模型</h3><h4 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure><h4 id="随机初始化参数"><a href="#随机初始化参数" class="headerlink" title="随机初始化参数"></a>随机初始化参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机初始化参数</span></span><br><span class="line"><span class="comment"># 从 N(0,0.01)的正态分布中采样，2*1的列向量，反向传播时，该tensor就会自动求导</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>更新这些参数，直到这些参数足够拟合训练数据。每次更新都需要计算损失函数关于模型参数的梯度，<code>requires_grad=True</code> 对这个参数引入自动微分计算梯度</p><h3 id="2-1-4-定义损失函数"><a href="#2-1-4-定义损失函数" class="headerlink" title="2.1.4 定义损失函数"></a>2.1.4 定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="2-1-5-定义优化算法"><a href="#2-1-5-定义优化算法" class="headerlink" title="2.1.5 定义优化算法"></a>2.1.5 定义优化算法</h3><p>若损失函数计算的是整个样本集上的损失</p><script type="math/tex;mode=display">\ell\left(\mathbf{X},\mathbf{y},\mathbf{w}\right)=\frac{1}{2n}\sum\limits_{i=1}^n\left[y_i-\left(\mathbf{x}_i\mathbf{w}^T+b\right)\right]^2=\frac{1}{2n}\Vert \mathbf{y}-\mathbf{X}\mathbf{w}^T\Vert^2</script><p>其梯度为</p><script type="math/tex;mode=display">\frac{\partial}{\partial \mathbf{w}}\ell(\mathbf{X},\mathbf{y},\mathbf{w})=\frac{1}{n}\left(\mathbf{y}-\mathbf{X}\mathbf{w}^T\right)^T\mathbf{X}</script><p>若采用小批量随机梯度下降法，用 $\vert \mathcal{B}\vert$ 个样本的小批量样本集 $\mathcal{B}$</p><script type="math/tex;mode=display">\frac{\partial}{\partial \mathbf{w}}\ell(\mathbf{X},\mathbf{y},\mathbf{w})=\frac{1}{\vert \mathcal{B}\vert}\left(\mathbf{y}-\mathbf{X}\mathbf{w}^T\right)^T\mathbf{X}</script><p>沿着负梯度方向更新参数，每步更新的步长由学习率 <code>lr</code> 决定</p><script type="math/tex;mode=display">\mathbf{w}_t\leftarrow\mathbf{w}_{t-1}-\frac{\eta}{\vert \mathcal{B}\vert}\sum\limits_{i\in\mathbf{I}_{\mathcal{B}}}\frac{\partial}{\partial \mathbf{w}}\ell(\mathbf{x}_i,\mathbf{y}_i,\mathbf{w}_t)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#param是由参数迭代而来，param默认会继承参数的自动微分，</span></span><br><span class="line">    <span class="comment">#	而更新后的param不需要计算其梯度，所以torch.no_grad()</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure><ul><li><p>若一个节点（叶子变量：自己创建的tensor）requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True</p></li><li><p>当requires_grad设置为False时，反向传播时就不会自动求导了，因此大大节约了显存或内存</p><p><code>with torch.no_grad</code> 起到了截断自动微分的作用，对不需要计算微分的部分进行截断</p></li></ul><h3 id="2-1-6-训练过程"><a href="#2-1-6-训练过程" class="headerlink" title="2.1.6 训练过程"></a>2.1.6 训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;1.生成数据&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 功能：根据真实的参数，基于线性模型生成带噪音的随机训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#生成训练集矩阵，n*|w|，从正态分布N(0,1)中抽取输入值</span></span><br><span class="line">    <span class="comment">#	n行表示n个数据，|w| 列表示一个数据有|w|个特征</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment"># y是n维真实输出向量</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#在真实输出上加噪音，噪音服从N(0,0.01)</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="comment">#y的行数无所谓，列数指定</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"><span class="comment"># features中的每一行都包含一个二维数据样本</span></span><br><span class="line"><span class="comment"># labels中的每一行都包含一维标签值</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2. 模块定义&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2.1 小批量生成模块&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment">#输入：批量大小、整体数据集，</span></span><br><span class="line"><span class="comment">#输出：指定大小的批量集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    <span class="comment">#数据数为特征数</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    <span class="comment">#为输入数据的索引创建列表</span></span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 对索引列表随机排序</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="comment">#功能：数据每隔一个批量封装为可迭代对象的一项，这些批量项被封装为一个可迭代对象作为函数返回值</span></span><br><span class="line">    <span class="comment"># range(start,end,step)从start到end间隔step-1个数取一个数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="comment">#从乱序索引列表中中逐次选择批量</span></span><br><span class="line">        batch_indices = torch.tensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="comment"># yield会将返回值封装为可迭代对象的一个item，item有两个,可用X,y解构可得feature[]与label</span></span><br><span class="line">        <span class="comment">#	yield一次，主调函数接收一次</span></span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2.2 模型定义:线性回归模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2.3 损失函数定义:均方损失&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2.4 随机梯度下降&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#param是由参数迭代而来，param默认会继承参数的自动微分，</span></span><br><span class="line">    <span class="comment">#而更新后的param不需要计算其梯度，所以torch.no_grad()</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;3. 初始化超参数&#x27;&#x27;&#x27;</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;4. 开始训练&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#随机初始化参数</span></span><br><span class="line"><span class="comment"># 从 N(0,0.01)的正态分布中采样，2*1的列向量，反向传播时，该tensor就会自动求导</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练集上进行三轮训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):  </span><br><span class="line">        <span class="comment"># hat_y是2维列向量。l为小批量平方损失向量</span></span><br><span class="line">        l = squared_loss(linreg(X, w, b), y)</span><br><span class="line">        <span class="comment"># 对同一批量的平方损失求和，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment">#计算本轮迭代后的损失</span></span><br><span class="line">        train_l = squared_loss(linreg(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;5. 对比真实参数&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="2-2-基于pytorch实现线性回归"><a href="#2-2-基于pytorch实现线性回归" class="headerlink" title="2.2 基于pytorch实现线性回归"></a>2.2 基于pytorch实现线性回归</h2><h3 id="2-2-1-生成数据"><a href="#2-2-1-生成数据" class="headerlink" title="2.2.1 生成数据"></a>2.2.1 生成数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch.utils 是torch中的一个数据处理包</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-2-数据迭代器-理解"><a href="#2-2-2-数据迭代器-理解" class="headerlink" title="2.2.2 数据迭代器(理解)"></a>2.2.2 数据迭代器(理解)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#将torch张量类型的数据集转换为torch中的张量数据集类型TensorDataset</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="comment">#shuffle：打乱数据集</span></span><br><span class="line">    <span class="comment"># 按批量大小从数据集中返回小批量数据，并将这些数据封装为Pytorch迭代器</span></span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># data_iter是Pytorch迭代器对象</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure><p>在 <code>data.DataLoader()</code> 创建的迭代器对象中，封装了数据元组</p><ul><li><p>将整个数据集按批量大小划分为多个批量，并返回划分后的 <strong>可迭代对象</strong></p></li><li><p><strong>可迭代对象可视为一个列表，列表的每一项封装了一个批量(元组)，对一个项操作，相当于对一个批量的操作</strong></p></li><li><strong>遍历可迭代对象，相当于逐批量获取数据</strong></li></ul><p>为查看迭代器内容，需要使用 <code>iter()</code> 构造器将 Pytorch迭代器转换为 Python迭代器，然后使用 <code>next(itr)</code> 获取第一项</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一批量的数据:\n&quot;</span>,<span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter)))</span><br></pre></td></tr></table></figure><h3 id="2-2-3-初始化模型"><a href="#2-2-3-初始化模型" class="headerlink" title="2.2.3 初始化模型"></a>2.2.3 初始化模型</h3><h4 id="模型定义-1"><a href="#模型定义-1" class="headerlink" title="模型定义"></a>模型定义</h4><p>对于标准深度学习模型，我们可以使用框架预定义好的层。每一层都可以理解为带有输入和输出的模型</p><p>使用框架之后，我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。</p><ul><li>这种情况类似于为自己的博客从零开始编写网页。也能使用博客框架，博客框架环境配置好后，只需要关注博客内容即可</li></ul><p>在Pytorch中，定义一个模型变量 <code>net</code> ，<code>net</code> 是一个 <code>Sequential</code> 类</p><ul><li><code>Sequential</code> 将多个层串联在一起。当给定输入数据时，模型变量将数据传入到第一层，第一层的输出作为第二层的输入，以此类推</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#nn.Linear(输入向量维度,输出向量维度)</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h4 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#线性模型可以视为单层神经网络，在框架中只有第一层</span></span><br><span class="line"><span class="comment">#net[0].weight.data指定线性层的权重；</span></span><br><span class="line"><span class="comment">#	.normal_(means,var)，从均值为mean方差为var的正态分布中挑选初始值</span></span><br><span class="line"><span class="comment">#net[0].bias.data指定线性层的偏置</span></span><br><span class="line"><span class="comment">#	.fill_(x)，用x填充</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><p><code>Sequential</code> 实例的 <code>parameters()</code> 函数可以获取当前层的参数，返回的是参数的迭代对象</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.0059, -0.0101]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.], requires_grad=True)</span><br></pre></td></tr></table></figure><h3 id="2-2-4-损失函数"><a href="#2-2-4-损失函数" class="headerlink" title="2.2.4 损失函数"></a>2.2.4 损失函数</h3><p>在nn模块中，定义了各种损失函数</p><p>均方损失，<code>MSELoss</code> 类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失实例已经包含批量大小信息</span></span><br><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><h3 id="2-2-5-定义优化函数"><a href="#2-2-5-定义优化函数" class="headerlink" title="2.2.5 定义优化函数"></a>2.2.5 定义优化函数</h3><p>PyTorch在优化模块 <code>optim</code> 中实现了随机梯度下降算法(SGD)的许多变种</p><p>使用SGD实例时，要指定优化的参数</p><ul><li>小批量SGD只需要指定学习率</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>) </span><br></pre></td></tr></table></figure><h3 id="2-2-6-训练过程"><a href="#2-2-6-训练过程" class="headerlink" title="2.2.6 训练过程"></a>2.2.6 训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch.utils 是torch中的一个数据处理包</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#生成训练集矩阵，n*|w|，从正态分布N(0,1)中抽取输入值</span></span><br><span class="line">    <span class="comment">#	n行表示n个数据，|w| 列表示一个数据有|w|个特征</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment"># y是n维真实输出向量</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    <span class="comment">#在真实输出上加噪音，噪音服从N(0,0.01)</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="comment">#y的行数无所谓，列数指定</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#将torch张量类型的数据集转换为torch中的张量数据集类型TensorDataset</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="comment">#shuffle：打乱数据集</span></span><br><span class="line">    <span class="comment"># 按批量大小从数据集中返回小批量数据，并将这些数据封装为Pytorch迭代器</span></span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># data_iter是Pytorch迭代器对象</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#nn.Linear(输入向量维度,输出向量维度)</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment">#定义参数损失函数的优化算法</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>) </span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment">#对每个输入计算平方损失函数</span></span><br><span class="line">        <span class="comment"># 	l中已经包含了批量大小信息</span></span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        <span class="comment">#对参数的梯度置0</span></span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        <span class="comment">#对损失函数反向传播计算梯度并求和</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#参数更新一次</span></span><br><span class="line">        trainer.step()</span><br><span class="line">    <span class="comment">#计算本轮迭代更新参数后的误差</span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="2-3-softmax回归"><a href="#2-3-softmax回归" class="headerlink" title="2.3 softmax回归"></a>2.3 softmax回归</h2><p>回归任务：估计连续值</p><p>分类任务：预测一个离散的类别</p><ul><li>手写数字识别-10类</li><li>ImageNet：1000000张图片的，1000类自然物体分类</li><li>Kaggle上的分类问题：<ul><li>人类蛋白质显微镜图片28类</li><li>恶意软件分类9类</li><li>WIkipedia评论分为7类</li></ul></li></ul><p>回归任务到分类任务</p><div class="table-container"><table><thead><tr><th>回归任务</th><th>分类任务</th></tr></thead><tbody><tr><td>单连续数值输出</td><td>多离散输出</td></tr><tr><td>自然区间 $\mathbb{R}$</td><td>输出 $p(i)$ 是预测为第 $i$ 类的置信度</td></tr><tr><td>与真实值的差作为损失</td><td></td></tr><tr><td><img src="/posts/3050284380/image-20240317211949271.png" alt="image-20240317211949271"></td><td><img src="/posts/3050284380/image-20240317212003378.png" alt="image-20240317212003378"></td></tr></tbody></table></div><p>对类别编码，若有 $K$ 个类别，使用one-hot向量对真实样本 $(\mathbf{x}_i,y_i)$ 进行编码 $\mathbf{y}_i=[y_i^{(1)},y_i^{(2)},\cdots,y_i^{(K)}]$，令 $\kappa$ 为 $\mathbf{x}_i$ 的真实类别，则 $y_i^{(k)}=\begin{cases}1&amp;,k=\kappa\\0&amp;,k\neq \kappa\end{cases},k=1,\cdots,K$ ，样本 $\mathbf{x}_i$ 的真实类别 $y_i^{(k)}$ 服从一个分布 $p$</p><p>$\hat{y}_i^{(k)}=q(\hat{y}_i=k\vert \mathbf{x}_i)=softmax(o_i^{(k)})=\frac{e^{o_i^{(k)}}}{\sum\limits_{k=1}^Ke^{o_i^{(k)}}}=\frac{e^{\mathbf{x}_i\mathbf{w}_k^T}}{\sum\limits_{k=1}^Ke^{\mathbf{x}_i\mathbf{w}_k^T}}$ 表示将样本 $\mathbf{x}_i$ 预测为 $k$ 类的置信度，对样本 $\mathbf{x}_i$ 的类别预测置信度服从分布 $q$</p><ul><li>$o_i^{(k)}=\mathbf{w}^T_k\mathbf{x}_i$ 为经过线性模型 $\mathbf{w}_k^T\mathbf{x}$ 后 $\mathbf{x}_i$ 为第 $k$ 个类别的置信度净输出。</li><li>$\mathbf{o}_i=\left[o_i^{(1)},o_i^{(2)},\cdots,o_i^{(K)}\right]$ 为样本 $\mathbf{x}_i$ 经过线性模型后的置信度净输出向量</li><li>$\hat{\mathbf{y}}_i=softmax(\mathbf{o}_i)=\left[softmax\left(o_i^{(1)}\right),softmax\left(o_i^{(2)}\right),\cdots,softmax\left(o_i^{(k)}\right)\right]=\left[\hat{y}_i^{(1)},\hat{y}_i^{(2)},\cdots,\hat{y}_i^{(K)}\right]$</li></ul><p>所以目标函数为</p><script type="math/tex;mode=display">\hat{y}_i=\arg\max\limits_{k}\hat{y}_i^{(k)}</script><ul><li>目标是预测为正确类别的置信度远远大于其他类别<script type="math/tex;mode=display">\hat{y}_i^{(\kappa)}\gg\hat{y}_i^{(k)},k\neq \kappa\iff \hat{y}_i^{(\kappa)}-\hat{y}_i^{(k)}\ge \Delta(\kappa,k)</script></li></ul><p>通过交叉熵可以比较真实分布与预测分布的差异</p><ul><li>$H(p,q)=-\sum_k p_k\log q_k\ge 0$ 为交叉熵，当 $p_k=q_k$ 时，即两个分布相同时，交叉熵有最小值0</li></ul><p>将其作为损失函数</p><script type="math/tex;mode=display">\begin{aligned}
\ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)&=-\sum_k^Ky_i^{(k)}\log \hat{y}_i^{(k)}\\
&\xlongequal{y_i^{(k)}=\begin{cases}1&,k=\kappa\\0&,k\neq \kappa\end{cases}}-\log \hat{y}_i^{(\kappa)}\\
&=-\log\left(\frac{e^{\mathbf{x}_i\mathbf{w}_{\kappa}^T}}{\sum\limits_{k=1}^Ke^{\mathbf{x}_i\mathbf{w}_k^T}}\right)=\log\sum\limits_{k=1}^Ke^{\mathbf{x}_i\mathbf{w}_k^T}-\mathbf{x}_i\mathbf{w}_{\kappa}^T
\end{aligned}</script><p>对参数 $\mathbf{w}_{\kappa}$ 求梯度</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial }{\partial \mathbf{w}_{\kappa}}\ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)&=\frac{\partial}{\partial \mathbf{w}_\kappa}\left[\log\left(\cdots+e^{\mathbf{x}_i\mathbf{w}_{\kappa}^T}+\cdots\right)-\mathbf{x}_i\mathbf{w}_{\kappa}^T\right]\\
&=\frac{e^{\mathbf{x}_i\mathbf{w}_{\kappa}^T}\mathbf{x}_i}{\sum\limits_{k=1}^Ke^{\mathbf{x}_i\mathbf{w}_k^T}}-\mathbf{x}_i\\
&=\left(\hat{y}_i^{(\kappa)}-y_i^{(\kappa)}\right)\mathbf{x}_i
\end{aligned}</script><h3 id="分类数据集"><a href="#分类数据集" class="headerlink" title="分类数据集"></a>分类数据集</h3><p>MNIST数据集 (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90">LeCun <em>et al.</em>, 1998</a>) 是图像分类中广泛使用的数据集之一（手写数字识别数据集），但作为基准数据集过于简单</p><p>使用类似但更复杂的Fashion-MNIST数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># Pytorch对计算机视觉的实现库</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="comment"># 对数据处理工具库</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="comment"># 视觉库中对数据操作的模块</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h4 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h4><p>通过torchvision内置函数将数据集下载并读取到内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过ToTensor实例将图像数据从PIL类型转换成pytorch类型，32位浮点数格式，</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素的数值均在0～1之间</span></span><br><span class="line">trans = transforms.ToTensor()</span><br><span class="line"><span class="comment"># train=True		表示训练数据，train=False表示测试数据集</span></span><br><span class="line"><span class="comment"># transform=trans	表示下载后转换为tensor，不转换则只是图片</span></span><br><span class="line"><span class="comment"># download=True 	默认从网络下载</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;../data&quot;</span>, </span><br><span class="line">    train=<span class="literal">True</span>, </span><br><span class="line">    transform=trans, </span><br><span class="line">    download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/posts/3050284380/image-20240318094505252.png" alt="image-20240318094505252"></p><ul><li>整个下载包括下载、移动、解压三步</li></ul><h5 id="查看数据集"><a href="#查看数据集" class="headerlink" title="查看数据集"></a>查看数据集</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集中的图片数量</span></span><br><span class="line"><span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test)</span><br><span class="line">(<span class="number">60000</span>, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mnist中，每个样本用tuple表示，tuple[0]为输入，tuple[1]为其标签</span></span><br><span class="line"><span class="comment"># 查看第1个样本的输入张量x_1，其输出y_1为类别编号</span></span><br><span class="line"><span class="built_in">print</span>(mnist_train[<span class="number">0</span>][<span class="number">0</span>],mnist_train[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"><span class="comment">#张量类型的图片通道为1，宽高都是28，(c,h,w)=(1,28,28)</span></span><br><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h5 id="可视化数据集"><a href="#可视化数据集" class="headerlink" title="可视化数据集"></a>可视化数据集</h5><p>批量查看图片及其标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标签值转为标签文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="comment">#传入的labels是一个tensor列表，for i in labels中的i为每个样本的真实标签值，</span></span><br><span class="line">    <span class="comment">#[text_labels]会将这些标签文本转为列表</span></span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line">    <span class="comment">#相当于</span></span><br><span class="line">    <span class="comment">#	a = []</span></span><br><span class="line">    <span class="comment">#	for i in labels:</span></span><br><span class="line">    <span class="comment">#	    a[i]=text_labels[int(i)]</span></span><br><span class="line">    <span class="comment">#	return a</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#imgs为图片集，num_rows,num_cols为图片列表行列数，titles传入imgs相应的标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><p>读取两个样本，显示二者图像列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train, batch_size=<span class="number">2</span>)))</span><br><span class="line">show_images(X.reshape(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="number">1</span>, <span class="number">2</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure><p><img src="/posts/3050284380/image-20240318101336268.png" alt="image-20240318101336268"></p><h4 id="实现批量读取"><a href="#实现批量读取" class="headerlink" title="实现批量读取"></a>实现批量读取</h4><p>在每次迭代中，数据加载器每次都会[<strong>读取一小批量数据，大小为<code>batch_size</code></strong>]</p><p>通过内置数据迭代器，可以随机打乱所有样本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据加载器可启动进程数重载</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(</span><br><span class="line">    mnist_train, </span><br><span class="line">    batch_size, </span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure><p>可记录加载一个批量的时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="封装数据读取"><a href="#封装数据读取" class="headerlink" title="封装数据读取"></a>封装数据读取</h4><p>从视觉库的Fashion-MNIST数据集获取训练数据集和测试数据集。</p><p>返回：训练集和验证集的数据迭代器</p><p>输入：批量大小batch_size；resize图像大小调整</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 对图片的处理函数列表</span></span><br><span class="line">    <span class="comment">#	transforms.ToTensor()：将PIL类型的图片转换为张亮</span></span><br><span class="line">    <span class="comment">#	transforms.Resize(resize)：调整图片大小，调整为resize</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    <span class="comment"># 将多个图像变换操作组成一个变换操作序列</span></span><br><span class="line">    <span class="comment">#	输入：transforms类型的操作列表</span></span><br><span class="line">    <span class="comment">#	输出：返回组合后的操作序列</span></span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 返回数据集迭代器</span></span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure><p>通过resize参数调整图片大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64</span><br></pre></td></tr></table></figure><h3 id="2-3-1-Softmax实现"><a href="#2-3-1-Softmax实现" class="headerlink" title="2.3.1 Softmax实现"></a>2.3.1 Softmax实现</h3><h4 id="获取数据集批量"><a href="#获取数据集批量" class="headerlink" title="获取数据集批量"></a>获取数据集批量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><p>softmax输入需要是向量，所以需要把所有图像展平，视为一个向量，$1\times 28\times 28=784$ 的列向量 $img_{[1\times28\times 28]}\rightarrow\mathbf{x}_{[784\times 1]}$</p><ul><li>展平操作会损失很多空间信息，所以卷积神经网络会弥补这一缺陷</li></ul><p>一张图像可能得类别有10类，所以输出10维的列向量 $\mathbf{y}_{[10\times 1]}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="模型定义-2"><a href="#模型定义-2" class="headerlink" title="模型定义"></a>模型定义</h4><p>输入，原始数据矩阵：每个样本展平为 $\mathbf{x}_i\in\R^{1\times d}$ 的行向量，批量样本矩阵为 $\mathbf{X}=\begin{bmatrix}\mathbf{x}_1\\\mathbf{x}_2\\\vdots\\\mathbf{x}_n\end{bmatrix}\in \mathbb{R}^{n\times d}$</p><ul><li><p>本例中 $\mathbf{x}_i\in\R^{1\times 784}$ ，批量数据表示为 $\mathbf{X}=\begin{bmatrix}\mathbf{x}_1\\\vdots\\\mathbf{x}_{256}\end{bmatrix}\in\R^{256\times 784}$</p></li><li></li></ul><p>参数 $\mathbf{w}_k\in \mathbb{R}^{1\times d},k\in [1,K]$ ，参数矩阵 $\mathbf{W}=\begin{bmatrix}\mathbf{w}_1\\\mathbf{w}_2\\\vdots\\\mathbf{w}_{K}\end{bmatrix}\in \mathbb{R}^{K\times d}$ ，偏置 $\mathbf{b}\in \mathbb{R}^{1\times K}$ 行向量</p><ul><li>本例中，$\mathbf{w}_k\in \mathbb{R}^{1\times 784},k\in [1,10]$ ，参数矩阵 $\mathbf{W}=\begin{bmatrix}\mathbf{w}_1\\\mathbf{w}_2\\\vdots\\\mathbf{w}_{10}\end{bmatrix}\in \mathbb{R}^{10\times 784}$ ，偏置 $\mathbf{b}\in \mathbb{R}^{1\times 10}$</li></ul><p>净输出，净输出矩阵：每个样本的净输出向量为 $\mathbf{o}_i\in \mathbb{R}^{1\times K}$ 的行向量，批量样本净输出矩阵表示为 $\mathbf{O}=\begin{bmatrix}\mathbf{o}_1\\\mathbf{o}_2\\\vdots\\\mathbf{o}_{n}\end{bmatrix}\in \mathbb{R}^{n\times K}$</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{O}_{n\times K}&=
\mathbf{X}_{n\times d}\mathbf{W}^T_{d\times K}+\mathbf{b}_{1\times K}=
\begin{bmatrix}\mathbf{x}_1\\\mathbf{x}_2\\\vdots\\\mathbf{x}_n\end{bmatrix}[\mathbf{w}_1^T,\mathbf{w}_2^T,\cdots,\mathbf{w_K^T}]+\mathbf{b}\\
&=\begin{bmatrix}
\mathbf{x}_1\mathbf{w}_1^T&\mathbf{x}_1\mathbf{w}_2^T&\cdots&\mathbf{x}_1\mathbf{w}_K^T\\
\mathbf{x}_2\mathbf{w}_1^T&\mathbf{x}_2\mathbf{w}_2^T&\cdots&\mathbf{x}_2\mathbf{w}_K^T\\
\vdots&\vdots&\ddots&\vdots\\
\mathbf{x}_n\mathbf{w}_1^T&\mathbf{x}_n\mathbf{w}_2^T&\cdots&\mathbf{x}_n\mathbf{w}_K^T
\end{bmatrix}+[b_1,b_2,\cdots,b_K]=\begin{bmatrix}
\mathbf{x}_1\mathbf{w}_1^T+b_1&\mathbf{x}_1\mathbf{w}_2^T+b_2&\cdots&\mathbf{x}_1\mathbf{w}_K^T+b_K\\
\mathbf{x}_2\mathbf{w}_1^T+b_1&\mathbf{x}_2\mathbf{w}_2^T+b_2&\cdots&\mathbf{x}_2\mathbf{w}_K^T+b_K\\
\vdots&\vdots&\ddots&\vdots\\
\mathbf{x}_n\mathbf{w}_1^T+b_1&\mathbf{x}_n\mathbf{w}_2^T+b_2&\cdots&\mathbf{x}_n\mathbf{w}_K^T+b_K
\end{bmatrix}\\\\
&=\begin{bmatrix}
o_1^{(1)}&o_1^{(2)}&\cdots&o_1^{(K)}\\
o_2^{(1)}&o_2^{(2)}&\cdots&o_2^{(K)}\\
\vdots&\vdots&\ddots&\vdots\\
o_n^{(1)}&o_n^{(2)}&\cdots&o_n^{(K)}
\end{bmatrix}=\begin{bmatrix}
\mathbf{o}_1\\
\mathbf{o}_2\\
\vdots\\
\mathbf{o}_n\\
\end{bmatrix}
\in \mathbb{R}^{n\times K}
\end{aligned}</script><ul><li>本例中，净输出 $\mathbf{o}_i\in \mathbb{R}^{1\times 10}$ ，批量净输出矩阵表示为 $\mathbf{O}=\begin{bmatrix}\mathbf{o}_1\\\mathbf{o}_2\\\vdots\\\mathbf{o}_{256}\end{bmatrix}\in \mathbb{R}^{256\times 10}$<script type="math/tex;mode=display">\begin{aligned}
\mathbf{O}=\mathbf{X}\mathbf{W}^T+\mathbf{b}&=\begin{bmatrix}\mathbf{x}_1\\\mathbf{x}_2\\\vdots\\\mathbf{x}_{256}
\end{bmatrix}\times [\mathbf{w}_1^T,\mathbf{w}_2^T,\cdots,\mathbf{w}_{10}^T]+[b_1,b_2,\cdots,b_{10}]\\
&=\begin{bmatrix}
\mathbf{x}_1\mathbf{w}^T_1+b_1&\mathbf{x}_1\mathbf{w}_2^T+b_2&\cdots&\mathbf{x}_{1}\mathbf{w}_{10}^T+b_{10}\\
\mathbf{x}_2\mathbf{w}_1^T+b_1&\mathbf{x}_2\mathbf{w}_2^T+b_2&\cdots&\mathbf{x}_{2}\mathbf{w}_{10}^T+b_{10}\\
\vdots&\vdots&\ddots&\vdots\\
\mathbf{x}_{256}\mathbf{w}_1^T+b_1&\mathbf{x}_{256}\mathbf{w}_2^T+b_2&\cdots&\mathbf{x}_{256}\mathbf{w}_{10}^T+b_{10}
\end{bmatrix}\\\\
&\xlongequal{o_i^{(k)}=\mathbf{w}_{k}\cdot\mathbf{x}_i=\mathbf{x}_{i}\mathbf{w}_k^T}\begin{bmatrix}
o_1^{(1)}&o_1^{(2)}&\cdots&o_1^{(10)}\\
o_2^{(1)}&o_2^{(2)}&\cdots&o_2^{(10)}\\
\vdots&\vdots&\ddots&\vdots\\
o_{256}^{(1)}&o_{256}^{(2)}&\cdots&o_{256}^{(10)}\\
\end{bmatrix}=\begin{bmatrix}\mathbf{o}_1\\\mathbf{o}_2\\\vdots\\\mathbf{o}_{256}
\end{bmatrix}=\mathbf{O}\in \mathbb{R}^{256\times 10}
\end{aligned}</script></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment">#展平操作，将256*(1,28,28)的数据展平为 256*784的张量</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape(-<span class="number">1</span>,(W.shape[<span class="number">0</span>])), W) + b)</span><br></pre></td></tr></table></figure><p>所以一次softmax有三个操作</p><ol><li>对每个项求幂 <code>exp()</code></li><li>对每一行求和(在一个批量中，一行为一个样本)</li><li>将每一行除以其规范化常数，确保结果的和为1</li></ol><p>输入：净输出矩阵</p><p>输出：softmax后的置信度矩阵 / 归一化后的置信度矩阵 $\hat{\mathbf{Y}}=\begin{bmatrix}\hat{\mathbf{y}}_1\\\hat{\mathbf{y}}_2\\\vdots\\\hat{\mathbf{y}}_{n}\end{bmatrix}=\begin{bmatrix}\hat{\mathbf{y}}_1\\\hat{\mathbf{y}}_2\\\vdots\\\hat{\mathbf{y}}_{256}\end{bmatrix}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">O</span>):</span><br><span class="line">    O_exp = torch.exp(O)</span><br><span class="line">    partition = O_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> O_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure><script type="math/tex;mode=display">e^{\mathbf{O}}=\begin{bmatrix}
e^{o_1^{(1)}}&e^{o_1^{(2)}}&\cdots&e^{o_1^{(K)}}\\
e^{o_2^{(1)}}&e^{o_2^{(2)}}&\cdots&e^{o_2^{(K)}}\\
\vdots&\vdots&\ddots&\vdots\\
e^{o_{n}^{(1)}}&e^{o_{n}^{(2)}}&\cdots&e^{o_{n}^{(K)}}\\
\end{bmatrix}\quad,\mathbf{partition}=\begin{bmatrix}
\sum\limits_{k=1}^K e^{o_1^{(k)}}\\
\sum\limits_{k=1}^K e^{o_2^{(k)}}\\
\vdots\\
\sum\limits_{k=1}^K e^{o_n^{(k)}}\\
\end{bmatrix}\\
\mathbf{\hat{Y}}=softmax\left(\hat{\mathbf{O}}\right)=\begin{bmatrix}
softmax\left(\hat{\mathbf{o}}_1\right)\\
softmax\left(\hat{\mathbf{o}}_2\right)\\
\vdots\\
softmax\left(\hat{\mathbf{o}}_n\right)
\end{bmatrix}=\begin{bmatrix}
\hat{\mathbf{y}}_1\\
\hat{\mathbf{y}}_2\\
\vdots\\
\hat{\mathbf{y}}_n
\end{bmatrix}
\\
\hat{\mathbf{y}}_i=softmax(\mathbf{o}_i)=\left[\frac{e^{o_i^{(1)}}}{\sum\limits_{k=1}^{K} e^{o_i^{(k)}}},\frac{e^{o_i^{(2)}}}{\sum\limits_{k=1}^{K} e^{o_i^{(k)}}},\cdots,\frac{e^{o_i^{(K)}}}{\sum\limits_{k=1}^{K} e^{o_i^{(k)}}}\right]=\frac{e^{\mathbf{o}_i}}{partition_i}</script><h4 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h4><p>对于每个样本，其交叉熵损失为</p><script type="math/tex;mode=display">\ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)=-\log \hat{y}_i^{(\kappa)},\kappa为真实标签</script><p>所以只需要关注真实分类对应的softmax值即可，pytorch有简便方法，根据索引列表依次从每个列表取出相应列表项，即从 $\hat{\mathbf{Y}}=\begin{bmatrix}\hat{\mathbf{y}}_1\\\hat{\mathbf{y}}_2\\\vdots\\\hat{\mathbf{y}}_{256}\end{bmatrix}$ 中，取出 $\begin{bmatrix}\hat{y}_1^{(\kappa_1)},\hat{y}_2^{(\kappa_2)},\cdots,\hat{y}_{256}^{(\kappa_{256})}\end{bmatrix}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line"></span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([<span class="number">0.1000</span>, <span class="number">0.5000</span>])</span><br><span class="line">从y_hat[<span class="number">0</span>]中取出y_hat[<span class="number">0</span>][<span class="number">0</span>],从y_hat[<span class="number">1</span>]中取出y_hat[<span class="number">1</span>][<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p><strong>实现交叉熵损失函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line"><span class="comment"># cross_entropy(y_hat, y)</span></span><br></pre></td></tr></table></figure><h4 id="精确度计算函数"><a href="#精确度计算函数" class="headerlink" title="精确度计算函数"></a>精确度计算函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#若y_hat是矩阵，则通过argmax得出预测分类结果</span></span><br><span class="line">    <span class="comment"># argmax返回值最大的索引，即分类结果</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="comment">#预测正确的样本，相应的y_hat(i)=1；预测错误，相应的y_hat(i)=0</span></span><br><span class="line">    <span class="comment">#	将对比结果 cmp 求和，可得预测正确的数量</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment">#精确度</span></span><br><span class="line">accuracy(y_hat, y) / <span class="built_in">len</span>(y)</span><br></pre></td></tr></table></figure><h5 id="逐批量计算数据集的精确度"><a href="#逐批量计算数据集的精确度" class="headerlink" title="逐批量计算数据集的精确度"></a>逐批量计算数据集的精确度</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#若是nn框架，对模型评估，不需要更新参数，所以不计算梯度</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    <span class="comment">#Accumulator的作用是逐批量累加</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、总样本数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment">#在一个批量上，计算预测正确数和总样本数</span></span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="comment">#返回在当前数据集上模型的精确度       </span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">evaluate_accuracy(net, test_iter)</span><br><span class="line"><span class="comment">#随机分类的精确度</span></span><br><span class="line"><span class="number">0.1069</span></span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><h5 id="参数迭代器"><a href="#参数迭代器" class="headerlink" title="参数迭代器"></a>参数迭代器</h5><p><code>updater</code> 是模型参数的优化函数，它接受批量大小作为参数。 它可以是<code>d2l.sgd</code> 函数，也可以是框架的内置优化函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#作用：对参数的一次小批量随机梯度下降的更新</span></span><br><span class="line"><span class="comment">#输入：参数；学习率；划分的批量</span></span><br><span class="line"><span class="comment">#输出：更新后的参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br></pre></td></tr></table></figure><h5 id="模型的一次迭代"><a href="#模型的一次迭代" class="headerlink" title="模型的一次迭代"></a>模型的一次迭代</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 若使用框架定义模型函数，需要计算参数的梯度，将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用自定义的优化器和损失函数</span></span><br><span class="line">            <span class="comment"># 	对损失函数计算梯度</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            <span class="comment">#使用updater中定义的梯度下降法逐批量更新参数</span></span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="comment">#更新一轮后，将本轮的损失和、预测正确数、批量样本数累加    </span></span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回本轮训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><h5 id="多轮迭代"><a href="#多轮迭代" class="headerlink" title="多轮迭代"></a>多轮迭代</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 创建一个住批量显示训练指标的动态图像绘制函数</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure><h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><h4 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h4><p><img src="/posts/3050284380/image-20240318212547601.png" alt="image-20240318212547601"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;1.导入数据&#x27;&#x27;&#x27;</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 程序在运行时启用了多线程，而多线程的使用用到了freeze_support()函数。</span></span><br><span class="line"><span class="comment"># freeze_support()函数在linux和类unix系统上可直接运行，在windows系统中需要跟在main后边。</span></span><br><span class="line"><span class="comment"># 此处将d2l库中的get_dataloader_workers()返回值改为0，单线程运行</span></span><br><span class="line"><span class="comment">#输入：批量大小</span></span><br><span class="line"><span class="comment">#输出：训练集与测试集的可迭代对象</span></span><br><span class="line"><span class="comment">#作用：从torchvision库中分别下载Fashion_mnist的训练集与测试集，按批量大小划分数据集，并返回可迭代对象</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2.参数初始化&#x27;&#x27;&#x27;</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;3.模型定义&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># softmax归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">O</span>):</span><br><span class="line">    O_exp = torch.exp(O)</span><br><span class="line">    partition = O_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> O_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment">#展平操作，将256*(1,28,28)的数据展平为 256*784的张量</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape(-<span class="number">1</span>,(W.shape[<span class="number">0</span>])), W) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># X = torch.normal(0, 1, (2, 5))</span></span><br><span class="line"><span class="comment"># X_prob = softmax(X)</span></span><br><span class="line"><span class="comment"># print(X_prob, X_prob.sum(1))</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;4.交叉熵损失&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line"><span class="comment">##精确度</span></span><br><span class="line"><span class="comment">#accuracy(y_hat, y) / len(y)</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;5. 模型评估&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#计算预测正确的样本总数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#若y_hat是矩阵，则通过argmax得出预测分类结果</span></span><br><span class="line">    <span class="comment"># argmax返回值最大的索引，即分类结果</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="comment">#预测正确的样本，相应的y_hat(i)=1；预测错误，相应的y_hat(i)=0</span></span><br><span class="line">    <span class="comment">#	将对比结果 cmp 求和，可得预测正确的数量</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = torch.tensor([0, 2])</span></span><br><span class="line"><span class="comment"># y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])</span></span><br><span class="line"><span class="comment"># y_hat[[0, 1], y]</span></span><br><span class="line"><span class="comment"># print(accuracy(y_hat, y) / len(y))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义累加器，累加器实例可以逐批量累加</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#若是nn框架，对模型评估，不需要更新参数，所以不计算梯度</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    <span class="comment">#Accumulator的作用是逐批量累加</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、总样本数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment">#在一个批量上，计算预测正确数和总样本数</span></span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="comment">#返回在当前数据集上模型的精确度       </span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;6. 模型训练&#x27;&#x27;&#x27;</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"><span class="comment">#作用：对参数的一次小批量随机梯度下降的更新</span></span><br><span class="line"><span class="comment">#输入：参数；学习率；划分的批量</span></span><br><span class="line"><span class="comment">#输出：更新后的参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 若使用框架定义模型函数，需要计算参数的梯度，将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用自定义的优化器和损失函数</span></span><br><span class="line">            <span class="comment"># 	对损失函数计算梯度</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            <span class="comment">#使用updater中定义的梯度下降法逐批量更新参数</span></span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="comment">#更新一轮后，将本轮的损失和、预测正确数、批量样本数累加    </span></span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回本轮训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        train_loss, train_acc = train_metrics</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;epoch:&quot;</span>,epoch+<span class="number">1</span>,<span class="string">&quot;#train_loss:&quot;</span>,train_loss,<span class="string">&quot;,train_acc:&quot;</span>,train_acc,<span class="string">&quot;,test_acc:&quot;</span>,test_acc)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="comment">#输入：</span></span><br><span class="line"><span class="comment">#   tanin_iter对训练数据集按批量大小封装的可迭代对象</span></span><br><span class="line"><span class="comment">#   test_iter对测试集按批量大小封装的可迭代对象</span></span><br><span class="line"><span class="comment">#   loss损失函数</span></span><br><span class="line"><span class="comment">#   num_epochs训练次数</span></span><br><span class="line"><span class="comment">#   trainer参数更新器</span></span><br><span class="line"><span class="comment">#功能：在训练集上训练模型，使用trainer指定的参数更新方法更新模型参数，每轮训练后，在计算模型在测试集上的预测精确度，迭代指定轮数后停止</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure><p><img src="/posts/3050284380/image-20240318204613936.png" alt="image-20240318204613936"></p><p><img src="/posts/3050284380/image-20240318205323324.png" alt="image-20240318205323324"></p><h4 id="可视化预测结果"><a href="#可视化预测结果" class="headerlink" title="可视化预测结果"></a>可视化预测结果</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入：模型；测试集；可视化测试结果</span></span><br><span class="line"><span class="comment"># 功能：获取1个批量的测试集，获取测试集真实标签文本，对预测结果获取预测标签文本，指定每个图片的标签格式，可视化n个结果，并显示标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><h3 id="2-3-2-基于Pytorch实现Softmax"><a href="#2-3-2-基于Pytorch实现Softmax" class="headerlink" title="2.3.2 基于Pytorch实现Softmax"></a>2.3.2 基于Pytorch实现Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="comment">#由于d2l中没有train_ch3，所以引用手动实现中的训练主函数</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;./&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> line_model <span class="keyword">import</span> line_softmax1</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;1. 获取数据集&#x27;&#x27;&#x27;</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 记得在windows下运行将线程数修改为1</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2. 初始化模型&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状。因此，在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class="line"><span class="comment"># softmax层输入为1*28*28=784维列向量，输出为10维列向量</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入：当前层</span></span><br><span class="line"><span class="comment"># 功能：若是线性层，将其权重初始化为均值为0、方差为0.01的随机值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将初始化函数应用到网络各层，完成对线性层参数的随机初始化</span></span><br><span class="line">net.apply(init_weights);</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;3. 损失函数定义为交叉熵&#x27;&#x27;&#x27;</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;4.参数优化算法&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 参数：各层的参数：学习率</span></span><br><span class="line"><span class="comment"># 功能：获取参数优化器，利用SGD算法实现参数更新</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;5.模型训练&#x27;&#x27;&#x27;</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"><span class="comment">#输入：</span></span><br><span class="line"><span class="comment">#   tanin_iter对训练数据集按批量大小封装的可迭代对象</span></span><br><span class="line"><span class="comment">#   test_iter对测试集按批量大小封装的可迭代对象</span></span><br><span class="line"><span class="comment">#   loss损失函数</span></span><br><span class="line"><span class="comment">#   num_epochs训练次数</span></span><br><span class="line"><span class="comment">#   trainer参数更新器</span></span><br><span class="line"><span class="comment">#功能：在训练集上训练模型，使用trainer指定的参数更新方法更新模型参数，每轮训练后，在计算模型在测试集上的预测精确度，迭代指定轮数后停止</span></span><br><span class="line">line_softmax1.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><h3 id="2-3-3-关于损失函数的处理"><a href="#2-3-3-关于损失函数的处理" class="headerlink" title="2.3.3 关于损失函数的处理"></a>2.3.3 关于损失函数的处理</h3><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/LogSumExp">“LogSumExp技巧”</a></p><p>在 softmax 前，净输出 $o_i^{(k)}$ 是一个未归一化的值，可能非常大，则 $e^{o_i^{(k)}}$ 可能会大于数据类型允许的最大数字——<strong>上溢</strong> ，上溢会使得分母或分子无穷大，从而得到 <code>0</code> 、<code>inf</code> 、<code>nan</code> 的 $\hat{y}_i^{(k)}$</p><p>为避免上溢，在softmax 之前先让所有的 $o_i^{(k)}$ 减去 $\max(o^{(k)})$ ，即</p><script type="math/tex;mode=display">\begin{aligned}
\hat{y}_i^{(k)}&=\frac{e^{o_i^{(k)}-\max(o_i^{(k)})}e^{\max(o_i^{(k)})}}{\sum\limits_{k=1}^Ke^{o_i^{(k)}-\max(o_i^{(k)})}e^{\max(o_i^{(k)})}}\\
&=\frac{e^{o_i^{(k)}-\max(o_i^{(k)})}}{\sum\limits_{k=1}^Ke^{o_i^{(k)}-\max(o_i^{(k)})}}\\
\hat{\mathbf{y}}_i&=\frac{e^{\mathbf{o}_i^T-\max(o_i^{(k)})}}{\sum\limits_{k=1}^{K} e^{o_i^{(k)}-\max(o_i^{(k)})}}
\end{aligned}</script><p>在减法规范化后，可能有些 $o_i^{(k)}-\max(o_i^{(k)})$ 是比较大的负值，由于精度受限，$e^{o_i^{(k)}-\max(o_i^{(k)})}$ 会接近0——<strong>下溢</strong> 。这些值四舍五入后，使得 $\hat{\mathbf{y}}_i$ 为零，$\log(\hat{\mathbf{y}}_i)$ 会 <code>-inf</code> ，反向传播后会出现 <code>nan</code></p><p>尽管 <em>softmax</em> 计算的是指数函数，但之后交叉熵损失函数是指数函数，二者结合在一起可以避免反向传播过程中的数值稳定性问题。</p><p>因此，可在交叉熵损失函数中直接使用未规范化的净输出作为损失函数</p><script type="math/tex;mode=display">\begin{aligned}
\log(\hat{\mathbf{y}}_i)&=\log\left(\frac{e^{\mathbf{o}_i^T-\max(o_i^{(k)})}}{\sum\limits_{k=1}^{K} e^{o_i^{(k)}-\max(o_i^{(k)})}}\right)\\
&=\mathbf{o}_i^T-\max(o_i^{(k)})-\log\left(\sum\limits_{k=1}^{K} e^{o_i^{(k)}-\max(o_i^{(k)})}\right)
\end{aligned}</script><h1 id="3-前馈神经网络"><a href="#3-前馈神经网络" class="headerlink" title="3. 前馈神经网络"></a>3. 前馈神经网络</h1><h2 id="3-1-多层感知机"><a href="#3-1-多层感知机" class="headerlink" title="3.1 多层感知机"></a>3.1 多层感知机</h2><h3 id="3-1-1-线性感知机PLA"><a href="#3-1-1-线性感知机PLA" class="headerlink" title="3.1.1 线性感知机PLA"></a>3.1.1 线性感知机PLA</h3><p>感知机解决的是二分类问题</p><p><img src="/posts/3050284380/image-20240319124803900.png" alt="image-20240319124803900"></p><p><img src="/posts/3050284380/image-20240319124830766.png" alt="image-20240319124830766"></p><p>损失函数的梯度 $\frac{\partial \ell(y,\mathbf{x},\mathbf{w})}{\partial \mathbf{w}}=-y\mathbf{x}$ ，$\frac{\partial \ell(y,\mathbf{x},\mathbf{w})}{\partial b}=-y$</p><p><img src="/posts/3050284380/image-20240319124902919.png" alt="image-20240319124902919"></p><ul><li>收敛步数影响因素：数据规模(越大收敛越慢)、分界余量（越大收敛越快）</li></ul><p>缺陷：不能拟合XOR函数，因为单层感知机只能产生线性分割面</p><p><img src="/posts/3050284380/image-20240318222853913.png" alt="image-20240318222853913"></p><ul><li>$1\oplus1=1$，$-1\oplus-1=1$ ；$-1\oplus1=-1,1\oplus -1=-1$</li></ul><h3 id="3-1-2-多层感知机MLP"><a href="#3-1-2-多层感知机MLP" class="headerlink" title="3.1.2 多层感知机MLP"></a>3.1.2 多层感知机MLP</h3><blockquote><p>multi-layer perceptrons</p></blockquote><h4 id="XOR学习到单隐藏层感知机"><a href="#XOR学习到单隐藏层感知机" class="headerlink" title="XOR学习到单隐藏层感知机"></a>XOR学习到单隐藏层感知机</h4><p><img src="/posts/3050284380/image-20240318223339988.png" alt="image-20240318223339988"></p><p>先训练蓝色感知机，将1,3分为+类，将2,4分为-类</p><p>再训练黄色感知机，将1,2分为+类，将3,4分为-类</p><p>最后训练灰色感知机，将两个+类或两个-类分为+类，将一个+类和一个-类分为-类</p><h4 id="单隐藏层MLP"><a href="#单隐藏层MLP" class="headerlink" title="单隐藏层MLP"></a>单隐藏层MLP</h4><p>输入 $\mathbf{x}\in \mathbb{R}^{1\times d}$</p><p>隐藏层 $\mathbf{W}_1\in \mathbb{R}^{M_1\times d}$ ，$\mathbf{b}_{1}\in \mathbb{R}^{1\times M_1}$</p><ul><li>隐藏层输出=输出层输入：$\mathbf{h}_1=\sigma(\mathbf{x}\mathbf{W}_1^T+\mathbf{b}_1)\in \mathbb{R}^{1\times M_1}$</li><li>$\sigma(\cdot)$ 为按元素非线性激活函数</li></ul><p>输出层 $\mathbf{w}_2\in \mathbb{R}^{1\times M_1}$ ，$b_2\in \mathbb{R}$</p><ul><li>$\hat{o}=\mathbf{h}_1\mathbf{w}_2^T+b_2\in\R$</li></ul><h5 id="激活函数非线性"><a href="#激活函数非线性" class="headerlink" title="激活函数非线性"></a>激活函数非线性</h5><blockquote><p><strong>非线性激活函数是为了避免层数塌陷</strong></p></blockquote><p>若激活函数为线性函数，经过隐藏层后输出等价于</p><script type="math/tex;mode=display">\begin{aligned}
o&=\mathbf{h}_1\mathbf{w}^T+b_2\\
&=\left(\mathbf{x}\mathbf{W}_1^T+\mathbf{b}_1\right)\mathbf{w}^T+b_2\\
&=\mathbf{x}\mathbf{W}_1^T\mathbf{w}^T+b'
\end{aligned}</script><p>即多层嵌套后仍然是一个线性感知器，这样的学习效果不好</p><h5 id="激活函数类型"><a href="#激活函数类型" class="headerlink" title="激活函数类型"></a>激活函数类型</h5><p><img src="/posts/3050284380/image-20240319003424239.png" alt="image-20240319003424239"></p><p><img src="/posts/3050284380/image-20240319003441350.png" alt="image-20240319003441350"></p><p><img src="/posts/3050284380/image-20240319003458955.png" alt="image-20240319003458955"></p><p>ReLU(x)：节省资源，不需要计算指数函数</p><h5 id="softmax与单隐藏层多分类MLP"><a href="#softmax与单隐藏层多分类MLP" class="headerlink" title="softmax与单隐藏层多分类MLP"></a>softmax与单隐藏层多分类MLP</h5><p>输入 $\mathbf{x}\in \mathbb{R}^{1\times d}$</p><p>隐藏层 $\mathbf{W}_1\in \mathbb{R}^{M_1\times d}$ ，$\mathbf{b}_{1}\in \mathbb{R}^{1\times M_1}$</p><ul><li>隐藏层输出=输出层输入：$\mathbf{h}_1=\sigma(\mathbf{x}\mathbf{W}_1^T+\mathbf{b}_1)\in \mathbb{R}^{1\times M_1}$</li><li>$\sigma(\cdot)$ 为按元素非线性激活函数</li></ul><p>输出层 $\mathbf{W}_2\in \mathbb{R}^{K\times M_1}$ ，$\mathbf{b}_2\in \mathbb{R}^{1\times K}$</p><ul><li>$\hat{\mathbf{o}}=\mathbf{h}_1\mathbf{W}_2^T+\mathbf{b}_2\in\R^{1\times K}$</li><li>$\hat{\mathbf{y}}=softmax(\hat{\mathbf{o}})$</li></ul><h4 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h4><p><img src="/posts/3050284380/image-20240319011936498.png" alt="image-20240319011936498"></p><p>输入 $\mathbf{x}\in \mathbb{R}^{1\times d}$</p><p>第一隐藏层 $\mathbf{W}_1\in \mathbb{R}^{M_1\times d}$ ，$\mathbf{b}_{1}\in \mathbb{R}^{1\times M_1}$</p><ul><li>第一隐藏层输出=第二隐藏层输入：$\mathbf{h}_1=\sigma(\mathbf{x}\mathbf{W}_1^T+\mathbf{b}_1)\in \mathbb{R}^{1\times M_1}$</li><li>$\sigma(\cdot)$ 为按元素非线性激活函数</li></ul><p>第二隐藏层 $\mathbf{W}_2\in \mathbb{R}^{M_2\times M_1}$ ，$b_2\in \mathbb{R}^{1\times M_2}$</p><ul><li>第二隐藏层输出=第三隐藏层输入：$\mathbf{h}_2=\sigma(\mathbf{h}_1\mathbf{W}_2^T+\mathbf{b}_2)\in \mathbb{R}^{1\times M_2}$</li></ul><p>第三隐藏层 $\mathbf{W}_3\in \mathbb{R}^{M_3\times M_2}$ ，$b_3\in \mathbb{R}^{1\times M_3}$</p><ul><li>第三隐藏层输出=输出层输入：$\mathbf{h}_3=\sigma(\mathbf{h}_2\mathbf{W}_3^T+\mathbf{b}_3)\in \mathbb{R}^{1\times M_3}$</li></ul><p>输出层 $\mathbf{w}_4\in \mathbb{R}^{K\times M_3}$ ，$\mathbf{b}_4\in \mathbb{R}^{1\times K}$</p><ul><li>$\hat{\mathbf{o}}=\mathbf{h}_3\mathbf{W}_4^T+\mathbf{b}_4\in\R^{1\times K}$</li><li>$\hat{\mathbf{y}}=softmax(\hat{\mathbf{o}})$</li></ul><h5 id="超参数设置"><a href="#超参数设置" class="headerlink" title="超参数设置"></a>超参数设置</h5><ul><li>隐藏层数</li><li>每个隐藏层大小：每层隐藏层神经元个数</li></ul><p>超参数的设置需要根据经验，越复杂的输入，参数越复杂</p><ul><li>模型越复杂，$K_1$ 越大</li></ul><p>数据比较难：</p><ul><li>单隐藏层，$K_1$ 设置大一些</li><li>网络层数深一些，相对单隐藏层的 $K_1$ ，多隐藏层的 $K_1$ 会小一些，$K_1&gt;K_2&gt;K_3$</li></ul><p>数据复杂时，输入规模是比较大的，输出相对小，从输入到输出可以理解为一个压缩过程，这个压缩过程应该是逐渐变小的，这样损失的信息才最小</p><p>第一隐藏层可以选择较大的神经元，也可以对输入进行扩充，但一般不会在非第一隐藏层之后进行扩充，信息压缩后再复原是比较难得</p><h3 id="3-1-3-MLP实现"><a href="#3-1-3-MLP实现" class="headerlink" title="3.1.3 MLP实现"></a>3.1.3 MLP实现</h3><p>将Fashion-mnist数据集用MLP训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;./&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> line_model <span class="keyword">import</span> line_softmax1</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#num_inputs:输入层神经元个数</span></span><br><span class="line"><span class="comment">#num_outputs：输出层神经元个数</span></span><br><span class="line"><span class="comment">#num_hiddens：[0]-输入层神经元个数,[1]-输出层神经元个数,[2]-隐藏层神经元个数</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机初始化，不能随机初始化为0</span></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment">#拉成二维矩阵，一张三维图片(1,28,28)变为一维向量(1,784)；256张变为(256,784)的矩阵</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X@W1 + b1)  <span class="comment"># 这里“@”代表矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> (H@W2 + b2)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">line_softmax1.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure><p>同样训练10轮，损失降了，但精确度没降</p><h3 id="3-1-4-基于Pytorch的MLP实现"><a href="#3-1-4-基于Pytorch的MLP实现" class="headerlink" title="3.1.4 基于Pytorch的MLP实现"></a>3.1.4 基于Pytorch的MLP实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;./&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> line_model <span class="keyword">import</span> line_softmax1</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数数量：</span></span><br><span class="line"><span class="comment">#   展平层：256*(1,28,28)-&gt;(256,784)</span></span><br><span class="line"><span class="comment">#   线性隐藏层(256,784)*(784, 256)-&gt;(256,256)</span></span><br><span class="line"><span class="comment">#   激活隐藏层</span></span><br><span class="line"><span class="comment">#   输出层(256,256)*(256, 10)-&gt;(256,10)</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="comment"># 只需要初始化线性层的参数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment">#将参数应用到网络</span></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"></span><br><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># 对参数随机梯度下降</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment">#输入：</span></span><br><span class="line"><span class="comment">#   tanin_iter对训练数据集按批量大小封装的可迭代对象</span></span><br><span class="line"><span class="comment">#   test_iter对测试集按批量大小封装的可迭代对象</span></span><br><span class="line"><span class="comment">#   loss损失函数</span></span><br><span class="line"><span class="comment">#   num_epochs训练次数</span></span><br><span class="line"><span class="comment">#   trainer参数更新器</span></span><br><span class="line"><span class="comment">#功能：在训练集上训练模型，使用trainer指定的参数更新方法更新模型参数，每轮训练后，在计算模型在测试集上的预测精确度，迭代指定轮数后停止</span></span><br><span class="line">line_softmax1.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><img src="/posts/3050284380/image-20240330151233579.png" alt="image-20240330151233579"></p><p>过程中还是有一定的过拟合</p><h2 id="3-2-全连接前馈神经网络"><a href="#3-2-全连接前馈神经网络" class="headerlink" title="3.2 全连接前馈神经网络"></a>3.2 全连接前馈神经网络</h2><p>前馈神经网络（Feedforward Neural Network,FNN）也称为多层感知器（实际上前馈神经网络由多层softmax回归模型组成）</p><p><img src="/posts/3050284380/image-20231006000634803.png" alt="image-20231006000634803"></p><p>前馈神经网络中，各个神经元属于不同的层</p><p>每层神经元接收前一层神经元的信号，并输出到下一层</p><ul><li>输入层：第0层</li><li>输出层：最后一层</li><li>隐藏层：其他中间层</li></ul><p>整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示</p><h3 id="3-2-1-符号说明"><a href="#3-2-1-符号说明" class="headerlink" title="3.2.1 符号说明"></a>3.2.1 符号说明</h3><h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><div class="table-container"><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>$L$</td><td>神经网络层数</td></tr><tr><td>$M_l$</td><td>第 $l$ 层神经元个数</td></tr><tr><td>$f_l(\cdot)$</td><td>第 $l$ 层神经元的激活函数</td></tr></tbody></table></div><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><div class="table-container"><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>$\mathbf{W}_{l}\in \mathbb{R}^{M_l\times M_{l-1}}$</td><td>第 $l-1$ 层到第 $l$ 层的权重矩阵</td></tr><tr><td>$\mathbf{b}_{l}\in \mathbb{R}^{1\times M_l}$</td><td>第 $l-1$ 层到第 $l$ 层的偏置</td></tr></tbody></table></div><h4 id="活性值"><a href="#活性值" class="headerlink" title="活性值"></a>活性值</h4><div class="table-container"><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>$\mathbf{o}_{l}\in\R^{1\times M_l}$</td><td>第 $l$ 层神经元的净输入（净活性值）</td></tr><tr><td>$\mathbf{h}_{l}\in \mathbb{R}^{1\times M_{l}}$</td><td>第 $l$ 层神经元的输出（活性值）</td></tr></tbody></table></div><h3 id="3-2-2-信息传播公式"><a href="#3-2-2-信息传播公式" class="headerlink" title="3.2.2 信息传播公式"></a>3.2.2 信息传播公式</h3><p>神经网络的第 $l$ 层有 $M_l$ 个神经元，相应的有 $M_l$ 个净输入和活性值，所以二者需要由 $\mathbb{R}^{1\times M_l}$ 维向量来表示</p><p>第 $l$ 层的输入为第 $l-1$ 层的活性值，相应的为 $\mathbb{R}^{1\times M_{l-1}}$ 向量</p><p>故第 $l$ 层神经元的净输入需要经过一个 <strong>仿射变换</strong>，即</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{o}_{l}&=\mathbf{x}_l\mathbf{W}_{l}^{T}+\mathbf{b}^{(l)}，其中 \mathbf{W}_{l}\in \mathbb{R}^{M_l\times M_{l-1}}\\
&=\mathbf{h}_{l-1}\mathbf{W}_{l}^{T}+\mathbf{b}^{(l)}\\
&=f_{l-1}(\mathbf{o}_{l-1})\mathbf{W}_{l}^{T}+\mathbf{b}^{(l)}
\end{aligned}</script><p>获取活性值 $\mathbf{h}_{l}$ 需要经过一个 <strong>非线性变换</strong></p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{h}_{l}&=f_l(\mathbf{o}_{l})\\
&=f_l(\mathbf{h}_{l-1}\mathbf{W}_{l}^T+\mathbf{b}_{l})
\end{aligned}</script><p>进而可知，由输入到网络最后的输出 $\mathbf{h}_{L}$</p><script type="math/tex;mode=display">\mathbf{x}=\mathbf{h}_{0}\xrightarrow{\mathbf{W}_1}\mathbf{o}_{1}\xrightarrow{f_1()}\mathbf{h}_{1}\cdots\xrightarrow{f_{L-1}()}\mathbf{h}_{L-1}\xrightarrow{\mathbf{W}_{L}}\mathbf{o}_{L}=\phi(\mathbf{x};\mathbf{W};\mathbf{b})</script><p>其中 $\mathbf{W},\mathbf{b}$ 表示网络中所有层的连接权重和偏置</p><p>前馈神经网络可以通过逐层的信息传递，整个网络可以看做一个复合函数 $\phi(\mathbf{x};\mathbf{W};\mathbf{b})$</p><h4 id="通用近似定理"><a href="#通用近似定理" class="headerlink" title="通用近似定理"></a>通用近似定理</h4><p><img src="/posts/3050284380/image-20240320205108341.png" alt="image-20240320205108341"></p><p>根据通用近似定理，对于具有 <strong>线性输出层</strong> $z^{(l)}$ 和至少一个 <strong>具有挤压性质的激活函数</strong> $\phi(\cdot)$ 的隐藏层组成的前馈神经网络，只要隐藏层的神经元数量足够，就可以以任意精度来近似任何一个定义在实数空间中的有界闭函数</p><h3 id="3-2-3-应用于分类任务"><a href="#3-2-3-应用于分类任务" class="headerlink" title="3.2.3 应用于分类任务"></a>3.2.3 应用于分类任务</h3><blockquote><p>神经网络可以作为一个万能函数，用于进行复杂的特征转换或逼近一个条件分布</p></blockquote><p>在机器学习中，输入样本的特征对分类器性能的影响很大</p><p>若要获得很好的分类效果，需要将样本的原始特征向量 $\mathbf{x}$ 转换到更有效的特征向量 $\phi(x)$ ——特征抽取</p><p>多层前馈神经网络恰好可以看做一个非线性函数 $\phi(\cdot)$ ，将输入 $\mathbf{x}\in \mathbb{R}^{1\times d}$ 映射到输出 $\phi(x)\in \mathbb{R}^{K}$ ，因此可将多层前馈神经网络看作一种特殊的特征转换方法，其输出 $\phi(x)$ 作为分类器的输入</p><script type="math/tex;mode=display">\hat{\mathbf{y}}=g(\phi(\mathbf{x});\theta)</script><ul><li>$g(\cdot)$ 为分类器</li><li>$\theta$ 为分类器 $g(\cdot)$ 的参数</li><li>$\hat{\mathbf{y}}$ 为分类器输出</li></ul><p>若分类器 $g(\cdot)$ 为 $Logistic回归$ 或 $Softmax回归$ ，则相当于在输出层引入分类器，神经网络直接输出在不同类别的条件概率 $p(\hat{\mathbf{y}}\vert \mathbf{x})$</p><h4 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h4><p>对于二分类问题 $y\in \{0,1\}$ ，且采用 $Logistic回归$ ，神经网络的输出层只有一个神经元，其激活函数是 $Logistic函数$</p><script type="math/tex;mode=display">p(y=1\vert x)=logistic(o_{L})=h_{L}\in [0,1]</script><h4 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h4><p>对于多分类问题 $y\in \{1,\cdots,K\}$ ，如果使用 $Softmax$ 分类器，网络最后一层设置 $K$ 个神经元，其激活函数为 $Softmax$ ，网络最后一层的输出可以作为每个类的条件概率</p><script type="math/tex;mode=display">\hat{\mathbf{y}}=softmax(\mathbf{o}_{L})</script><p>其中，$\mathbf{o}_{L}\in \mathbb{R}^{1\times K}$ 为第 $L$ 层神经网络的净输出， $\hat{\mathbf{y}}\in\R^{1\times K}$ 为第 $L$ 层神经网络的活性值，每一维分别表示不同类别标签的预测条件概率</p><h3 id="3-2-4-参数学习与误差反向传播"><a href="#3-2-4-参数学习与误差反向传播" class="headerlink" title="3.2.4 参数学习与误差反向传播"></a>3.2.4 参数学习与误差反向传播</h3><h4 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h4><p>对于一个训练样本 $(\mathbf{x},\mathbf{y})$ ，使用有 $L$ 层的神经网络，第 $l$ 层的活性值为 $\mathbf{h}_l$</p><script type="math/tex;mode=display">\mathbf{h_l}=\sigma\left(\mathbf{o}_l\right)=\sigma(\mathbf{h}_{l-1}\mathbf{W}_l^T+\mathbf{b}_l)=f_l(\mathbf{h}_{l-1})</script><p>其中，$\mathbf{h}_{l-1}\in \mathbb{R}^{1\times M_{l-1}}$ ，$\mathbf{W}\in\R^{M_{l}\times M_{l-1}}$ ，$\mathbf{b}_l\in \mathbb{R}^{1\times M_l}$</p><p>输出层 $\hat{\mathbf{y}}=f_L\circ f_{L-1}\cdots \circ f_1(\mathbf{x})$</p><p>若采用交叉熵损失函数，其损失函数为</p><script type="math/tex;mode=display">\ell(\mathbf{y},\hat{\mathbf{y}})=-\mathbf{y}\log\hat{\mathbf{y}},\mathbf{y}\in \{0,1\}^{K}</script><p>给定训练集，$\mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^N$ ，将每个样本 $\mathbf{x}_i$ 输入给前馈网络得到 $\hat{\mathbf{y}}_i$ ，其结构化风险函数为</p><script type="math/tex;mode=display">\mathcal{R}(\mathbf{W},\mathbf{b})=\frac{1}{N}\sum\limits_{i=1}^N\frac{\partial \ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)}{\partial \mathbf{W}_{l}}+\lambda \Vert \mathbf{W}_{l}\Vert^2</script><ul><li><p>$\lambda$ 为超参数，$\lambda$ 越大，$\mathbf{W}$ 越接近于0</p></li><li><p>一般用 $Frobenius$ 范数（F范数）作为惩罚项</p><script type="math/tex;mode=display">\Vert\mathbf{W}\Vert_F^2=\sum\limits_{l=1}^L\sum\limits_{i=1}^{M_l}\sum\limits_{j=1}^{M_{l-1}}\left(w^{(ij)}_{l}\right)^2</script></li></ul><p>对于某一层网络参数，可以通过梯度下降的方法学习</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{W}_{l}&\leftarrow \mathbf{W}_{l}-\eta\frac{\partial \mathcal{R}(\mathbf{W},\mathbf{b})}{\partial \mathbf{W}_{l}}\\
&\leftarrow \mathbf{W}_{l}-\eta\left(\frac{1}{N}\sum\limits_{i=1}^N\frac{\partial \ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)}{\partial \mathbf{W}_{l}}+\lambda \Vert \mathbf{W}_{l}\Vert^2\right)\\
\mathbf{b}_{l}&\leftarrow \mathbf{b}_{l}-\eta\frac{\partial \mathcal{R}(\mathbf{W},\mathbf{b})}{\partial \mathbf{b}_{l}}\\
&\leftarrow \mathbf{b}_{l}-\eta\left(\frac{1}{N}\sum\limits_{i=1}^N\frac{\partial \ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)}{\partial \mathbf{b}_{l}}\right)
\end{aligned}</script><p>可见，参数求解的核心部分为 $\frac{\partial \ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)}{\partial \mathbf{W}_{l}}$ ，标量对矩阵的求导，事实上是对逐个元素求导，即</p><script type="math/tex;mode=display">\begin{cases}
\frac{\partial \ell}{\partial \mathbf{W}_{l}}=\frac{\partial \ell}{\partial \mathbf{o}^{l}}\frac{\partial \mathbf{o}_{l}}{\partial \mathbf{W}_{l}}\\
\frac{\partial \ell}{\partial \mathbf{b}_{l}}=\frac{\partial \ell}{\partial \mathbf{o}_{l}}\frac{\partial \mathbf{o}_{l}}{\partial \mathbf{b}_l}
\end{cases}</script><h4 id="隐藏层求导"><a href="#隐藏层求导" class="headerlink" title="隐藏层求导"></a>隐藏层求导</h4><p><strong>计算 $\frac{\partial \mathbf{o}_l}{\partial \mathbf{W}_l},\frac{\partial \mathbf{o}_l}{\partial \mathbf{b}_l}$</strong> ，其中：</p><script type="math/tex;mode=display">\begin{cases}
\mathbf{o}_{l}\in \mathbb{R}^{1\times M_{l}}\\
\mathbf{W}\in\R^{M_{l}\times M_{l-1}}\\
\mathbf{b}_l\in \mathbb{R}^{1\times M_l}
\end{cases}\Rightarrow \begin{cases}
\frac{\partial \mathbf{o}_l}{\partial \mathbf{W}_l}\in \mathbb{R}^{M_l\times M_l\times M_{l-1}}\\
\frac{\partial \mathbf{o}_l}{\partial \mathbf{b}_l}\in \mathbb{R}^{M_l\times M_l}
\end{cases}</script><p>因 $\mathbf{o}_l=\mathbf{h}_{l-1}\mathbf{W}_l^T+\mathbf{b}_l$</p><ul><li><p>计算 $\frac{\partial \mathbf{o}_l}{\partial \mathbf{W}_l}\in \mathbb{R}^{M_l\times 1}$</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{o}_l}{\partial w^{(i,j)}_l}&=\begin{bmatrix}
\frac{\partial \mathbf{o}_l^{(1)}}{\partial w^{(i,j)}_l}\\\vdots\\\frac{\partial \mathbf{o}_l^{(i)}}{\partial w^{(i,j)}_l}\\\vdots\\\frac{\partial \mathbf{o}_l^{\left(M_l\right)}}{\partial w^{(i,j)}_l}
\end{bmatrix}=\begin{bmatrix}0\\\vdots\\\frac{\partial}{\partial w^{(i,j)}_l}\left(\mathbf{h}_{l-1}\left(\mathbf{W}_l^T\right)^{(:,i)}+\mathbf{b}_l^{(i)}\right)\\\vdots\\0\end{bmatrix}\\
&=\begin{bmatrix}0\\\vdots\\\frac{\partial}{\partial w^{(i,j)}_l}\left(\sum\limits_{j=1}^{M_{l-1}}\mathbf{h}_{l-1}^{(j)}w_l^{(i,j)}+\mathbf{b}_l^{(i)}\right)\\\vdots\\0\end{bmatrix}=\left.
\begin{bmatrix}
0\\\vdots\\\mathbf{h}_{l-1}^{(j)}\\\vdots\\0
\end{bmatrix}
\right\}第i个元素
\end{aligned}</script><p>其中，$i\in [1,M_l],j\in [1,M_{l-1}]$</p><p>所以有 $\frac{\partial \mathbf{o}_l}{\partial \mathbf{W}_l}\in \mathbb{R}^{M_l\times M_l\times M_{l-1}}$</p><script type="math/tex;mode=display">\frac{\partial \mathbf{o}_l}{\partial \mathbf{W}_l}=\left.\underbrace{\begin{bmatrix}
\begin{bmatrix}
\mathbf{h}_{l-1}^{(1)}\\0\\\vdots\\0
\end{bmatrix}&\begin{bmatrix}
\mathbf{h}_{l-1}^{(2)}\\0\\\vdots\\0
\end{bmatrix}&\cdots&\begin{bmatrix}
\mathbf{h}_{l-1}^{\left(M_{l-1}\right)}\\0\\\vdots\\0
\end{bmatrix}\\
\begin{bmatrix}
0\\\mathbf{h}_{l-1}^{(1)}\\\vdots\\0
\end{bmatrix}&\begin{bmatrix}
0\\\mathbf{h}_{l-1}^{(2)}\\\vdots\\0
\end{bmatrix}&\cdots&\begin{bmatrix}
0\\\mathbf{h}_{l-1}^{\left(M_{l-1}\right)}\\\vdots\\0
\end{bmatrix}\\
\vdots&\vdots&\ddots&\vdots\\
\begin{bmatrix}
0\\0\\\vdots\\\mathbf{h}_{l-1}^{(1)}
\end{bmatrix}&\begin{bmatrix}
0\\0\\\vdots\\\mathbf{h}_{l-1}^{(2)}
\end{bmatrix}&\cdots&
\begin{bmatrix}
0\\0\\\vdots\\\mathbf{h}_{l-1}^{(M_{l-1})}
\end{bmatrix}
\end{bmatrix}}_{M_{l-1}}\quad\right\}M_l</script></li><li><p>计算 $\frac{\partial \mathbf{o}_l}{\partial \mathbf{b}_l}$</p><script type="math/tex;mode=display">\frac{\partial \mathbf{o}_l^{(i)}}{\partial \mathbf{b}_l}=\begin{bmatrix}
\frac{\partial \mathbf{o}_l^{(i)}}{\partial \mathbf{b}_l^{(1)}}&\cdots&\frac{\partial \mathbf{o}_l^{(i)}}{\partial \mathbf{b}_l^{(i)}}&\cdots&\frac{\partial \mathbf{o}_l^{(i)}}{\partial \mathbf{b}_l^{(M_l)}}
\end{bmatrix}=\begin{bmatrix}0&\cdots&1&\cdots&0\end{bmatrix}\in \mathbb{R}^{1\times M_l}</script><p>其中，$i\in [1,M_l]$</p><script type="math/tex;mode=display">\frac{\partial \mathbf{o}_l}{\partial \mathbf{b}_l}=\begin{bmatrix}
\frac{\partial \mathbf{o}_l^{(1)}}{\partial \mathbf{b}_l}\\
\vdots\\
\frac{\partial \mathbf{o}_l^{(i)}}{\partial \mathbf{b}_l}\\
\vdots\\
\frac{\partial \mathbf{o}_l^{(M_l)}}{\partial \mathbf{b}_l}
\end{bmatrix}=\begin{bmatrix}
1&\cdots&0&\cdots&0\\
\vdots&\ddots&\vdots&\ddots&\vdots\\
0&\cdots&1&\cdots&0\\
\vdots&\ddots&\vdots&\ddots&\vdots\\
0&\cdots&0&\cdots&1\\
\end{bmatrix}=\mathbf{I}_{M_l}\in \mathbb{R}^{M_l\times M_l}</script></li></ul><h4 id="误差项求导"><a href="#误差项求导" class="headerlink" title="误差项求导"></a>误差项求导</h4><p>误差项 $\frac{\partial \ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)}{\partial \mathbf{o}_{l}}$ 表示第 $l$ 层神经元对最终损失的影响，也反映了最终损失对第 $l$ 层神经元的敏感程度，不同神经元对网络能力的贡献程度，从而比较好地解决了贡献度分配问题</p><script type="math/tex;mode=display">\begin{aligned}
\delta_{l}&\triangleq\frac{\partial \ell(\mathbf{y}_i,\hat{\mathbf{y}}_i)}{\partial \mathbf{o}_{l}}=
\begin{bmatrix}
\frac{\partial \ell}{\partial \mathbf{o}_l^{(1)}}&\cdots&\frac{\partial \ell}{\partial \mathbf{o}_l^{(M_l)}}
\end{bmatrix}
\triangleq \begin{bmatrix}
\delta_{l}^{(1)}&\cdots&\delta_{l}^{(M_l)}
\end{bmatrix}
\in \mathbb{R}^{1\times M_l}\\
&=\frac{\partial \ell}{\partial \mathbf{o}_{l+1}}\frac{\partial \mathbf{o}_{l+1}}{\partial \mathbf{h}_l}\frac{\partial \mathbf{h}_l}{\partial \mathbf{o}_l}
\end{aligned}</script><p>其中，</p><ul><li><p>$\frac{\partial \mathbf{h}_l}{\partial \mathbf{o}_l}\in \mathbb{R}^{M_l\times M_l}$</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{h}_l}{\partial \mathbf{o}_l}&=\begin{bmatrix}
\frac{\partial \sigma\left(o^{(1)}_l\right)}{\partial o^{(1)}_l}&\frac{\partial \sigma\left(o^{(1)}_l\right)}{\partial o^{(2)}_l}&\cdots&\frac{\partial \sigma\left(o^{(1)}_l\right)}{\partial o^{(M_{l})}_l}\\
\frac{\partial \sigma\left(o^{(2)}_l\right)}{\partial o^{(1)}_l}&\frac{\partial \sigma\left(o^{(2)}_l\right)}{\partial o^{(2)}_l}&\cdots&\frac{\partial \sigma\left(o^{(2)}_l\right)}{\partial o^{(M_{l})}_l}\\
\vdots&\vdots&\ddots&\vdots\\
\frac{\partial \sigma\left(o^{(M_{l})}_l\right)}{\partial o^{(1)}_l}&\frac{\partial \sigma\left(o^{(M_{l})}_l\right)}{\partial o^{(2)}_l}&\cdots&\frac{\partial \sigma\left(o^{(M_{l})}_l\right)}{\partial o^{(M_{l})}_l}\\
\end{bmatrix}=\begin{bmatrix}
\sigma'\left(o_l^{(1)}\right)&0&\cdots&0\\
0&\sigma'\left(o_l^{(2)}\right)&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\sigma'\left(o^{(M_{l})}_l\right)
\end{bmatrix}\\
&=diag\left[\sigma'\left(o_l^{(i)}\right)\right],i\in \left[1,M_{l}\right]\\
&=diag\left[\sigma'\left(\mathbf{h}_{l-1}\mathbf{W}_l^T+\mathbf{b}_l\right)\right]
\end{aligned}</script></li><li><p>由于第 $l+1$ 层输入 $\mathbf{x}_{l+1}$ 为第 $l$ 层活性值 $\mathbf{h}_l$ ，$\mathbf{o}_{l+1}=\mathbf{x}_{l+1}\mathbf{W}_{l+1}^T+\mathbf{b}_{l+1}=\mathbf{h}_{l}\mathbf{W}_{l+1}^T+\mathbf{b}_{l+1}$</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{o}_{l+1}}{\partial \mathbf{h}_{l}}&=\frac{\partial \left(\mathbf{h}_{l}\mathbf{W}_{l+1}^T+\mathbf{b}_{l+1}\right)}{\partial \mathbf{h}_{l}}\in \mathbb{R}^{M_{l+1}\times M_{l}}\\
&=\mathbf{W}_{l+1}
\end{aligned}</script></li></ul><p>则有，</p><script type="math/tex;mode=display">\begin{aligned}
\delta_l&\triangleq\frac{\partial \ell}{\partial \mathbf{o}_l}\\
&=\frac{\partial \ell}{\partial \mathbf{o}_{l+1}}\frac{\partial \mathbf{o}_{l+1}}{\partial \mathbf{h}_l}\frac{\partial \mathbf{h}_l}{\partial \mathbf{o}_l}\\
&=\left(\delta_{l+1}\mathbf{W}_{l+1}\right)\odot diag\left[\sigma'\left(o_l^{(i)}\right)\right]\in \mathbb{R}^{1\times M_l}
\end{aligned}</script><p>其中，$\odot$ 为哈达姆积，表示每个元素相乘</p><script type="math/tex;mode=display">\begin{aligned}
\delta_{l+1}\mathbf{W}_{l+1}&=\begin{bmatrix}
\delta_{l+1}^{(1)}&\cdots&\delta_{l+1}^{(M_l)}&\delta_{l+1}^{(M_{l+1})}
\end{bmatrix}\begin{bmatrix}
w_{l+1}^{(11)}&w_{l+1}^{(12)}&\cdots&w_{l+1}^{\left(1M_l\right)}\\
\vdots&\vdots&\ddots&\vdots\\
w_{l+1}^{\left(M_{l}1\right)}&w_{l+1}^{\left(M_{l}2\right)}&\cdots&w_{l}^{\left(M_{l}M_l\right)}\\
w_{l+1}^{\left(M_{l+1}1\right)}&w_{l+1}^{\left(M_{l+1}2\right)}&\cdots&w_{l+1}^{\left(M_{l+1}M_l\right)}
\end{bmatrix}\\
&=\begin{bmatrix}
\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,1)}&\cdots
&
\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,M_l)}
\end{bmatrix}\in \mathbb{R}^{1\times M_{l}}
\end{aligned}</script><script type="math/tex;mode=display">\begin{aligned}
&\left[\delta_{l+1}\mathbf{W}_{l+1}\right]\odot diag\left[\sigma'\left(o_l^{(i)}\right)\right]\\
=&\begin{bmatrix}
\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,1)}&\cdots
&
\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,M_l)}
\end{bmatrix}\begin{bmatrix}
\sigma'\left(o_l^{(1)}\right)&0&\cdots&0\\
0&\sigma'\left(o_l^{(2)}\right)&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\sigma'\left(o^{(M_{l})}_l\right)
\end{bmatrix}\\
=&\begin{bmatrix}
\sigma'\left(o_l^{(1)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,1)}&\sigma'\left(o_l^{(2)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta_{l+1}^{(j)}w_{l+1}^{(j,2)}&\cdots&\sigma'\left(o_l^{(M_l)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,M_l)}
\end{bmatrix}\\
\triangleq&\begin{bmatrix}
\delta_l^{(1)}&\delta_l^{(2)}&\cdots&\delta_l^{(M_l)}
\end{bmatrix}=\delta_l
\end{aligned}</script><h5 id="误差的反向传播"><a href="#误差的反向传播" class="headerlink" title="误差的反向传播"></a>误差的反向传播</h5><p>第 $l$ 层的误差项可以通过第 $l+1$ 层的误差项计算得到，这就是 <strong>误差的反向传播</strong></p><h4 id="卷积层梯度合并"><a href="#卷积层梯度合并" class="headerlink" title="卷积层梯度合并"></a>卷积层梯度合并</h4><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \ell}{\partial \mathbf{W}_{l}}&=\frac{\partial \ell}{\partial \mathbf{o}_{l}}\frac{\partial \mathbf{o}_{l}}{\partial \mathbf{W}_{l}}\\
=&\begin{bmatrix}
\delta_l^{(1)}&\delta_l^{(2)}&\cdots&\delta_l^{(M_l)}
\end{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
\mathbf{h}_{l-1}^{(1)}\\0\\\vdots\\0
\end{bmatrix}&\begin{bmatrix}
\mathbf{h}_{l-1}^{(2)}\\0\\\vdots\\0
\end{bmatrix}&\cdots&\begin{bmatrix}
\mathbf{h}_{l-1}^{\left(M_{l-1}\right)}\\0\\\vdots\\0
\end{bmatrix}\\
\begin{bmatrix}
0\\\mathbf{h}_{l-1}^{(1)}\\\vdots\\0
\end{bmatrix}&\begin{bmatrix}
0\\\mathbf{h}_{l-1}^{(2)}\\\vdots\\0
\end{bmatrix}&\cdots&\begin{bmatrix}
0\\\mathbf{h}_{l-1}^{\left(M_{l-1}\right)}\\\vdots\\0
\end{bmatrix}\\
\vdots&\vdots&\ddots&\vdots\\
\begin{bmatrix}
0\\0\\\vdots\\\mathbf{h}_{l-1}^{(1)}
\end{bmatrix}&\begin{bmatrix}
0\\0\\\vdots\\\mathbf{h}_{l-1}^{(2)}
\end{bmatrix}&\cdots&
\begin{bmatrix}
0\\0\\\vdots\\\mathbf{h}_{l-1}^{(M_{l-1})}
\end{bmatrix}
\end{bmatrix}\\
=&\delta_l^{(1)}\begin{bmatrix}
\mathbf{h}_{l-1}^{(1)}&\mathbf{h}_{l-1}^{(2)}&\cdots&\mathbf{h}_{l-1}^{(M_{l-1})}\\
0&0&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&0\\
\end{bmatrix}+\delta_l^{(2)}\begin{bmatrix}
0&0&\cdots&0\\
\mathbf{h}_{l-1}^{(1)}&\mathbf{h}_{l-1}^{(2)}&\cdots&\mathbf{h}_{l-1}^{(M_{l-1})}\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&0\\
\end{bmatrix}+\cdots+\delta_l^{(M_l)}\begin{bmatrix}
0&0&\cdots&0\\
0&0&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
\mathbf{h}_{l-1}^{(1)}&\mathbf{h}_{l-1}^{(2)}&\cdots&\mathbf{h}_{l-1}^{(M_{l-1})}\\
\end{bmatrix}\\
=&\begin{bmatrix}
\delta_l^{(1)}\mathbf{h}_{l-1}^{(1)}&\delta_l^{(1)}\mathbf{h}_{l-1}^{(2)}&\cdots&\delta_l^{(1)}\mathbf{h}_{l-1}^{(M_{l-1})}\\
\delta_l^{(2)}\mathbf{h}_{l-1}^{(1)}&\delta_l^{(2)}\mathbf{h}_{l-1}^{(2)}&\cdots&\delta_l^{(2)}\mathbf{h}_{l-1}^{(M_{l-1})}\\
\vdots&\vdots&\ddots&\vdots\\
\delta_l^{(M_l)}\mathbf{h}_{l-1}^{(1)}&\delta_l^{(M_l)}\mathbf{h}_{l-1}^{(2)}&\cdots&\delta_l^{(M_l)}\mathbf{h}_{l-1}^{(M_{l-1})}
\end{bmatrix}
\end{aligned}</script><p>所以，$\begin{bmatrix}\frac{\partial \ell}{\partial \mathbf{W}_{l}}\end{bmatrix}_{i,j}=\delta_l^{(i)}\mathbf{h}_{l-1}^{(j)}$</p><script type="math/tex;mode=display">\frac{\partial \ell}{\partial \mathbf{W}_{l}}=\delta_l^T \mathbf{h}_{l-1}</script><script type="math/tex;mode=display">\begin{bmatrix}\frac{\partial \ell}{\partial \mathbf{W}_{l}}\end{bmatrix}_{i,j}=\delta_l^{(i)}\mathbf{h}_{l-1}^{(j)}=\left[\sigma'\left(o_l^{(i)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta_{l+1}^{(j)}w_{l+1}^{(j,i)}\right]\mathbf{h}_{l-1}^{(j)}</script><p>同理，$\ell$ 关于第 $l$ 层偏置 $\mathbf{b}_l$ 的梯度为</p><script type="math/tex;mode=display">\frac{\partial \ell}{\partial \mathbf{b}_{l}}=\frac{\partial \ell}{\partial \mathbf{o}_{l}}\frac{\partial \mathbf{o}_{l}}{\partial \mathbf{b}_l}=\delta_l^T</script><p>其中，</p><script type="math/tex;mode=display">\begin{aligned}
\delta_l&=\left[\delta_{l+1}\mathbf{W}_{l+1}\right]\odot diag\left[\sigma'\left(o_l^{(i)}\right)\right]\\
&=\begin{bmatrix}
\sigma'\left(o_l^{(1)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,1)}&\sigma'\left(o_l^{(2)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta_{l+1}^{(j)}w_{l+1}^{(j,2)}&\cdots&\sigma'\left(o_l^{(M_l)}\right)\sum\limits_{j=1}^{M_{l+1}}\delta^{(j)}_{l+1}w_{l+1}^{(j,M_l)}
\end{bmatrix}
\end{aligned}</script><p>假设有 $L$ 层神经网络</p><script type="math/tex;mode=display">\ell(\mathbf{y},\mathbf{\hat{y}})=-\mathbf{y}\log \hat{\mathbf{y}}=-\mathbf{y}\log \mathbf{h}_L=-\log\mathbf{h}_L^{(\mathcal{I}(\mathbf{y}))}=-\log \left[f_L\left(\mathbf{o}_L\right)\right]^{(\mathcal{I}(\mathbf{y}))}</script><script type="math/tex;mode=display">\delta_{L}=\frac{\partial \ell}{\partial \mathbf{o}_L}=\frac{\partial}{\partial \mathbf{o_L}}\left\{-\log \left[f_L\left(\mathbf{o}_L\right)\right]^{(\mathcal{I}(\mathbf{y}))}\right\}</script><p>$\mathcal{I}(\mathbf{y})$ 为 $\mathbf{y}$ 向量中元素值为1的索引</p><h4 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h4><p>在计算出每一层的误差项后，就可以求得本层的梯度，可以用随机梯度下降法来训练前馈神经网络</p><ol><li>前馈计算每一层的净输入 $z^{(l)}$ 和净激活值 $a^{(l)}$ ，直至最后一层</li><li>反向传播计算每一层的误差项 $\delta^{(l)}$</li><li>计算每一层的偏导数，并更新参数</li></ol><p><img src="/posts/3050284380/image-20231007113640918.png" alt="image-20231007113640918"></p><h4 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h4><p>神经网络的参数学习比线性模型更加困难</p><ul><li>非凸优化问题</li><li>梯度消失问题</li></ul><h5 id="非凸优化问题"><a href="#非凸优化问题" class="headerlink" title="非凸优化问题"></a>非凸优化问题</h5><p>神经网络的优化问题是一个非凸优化问题</p><script type="math/tex;mode=display">y=\sigma(w_2\sigma(w_1x))</script><p><img src="/posts/3050284380/image-20231007175031669.png" alt="image-20231007175031669"></p><h5 id="梯度消失问题"><a href="#梯度消失问题" class="headerlink" title="梯度消失问题"></a>梯度消失问题</h5></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/3050284380/" title="2+3.线性模型与全连接前馈神经网络">https://amostian.github.io/posts/3050284380/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3226843952/" rel="prev" title="0.动手学深度学习"><i class="fa fa-chevron-left"></i> 0.动手学深度学习</a></div><div class="post-nav-item"><a href="/posts/2645721241/" rel="next" title="4.动手学深度学习-模型评估">4.动手学深度学习-模型评估 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">2. 线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0"><span class="nav-text">2.1 线性回归实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="nav-text">2.1.1 生成数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-%E7%94%9F%E6%88%90%E6%89%B9%E9%87%8F"><span class="nav-text">2.1.2 生成批量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-text">2.1.3 初始化模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="nav-text">随机初始化参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-4-%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">2.1.4 定义损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-5-%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-text">2.1.5 定义优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-6-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-text">2.1.6 训练过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">2.2 基于pytorch实现线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="nav-text">2.2.1 生成数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8-%E7%90%86%E8%A7%A3"><span class="nav-text">2.2.2 数据迭代器(理解)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-text">2.2.3 初始化模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89-1"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-text">随机初始化模型参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">2.2.4 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-5-%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0"><span class="nav-text">2.2.5 定义优化函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-6-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-text">2.2.6 训练过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-softmax%E5%9B%9E%E5%BD%92"><span class="nav-text">2.3 softmax回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">分类数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">下载数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">查看数据集</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">可视化数据集</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E6%89%B9%E9%87%8F%E8%AF%BB%E5%8F%96"><span class="nav-text">实现批量读取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%81%E8%A3%85%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="nav-text">封装数据读取</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-Softmax%E5%AE%9E%E7%8E%B0"><span class="nav-text">2.3.1 Softmax实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%B9%E9%87%8F"><span class="nav-text">获取数据集批量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="nav-text">初始化参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89-2"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">交叉熵损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B2%BE%E7%A1%AE%E5%BA%A6%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0"><span class="nav-text">精确度计算函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%90%E6%89%B9%E9%87%8F%E8%AE%A1%E7%AE%97%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%B2%BE%E7%A1%AE%E5%BA%A6"><span class="nav-text">逐批量计算数据集的精确度</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-text">训练</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="nav-text">参数迭代器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%80%E6%AC%A1%E8%BF%AD%E4%BB%A3"><span class="nav-text">模型的一次迭代</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E8%BD%AE%E8%BF%AD%E4%BB%A3"><span class="nav-text">多轮迭代</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B"><span class="nav-text">预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E5%90%88"><span class="nav-text">整合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-text">可视化预测结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-%E5%9F%BA%E4%BA%8EPytorch%E5%AE%9E%E7%8E%B0Softmax"><span class="nav-text">2.3.2 基于Pytorch实现Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-%E5%85%B3%E4%BA%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%A4%84%E7%90%86"><span class="nav-text">2.3.3 关于损失函数的处理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">3. 前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-text">3.1 多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E7%BA%BF%E6%80%A7%E6%84%9F%E7%9F%A5%E6%9C%BAPLA"><span class="nav-text">3.1.1 线性感知机PLA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAMLP"><span class="nav-text">3.1.2 多层感知机MLP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#XOR%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-text">XOR学习到单隐藏层感知机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82MLP"><span class="nav-text">单隐藏层MLP</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="nav-text">激活函数非线性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%B1%BB%E5%9E%8B"><span class="nav-text">激活函数类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#softmax%E4%B8%8E%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82%E5%A4%9A%E5%88%86%E7%B1%BBMLP"><span class="nav-text">softmax与单隐藏层多分类MLP</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-text">多隐藏层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-text">超参数设置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-MLP%E5%AE%9E%E7%8E%B0"><span class="nav-text">3.1.3 MLP实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84MLP%E5%AE%9E%E7%8E%B0"><span class="nav-text">3.1.4 基于Pytorch的MLP实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">3.2 全连接前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E"><span class="nav-text">3.2.1 符号说明</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0"><span class="nav-text">参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B4%BB%E6%80%A7%E5%80%BC"><span class="nav-text">活性值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-%E4%BF%A1%E6%81%AF%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F"><span class="nav-text">3.2.2 信息传播公式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86"><span class="nav-text">通用近似定理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-%E5%BA%94%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="nav-text">3.2.3 应用于分类任务</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-text">二分类问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-text">多分类问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-4-%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">3.2.4 参数学习与误差反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-text">参数学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82%E6%B1%82%E5%AF%BC"><span class="nav-text">隐藏层求导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E9%A1%B9%E6%B1%82%E5%AF%BC"><span class="nav-text">误差项求导</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">误差的反向传播</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E6%A2%AF%E5%BA%A6%E5%90%88%E5%B9%B6"><span class="nav-text">卷积层梯度合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B"><span class="nav-text">算法过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-text">优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-text">非凸优化问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98"><span class="nav-text">梯度消失问题</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">216</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">82</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1150.8k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">46:51</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>