<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="免模型学习的价值函数计算方法  蒙特卡洛方法：将策略迭代中，基于模型的部分替换为免模型部分 更新时间：2024-2-24 10:28:04  时序差分方法 更新时间：2024-2-27 23:34:53   [TOC]"><meta property="og:type" content="article"><meta property="og:title" content="3-基于价值表格的免模型学习"><meta property="og:url" content="https://amostian.github.io/posts/1184554208/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="免模型学习的价值函数计算方法  蒙特卡洛方法：将策略迭代中，基于模型的部分替换为免模型部分 更新时间：2024-2-24 10:28:04  时序差分方法 更新时间：2024-2-27 23:34:53   [TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223105012439.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223155537861.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223165057968.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223165703357.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223165756497.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240222174121229.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224102256712.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224094801791.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240222233322816.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224101526505.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224101639774.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/Figure_1.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240225111350459.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240227163416632.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240227163505422.png"><meta property="article:published_time" content="2024-02-24T02:28:04.000Z"><meta property="article:modified_time" content="2024-03-10T13:05:27.733Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="强化学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/1184554208/image-20240223105012439.png"><link rel="canonical" href="https://amostian.github.io/posts/1184554208/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>3-基于价值表格的免模型学习 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">58</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">355</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/1184554208/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">3-基于价值表格的免模型学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-02-24 10:28:04" itemprop="dateCreated datePublished" datetime="2024-02-24T10:28:04+08:00">2024-02-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-03-10 21:05:27" itemprop="dateModified" datetime="2024-03-10T21:05:27+08:00">2024-03-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">5.3k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>11 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>免模型学习的价值函数计算方法</p><ul><li><p>蒙特卡洛方法：将策略迭代中，基于模型的部分替换为免模型部分</p><p>更新时间：2024-2-24 10:28:04</p></li><li><p>时序差分方法</p><p>更新时间：2024-2-27 23:34:53</p></li></ul><p>[TOC]</p><span id="more"></span><h2 id="3-1-蒙特卡洛方法——免模型"><a href="#3-1-蒙特卡洛方法——免模型" class="headerlink" title="3.1 蒙特卡洛方法——免模型"></a>3.1 蒙特卡洛方法——免模型</h2><blockquote><p>基于蒙特卡洛方法的免模型学习是一种 <strong>广义策略迭代</strong> (generalized policy iteration, GPI)</p><ul><li>不断地在 <em>策略评估, PE</em> 与 <em>策略改进, PI</em> 之间切换</li><li>策略评估不需要精确的 $Q$ 值与 $V$ 值</li></ul></blockquote><ul><li><p>MC基本思想</p></li><li><p>MC Basic：实现从有模型学习到无模型学习的转化</p></li><li>MC探索性开始<ul><li>提高数据的使用效率</li></ul></li><li>去除探索性开始条件<ul><li>MC $\varepsilon-贪心$</li></ul></li></ul><h3 id="3-1-1-蒙特卡洛基本思想"><a href="#3-1-1-蒙特卡洛基本思想" class="headerlink" title="3.1.1 蒙特卡洛基本思想"></a>3.1.1 蒙特卡洛基本思想</h3><p>掷硬币，用随机变量 $X$ 表示结果，$+1,-1$ 表示两种结果，求期望 $E[X]$</p><ul><li><p>基于模型的方法</p><p>当我们已知硬币是均匀的，或任何能保证得出 $P(X=+1)=P(X=-1)=0.5$ 的条件，在这种先验知识的条件下，我们可以对实验环境进行建模，用 $P(X=+1)=P(X=-1)=0.5$ 表示环境模型，进而可基于模型的方法估计期望</p><p>$E[X]=\sum\limits_{x}xP(x)=0$</p></li><li><p>基于MC的方法：<strong>采样取平均</strong></p><p>在不知道任何先验知识的前提下，就无法对实验环境进行建模</p><p>无模型时用数据(经验,experience)：通过多次实验，可以得到样本集 $\{x_1,x_2,\cdots,x_N\}$</p><script type="math/tex;mode=display">E[X]\approx \overline{X}=\frac{1}{N}\sum\limits_{j=1}^Nx_j</script><p><img src="/posts/1184554208/image-20240223105012439.png" alt="image-20240223105012439"></p><p>可见，随着实验次数的增多，$\overline{X}\xrightarrow{N\rightarrow \infty}E[X]$</p></li></ul><h4 id="MC方法理论依据——大数定律"><a href="#MC方法理论依据——大数定律" class="headerlink" title="MC方法理论依据——大数定律"></a>MC方法理论依据——大数定律</h4><p>对于一个随机变量 $X$ ，假设 $\{x_j\}_{j=1}^{N}$ 是独立同分布样本集，令 $\overline{X}=\frac{1}{N}\sum\limits_{j=1}^Nx_j$ ，则有，</p><script type="math/tex;mode=display">E[\overline{X}]=E[X]\\
Var[\overline{X}]=\frac{1}{N}Var[X]</script><p>即 $\overline{X}$ 是 $E[X]$ 的无偏估计，且方差随着实验次数的增多减小</p><h3 id="3-1-2-MC-Basic"><a href="#3-1-2-MC-Basic" class="headerlink" title="3.1.2 MC Basic"></a>3.1.2 MC Basic</h3><h4 id="策略迭代分析"><a href="#策略迭代分析" class="headerlink" title="策略迭代分析"></a>策略迭代分析</h4><p>PE：$\mathbf{V}_{\pi^{(t)}}=\mathbf{R}_{\pi^{(t)}}+\gamma P_{\pi^{(t)}}\mathbf{V}_{\pi^{(t)}}$</p><p>PI：$\pi^{(t+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(t)}}\right)$</p><p>对于策略改进步骤，针对每个状态 $s\in \mathcal{S}$</p><script type="math/tex;mode=display">\begin{aligned}
\pi^{(t+1)}(a\vert s)&=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)\left[\sum\limits_{r'}P(r'\vert s,a)r'+\sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(t)}}(s')\right]\\
&=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(t)}}(s,a)
\end{aligned}</script><p>若环境动态特性 $P(s’,r\vert s,a)$ 已知，可通过对动态规划方法计算 $Q_{\pi^{(t)}}(s,a)$</p><p>若环境动态特性 $P(s’,r\vert s,a)$ 未知，则采用蒙特卡洛方法计算</p><script type="math/tex;mode=display">Q_{\pi^{(t)}}(s,a)=E_{\pi^{(t)}}[G_{t}\vert s_t=s,a_t=a]\\
\downarrow\\
均值估计问题\leftarrow MC方法</script><h4 id="MC-Basic算法"><a href="#MC-Basic算法" class="headerlink" title="MC Basic算法"></a>MC Basic算法</h4><ol><li><p>从 $(s,a)$ 开始，基于策略 $\pi^{(t)}$ 生成轨迹 $\tau$</p></li><li><p>计算 $\tau$ 的回报 $g(s,a)$</p><script type="math/tex;mode=display">g^{(j)}(s,a)=r(s,a)+\gamma r(s_{t+1},a_{t+1})+\gamma^2r(s_{t+2},a_{t+2})+\cdots</script><ul><li>若采用确定性策略，且环境也是确定的。若估计 $Q(s_1,a_1)$ ，则基于当前策略 $\pi^{(t)}$ 从 $(s_1,a_1)$ 出发，不管采样多少次，得到的轨迹都是同一条，故只需要采样一次。因此估计整个Q表只需要 $\sum\limits_{s\in \mathcal{S}} \vert \mathcal{A}(s)\vert$ 条轨迹，即 $N=1$</li><li>若采用随机性策略或确定性策略但环境是随机的。若估计 $Q(s_1,a_1)$ ，则基于当前策略 $\pi^{(t)}$ 从 $(s_1,a_1)$ 出发，需要采集从 $(s_1,a_1)$ 出发的 $N$ 条轨迹才能估计其状态价值 $Q(s_1,a_1)$ 。因此估计整个Q表共需计算 $\sum\limits_{s\in \mathcal{S}} \vert \mathcal{A}(s)\vert\times N$ 条轨迹的回报，即 $N$ 为足够大的数值</li></ul></li><li><p>执行 $N$ 次 1,2，可得到一个回报集 $\{g^{(j)}(s,a)\}_{j=1}^N$ ，计算动作价值</p><script type="math/tex;mode=display">Q_{\pi^{(t)}}(s,a)=E_{\pi^{(t)}}[G_{t}\vert s_t=s,a_t=a]\approx\overline{g}(s,a)=\frac{1}{N}\sum\limits_{j=1}^Ng^{(j)}(s,a)</script></li></ol><h5 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h5><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：初始化策略 \pi^{(0)}\\
&目标：寻找最优策略\\
&当价值函数未收敛，进行第 t 轮迭代:\\
&\quad 对于每个状态 s\in \mathcal{S}:\\
&\qquad 对于每个动作a\in \mathcal{A}(s):\\
&\qquad \quad MC-based 策略评估:\\
&\qquad \qquad 以(s,a) 为起点，基于策略\pi^{(t)} 生成足够多的回合，计算这些回合的回报g(s,a)\\
&\qquad \qquad Q_{\pi^{(t)}}(s,a)=\overline{g}(s,a)\\
&\qquad MC-based 策略改进：\\
&\qquad \quad \pi^{(t+1)}=\begin{cases}
1&,a=a^{(t)}_*\\
0&,a\neq a^{(t)}_*
\end{cases}\qquad a^{(t)}_*=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(t)}}(s,a) 
\\
\hline
\end{array}</script><h4 id="MC-Basic例子"><a href="#MC-Basic例子" class="headerlink" title="MC Basic例子"></a>MC Basic例子</h4><p><img src="/posts/1184554208/image-20240223155537861.png" alt="image-20240223155537861"></p><ul><li>初始策略 $\pi^{(0)}$</li><li>奖励设置 $r_{boundary}=-1,r_{forbidden}=-1,r_{target}=1,\gamma=0.9$</li></ul><p>对于当前策略 $\pi^{(t)}$</p><ol><li><p>策略评估：计算 $Q_{\pi^{(t)}}(s,a)$</p><p>对于示例，有9个状态，每个状态由5个动作，即 $Q$ 表需要计算 $9\times 5=45$ 个 $Q$ 值，由于此例采用确定性策略，所以只需要45条轨迹</p></li><li><p>策略改进：贪心算法选择最优决策</p><script type="math/tex;mode=display">a_*^{(t)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(t)}}(s,a)</script></li></ol><p>以 $Q_{\pi^{(0)}}(s_1,a)$ 的计算为例</p><p><strong>1. 策略评估</strong></p><p>从 $(s_1,a_1)$ 开始，轨迹为 $s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_1)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{1}{1-\gamma}</script><p>从 $(s_1,a_2)$ 开始，轨迹为 $s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9\xrightarrow{a_5}s_9\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_2)=0+\gamma(0)+\gamma^2(0)+\gamma^3(1)+\cdots=\frac{\gamma^3}{1-\gamma}</script><p>从 $(s_1,a_3)$ 开始，轨迹为 $s_1\xrightarrow{a_3}s_4\xrightarrow{a_2}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9\xrightarrow{a_5}s_9\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_3)=0+\gamma(0)+\gamma^2(0)+\gamma^3(1)+\cdots=\frac{\gamma^3}{1-\gamma}</script><p>从 $(s_1,a_4)$ 开始，轨迹为 $s_1\xrightarrow{a_4}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_4)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{1}{1-\gamma}</script><p>从 $(s_1,a_5)$ 开始，轨迹为 $s_1\xrightarrow{a_5}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_5)=0+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{\gamma}{1-\gamma}</script><p><strong>2. 策略改进</strong></p><p>通过观察动作价值，可知在 $s_1$ 时，最佳动作是 $a_2,a_3$</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_2)=Q_{\pi^{(0)}}(s_1,a_3)</script><p>因此，策略改进为</p><script type="math/tex;mode=display">\pi^{(1)}(a_2\vert s_1)或\pi^{(1)}(a_3\vert s_1)=1</script><h4 id="MC-Basic特点"><a href="#MC-Basic特点" class="headerlink" title="MC Basic特点"></a>MC Basic特点</h4><p>MCBasic算法是MC-based RL的核心，但由于其数据利用效率低，所以并不实用</p><p>因为策略迭代算法是收敛的，所以有足够回合数据的MC Basic也是收敛的</p><p>回合长度，<strong>直观上可以理解为探索半径</strong>。理论上越长越好，可以使计算出的回报更为精确，但实际上不可能无穷长，只要能让所有状态到达目标状态即可</p><h5 id="关于回合长度的例子"><a href="#关于回合长度的例子" class="headerlink" title="关于回合长度的例子"></a>关于回合长度的例子</h5><ul><li>奖励设置 $r_{boundary}=-1,r_{forbidden}=-10,r_{target}=1,\gamma=0.9$</li></ul><p>通过当前策略可到达目标状态的当前状态，其状态价值是正数；0表示未到达目标，若为负数，则表示有进入禁入区域或越界的中间状态</p><p><img src="/posts/1184554208/image-20240223165057968.png" alt="image-20240223165057968"></p><p>当回合长度为1时，只有目标状态一步之内的状态的最佳策略是正确的</p><p>当回合长度短时，只有离目标状态近的状态才有非0的状态价值，进而找到最优的策略；随着回合长度的增加，离目标状态远的状态才能到达目标状态，从而找到最优策略</p><p><img src="/posts/1184554208/image-20240223165703357.png" alt="image-20240223165703357"></p><p><img src="/posts/1184554208/image-20240223165756497.png" alt="image-20240223165756497"></p><h3 id="3-1-3-探索性开始"><a href="#3-1-3-探索性开始" class="headerlink" title="3.1.3 探索性开始"></a>3.1.3 探索性开始</h3><h4 id="探索性开始的必要性"><a href="#探索性开始的必要性" class="headerlink" title="探索性开始的必要性"></a>探索性开始的必要性</h4><p><strong>探索</strong>：确保每个 $(s,a)$ 都能被访问到</p><ul><li>需要保证从一个访问出发有多个回合，只有这样才能用后边的奖励来估计回报，进一步估计动作价值</li><li>同时，为了不遗漏最优决策，需要保证每个$(s,a)$ 都能够被访问到</li></ul><p><strong>开始</strong> ：要生成以 $(s,a)$ 开始的回合</p><ul><li><p>若要计算以 $(s,a)$ 开始的回合的回报，有两种方法</p><ul><li>start：计算以 $(s,a)$ 开始的回合</li></ul></li></ul><ul><li><p>visit 从其他 $(s’,a’)$ 开始，中间经过 $(s,a)$ ，将回合前截断，计算 $(s,a)$ 之后的回报</p><p>但由于轨迹的生成依赖于策略与环境，无法保证以 $(s,a)$ 开始仍能获取后续轨迹，即不保证可复现</p></li></ul><h4 id="提高策略评估的效率"><a href="#提高策略评估的效率" class="headerlink" title="提高策略评估的效率"></a>提高策略评估的效率</h4><blockquote><p>访问：在每个回合中，每出现一次 $(s_t,a_t)$ 都为对其的一次访问</p></blockquote><ul><li>数据的高效利用</li><li>策略更新时机</li></ul><h5 id="数据的高效利用"><a href="#数据的高效利用" class="headerlink" title="数据的高效利用"></a>数据的高效利用</h5><p>一个以 $(s_1,a_2)$ 开始的回合，仅用于计算 $Q(s_1,a_2)$ ，并为充分利用这一经验</p><p><strong>一个回合可以作为多个Q值的经验</strong></p><p><img src="/posts/1184554208/image-20240222174121229.png" alt="image-20240222174121229"></p><p>采用 <em>first-visit</em> 方法，对于出现在同一个回合中的多个同一访问 $(s_t,a_t)$ ，只将其第一次出现之后的轨迹作为 $(s_t,a_t)$ 的一条轨迹</p><p>采用 <em>every-visit</em> 方法，对于出现在同一个回合中的多个同一访问 $(s_t,a_t)$ ， 将每个 $(s_t,a_t)$ 之后的轨迹都作为 $(s_t,a_t)$ 的一条轨迹</p><h5 id="策略更新时机——离线变在线"><a href="#策略更新时机——离线变在线" class="headerlink" title="策略更新时机——离线变在线"></a>策略更新时机——离线变在线</h5><ul><li><p>在策略评估中，若评估 $(s_1,a_1)$ 的动作价值 $Q(s_1,a_1)$ ，则需要收集 $N$ 条以 $(s_1,a_1)$ 开始的轨迹，然后计算这些轨迹的回报均值去近似动作价值</p><p>被 MC Basic 算法采用，缺点是需要等所有的回合收集结束后再开始计算均值</p></li><li><p>使用一个回合的回报在线更新动作价值</p><p>优点：可以在线逐回合改进策略</p></li></ul><h4 id="伪代码：MC探索性开始"><a href="#伪代码：MC探索性开始" class="headerlink" title="伪代码：MC探索性开始"></a>伪代码：MC探索性开始</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：初始化策略\pi^{(0)},Q(s,a),R(s,a)=0,Num(s,a)=0,\forall s\in \mathcal{S},a\in \mathcal{A}(s)\\
&目标：寻找最优策略\\
&\\
&对于每个回合:\\
&\quad生成回合:随机选择回合起点(s_0,a_0)，并确保所有的访问都有可能被选择到。\\
&\quad \qquad\qquad 基于当前策略 \pi^{(t)}生成回合长度为T的回合:s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1}\\
&\quad 初始化:g\leftarrow 0\\
&\quad 对于一个回合的每一步，t=T-1,T-2,\cdots,0:\\
&\qquad g\leftarrow \gamma g+r_{t+1}\\
&\qquad 使用first-visit方法:\\
&\quad \qquad若(s_t,a_t)未出现在前子轨迹(s_0,a_0,s_1,a_1,\cdots,s_{t_1},a_{t-1}),则:\\
&\qquad \qquad R(s_t,a_t)\leftarrow R(s_t,a_t)+g //采用第一次访问策略，只有第一次出现才算(s_t,a_t)的经验\\
&\qquad \qquad Num(s_t,a_t)\leftarrow Num(s_t,a_t)+1\\
&\qquad \qquad 策略评估：\\
&\quad\qquad \qquad Q(s_t,a_t)=average(R(s_t,a_t))\\
&\qquad \qquad 策略改进：\\
&\quad \qquad \qquad \pi^{(t+1)}(a^*\vert s_t)=1,其中a^*=\mathop{\mathrm{argmax}}\limits_{a}Q(s_t,a) 
\\
\hline
\end{array}</script><h3 id="3-1-4-MC无探索性开始"><a href="#3-1-4-MC无探索性开始" class="headerlink" title="3.1.4 MC无探索性开始"></a>3.1.4 MC无探索性开始</h3><p>实际中，探索性开始很难实现。以每个 $(s,a)$ 为开始的回合收集成本很高</p><blockquote><p>软策略,soft policy：每个动作都有概率被采用的策略，称为soft policy</p><script type="math/tex;mode=display">\begin{cases}
确定性策略\leftarrow 贪心算法\\
随机性策略\leftarrow 软策略
\end{cases}</script></blockquote><p>soft policies：去除探索性开始的条件</p><ul><li>使用软策略，可以确保一些回合足够长，使得所有的 $(s,a)$ 都能出现足够的次数，只需要从一个或几个 $(s,a)$ 出发，就能覆盖到其他的访问，从而不需要大量的回合数据</li></ul><h4 id="varepsilon-贪心"><a href="#varepsilon-贪心" class="headerlink" title="$\varepsilon-贪心$"></a>$\varepsilon-贪心$</h4><p>在强化学习中，使用的软策略为 $\varepsilon-贪心$ 算法</p><script type="math/tex;mode=display">\begin{aligned}
&\pi(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,对于最优动作\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,对于其他 \vert \mathcal{A}(s)\vert-1个非最优动作
\end{cases}\\
&其中，\varepsilon\in[0,1]，\vert \mathcal{A}\vert(s)为状态s的动作数
\end{aligned}</script><ul><li>若 $\varepsilon=0.2$ ，$\vert \mathcal{A}\vert(s)=5$ ，则非最优动作的概率为 $\frac{0.2}{5}=0.04$ ，最优动作的概率为 $1-0.04\times 4=0.84$</li></ul><p>当前状态 $s$ 下的最优决策有最大可能被选择，同时其他非最优决策也有一定的概率被选择到</p><p>最优决策被选择的概率比非最优决策被选择的概率大</p><ul><li>$1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)=1-\varepsilon+\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}\ge \frac{\varepsilon}{\vert \mathcal{A}\vert(s)}$</li></ul><p>$\varepsilon-贪心$ 可以更好地平衡探索与利用</p><ul><li>$\varepsilon=0$ ，变为贪心策略，探索少但利用多</li><li>$\varepsilon=1$ ，每个动作都被平等探索，探索多但利用少</li></ul><h4 id="将-varepsilon-贪心-应用于强化学习"><a href="#将-varepsilon-贪心-应用于强化学习" class="headerlink" title="将 $\varepsilon-贪心$ 应用于强化学习"></a>将 $\varepsilon-贪心$ 应用于强化学习</h4><p>在MC Basic与MC探索性开始算法中，将策略改进步骤的贪心算法改为 $\varepsilon-贪心$ 算法</p><ul><li><p>贪心算法 $\pi^{(t+1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(t)}}(s,a)$</p><script type="math/tex;mode=display">\pi^{(t+1)}(a\vert s)=\begin{cases}
1,&a=a^{(t)}_*\\
0,&a\neq a^{(t)}_*
\end{cases}\\
其中,a^{(t)}_*=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}Q_{\pi^{(t)}}(s,a)</script></li><li><p>$\varepsilon-贪心$ 算法 $\pi^{(t+1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi_{\varepsilon}}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(t)}}(s,a)$</p><script type="math/tex;mode=display">\pi^{(t+1)}(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,a=a^{(t)}_*\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,a\neq a^{(t)}_*
\end{cases}</script></li></ul><p>关于 $\Pi$ 与 $\Pi_{\varepsilon}$ 的理解</p><ul><li>$\Pi_{\varepsilon}$ 表示除最优决策外其他决策等可能的策略</li><li>$\Pi$ 表示所有可能的策略</li></ul><h5 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h5><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：初始化策略\pi^{(0)},\varepsilon\in [0,1],,Q(s,a),R(s,a)=0,Num(s,a)=0,\forall s\in \mathcal{S},a\in \mathcal{A}(s)\\
&目标：寻找最优策略\\
&\\
&对于每个回合:\\
&\quad 生成回合:随机选择回合起点(s_0,a_0),基于当前策略 \pi^{(t)}生成回合长度为T的回合:s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1}\\
&\quad 初始化:g\leftarrow 0\\
&\quad 对于一个回合的每一步，t=T-1,T-2,\cdots,0:\\
&\qquad g\leftarrow \gamma g+r_{t+1}\\
&\qquad 使用every-visit方法:\\
&\qquad \qquad R(s_t,a_t)\leftarrow R(s_t,a_t)+g\\
&\qquad \qquad Num(s_t,a_t)\leftarrow Num(s_t,a_t)+1\\
&\qquad \qquad 策略评估:\\
&\quad\qquad \qquad Q(s_t,a_t)=average(R(s_t,a_t))\\
&\qquad \qquad 策略改进:\\
&\quad\qquad \qquad 令a_*^{(t)}=\mathop{\mathrm{argmax}}\limits_{a}Q(s_t,a)，且\pi^{(t+1)}(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,a=a_*^{(t)}\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,a\neq a_*^{(t)}
\end{cases}\\
\hline
\end{array}</script><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>优点： $\varepsilon$ 越大，探索性越强，避开了探索性开始的条件</p><p>缺点：生成的最优策略的最优性越差</p><ul><li><p>由于 $\varepsilon-贪心$ 生成的最优策略 $\pi^*_{\varepsilon}$ 仅是属于 $\Pi_{\varepsilon}$ 中的最优策略</p></li><li><p><strong>$\varepsilon$不能太大</strong> ：当 $\varepsilon$ 小时，探索性小， $\varepsilon-贪心$ 就趋于贪心，用 $\varepsilon-贪心$ 找到的最优策略接近于贪心算法找到的最优策略</p><p><strong>实际中，开始时 $\varepsilon$ 比较大，让其具有较强的探索能力；之后让 $\varepsilon$ 趋向于0确保得到的策略具有最优性</strong></p></li></ul><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><h5 id="varepsilon-越大，探索性越强"><a href="#varepsilon-越大，探索性越强" class="headerlink" title="$\varepsilon$ 越大，探索性越强"></a>$\varepsilon$ 越大，探索性越强</h5><p><img src="/posts/1184554208/image-20240224102256712.png" alt="image-20240224102256712"></p><p>共125个 $(s,a)$，$\varepsilon$ 较大，探索性强，一个100 0000步的回合就探索了很多(s,a)</p><p>两个1000000步的回合就能找到基于 $\varepsilon-贪心$ 的最优策略</p><p><img src="/posts/1184554208/image-20240224094801791.png" alt="image-20240224094801791"></p><h5 id="牺牲最优性"><a href="#牺牲最优性" class="headerlink" title="牺牲最优性"></a>牺牲最优性</h5><p>$\varepsilon-贪心$ 通过探索性得到了一些好处，但同时牺牲了一些状态的最优性</p><p>$r_{boundary}=-1,r_{forbidden}=-10,r_{target}=1,\gamma=0.9$</p><p><img src="/posts/1184554208/image-20240222233322816.png" alt="image-20240222233322816"></p><p>一致性：在每个状态下，采取最优动作的可能性最大</p><p><img src="/posts/1184554208/image-20240224101526505.png" alt="image-20240224101526505"></p><p><img src="/posts/1184554208/image-20240224101639774.png" alt="image-20240224101639774"></p><p>最优策略是基于最优状态价值定义的，所以最优状态价值在反映策略的好坏</p><p>虽然所有的策略都与最优策略保持一致，但由于还可能采取其他非最优动作，所以状态价值会比贪心算法的最优状态价值小，其最优性越来越差</p><p>在实际应用中，我们会将基于 $\varepsilon-贪心$ 算法的最优策略转为基于贪心算法的策略，希望这个策略与最优策略保持一致。但随着 $\varepsilon$ 的增大，二者已不具有一致性，所以 $\varepsilon$ 不能太大。一个技巧就是$\varepsilon$ 先大后小</p><h2 id="3-2-随机近似与随机梯度下降"><a href="#3-2-随机近似与随机梯度下降" class="headerlink" title="3.2 随机近似与随机梯度下降"></a>3.2 随机近似与随机梯度下降</h2><blockquote><p><strong>时序差分方法</strong> 是随机近似的一种特殊情况</p></blockquote><ol><li><p>均值近似的不同理解</p><p>使用数据集 $\{x_t\}$ 估计均值 $E[X]$</p><script type="math/tex;mode=display">w_{t+1}=w_t-\frac{1}{t}(w_t-x_t)</script></li><li><p>Robbins-Monro 算法，RM算法</p><p>使用观测集 $\{\tilde{g}(w_t,\eta_t)\}$ 估计未知方程 $g(w)=0$ 的解</p><script type="math/tex;mode=display">w_{t+1}=w_t-\alpha_t\tilde{g}(w_t,\eta_t)</script><ul><li>算法描述</li><li>示例</li><li>收敛性分析</li><li>应用于均值近似</li></ul></li><li><p>随机梯度下降——RM算法的一种特殊情况</p><p>使用梯度集 $\{\bigtriangledown_wf(w_t,x_t)\}$ 求解最优化问题 $\min\limits_{w}J(w)=\min\limits_{w}E[f(w,x)]$ 的最优解，令 $g(w)=E[\bigtriangledown_wf(w,x)]=0$ ，梯度 $\bigtriangledown_wf(w_t,\eta_t)$ 相当于期望的带误差观测</p><script type="math/tex;mode=display">w_{t+1}=w_t-a_t\bigtriangledown_wf(w_t,\eta_t)</script><ul><li>算法描述</li><li>示例</li><li>收敛性</li><li>确定性公式</li><li>BGD，MBGD，SGD</li></ul></li></ol><h3 id="3-2-1-均值近似的不同理解"><a href="#3-2-1-均值近似的不同理解" class="headerlink" title="3.2.1 均值近似的不同理解"></a>3.2.1 均值近似的不同理解</h3><blockquote><p>对于一个随机变量，目标是估计其期望 $E[X]$ ，假设有一组独立同分布的数据集 $\{x_i\}_{i=1}^N$ ，期望可以用其均值近似 $E[X]\approx \overline{X}=\frac{1}{N}\sum\limits_{i=1}^Nx_i$</p></blockquote><p>强化学习中有很多期望，除价值函数外，还有很多量需要用数据去估计</p><h4 id="增量式均值计算方法"><a href="#增量式均值计算方法" class="headerlink" title="增量式均值计算方法"></a>增量式均值计算方法</h4><ul><li>收集所有的轨迹，再计算其均值：必须等 $N$ 条数据全部都收集完后，才能取平均</li><li>增量式(迭代式)</li></ul><p>对于非增量式计算</p><script type="math/tex;mode=display">w_{t+1}=\frac{1}{t}\sum\limits_{i=1}^tx_i,t=1,2,\cdots\\
w_{t}=\frac{1}{t-1}\sum\limits_{i=1}^{t-1}x_i,t=2,3\cdots</script><p>因此有</p><script type="math/tex;mode=display">\begin{aligned}
w_{t+1}&=\frac{1}{t}\sum\limits_{i=1}^tx_i=\frac{1}{t}\left(\sum\limits_{i=1}^{t-1}x_i+x_t\right)\\
&=\frac{1}{t}((t-1)w_{t}+x_t)\\
&=w_t-\frac{1}{t}(w_t-x_t)
\end{aligned}</script><p>利用上式可以增量式计算均值 $\overline{X}$</p><script type="math/tex;mode=display">\begin{array}{ll}
当有一个数据 &w_1=x_1\\
当有一个数据 &w_2=x_1\\
当有两个数据 &w_3=w_2-\frac{1}{2}(w_2-x_2)=\frac{1}{2}(x_1+x_2)\\
当有三个数据 &w_4=w_3-\frac{1}{3}(w_3-x_3)=\frac{1}{3}(x_1+x_2+x_3)\\
&\vdots\\
当有t个数据 &w_{t+1}=w_t-\frac{1}{t}(w_t-x_t)=\frac{1}{t}(x_1+x_2+\cdots+x_t)=\frac{1}{t}\sum\limits_{i=1}^tx_i
\end{array}</script><h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul><li>增量式均值估计每采样一次都能更新一次</li><li>均值估计一开始由于数据量不充分，所以并不精确 $(w_{t+1\neq E[X]})$ 。但随着样本数的增多，估计会越来越准确 $(w_{t+1}\xrightarrow{t\rightarrow N}E[X])$</li></ul><h4 id="广义增量式均值估计"><a href="#广义增量式均值估计" class="headerlink" title="广义增量式均值估计"></a>广义增量式均值估计</h4><script type="math/tex;mode=display">w_{t+1}=w_{t}-\alpha_{t}(w_{t}-x_t)</script><p>将 $\frac{1}{t}$ 替换为 $\alpha_t$ ，仍可收敛于 $E[X]$</p><ul><li>属于特殊的 <strong>随机近似算法</strong></li><li>属于 <strong>随机梯度下降算法</strong></li></ul><h3 id="3-2-2-RM算法"><a href="#3-2-2-RM算法" class="headerlink" title="3.2.2 RM算法"></a>3.2.2 RM算法</h3><p>随机近似算法 (stochastic approximation, SA) ：用于方程求解或最优化问题的一类随机的、迭代的算法</p><ul><li>相较于其他方程求解算法(梯度上升/下降算法)，随机近似算法不需要知道方程的具体形式<ul><li>方程的表达式 $g$ 是已知的，可以通过很多数值算法求解</li><li>方程的表达式 $g$ 是未知的（神经网络：已知输入输出，但不知道具体的网络结构）</li></ul></li></ul><p>Robbins-Monro算法：是SA中的一种代表性算法</p><ul><li>随机梯度下降算法、增量式均值估计都属于RM算法</li></ul><p>方程求解问题应用于最优化求解：求解 $J(w)$ 的最优化问题，利用梯度下降法求解 $g(w)=\bigtriangledown_w J(w)=0$ 也是一个方程求解问题</p><p><strong>问题定义</strong></p><p>求解表达式未知的方程的根</p><script type="math/tex;mode=display">g(w)=0</script><p>其中，$w$ 为变量，$g(\cdot)$ 为表示未知方程的函数，$w^*$ 为方程的解</p><p><strong>求解</strong></p><p>RM算法是一种迭代式算法</p><script type="math/tex;mode=display">\begin{aligned}
w_{t+1}&=w_{t}-\alpha_t\widetilde{g}(w_{t},\eta_{t})\\
&=w_t-\alpha_t[g(w_{t})+\eta_{t}]
\end{aligned}</script><ul><li>$w_{t}$ 表示对 $w$ 的第 $t$ 次迭代</li><li>函数 $g(\cdot)$ 是一个黑盒函数， $g(w_t)$ 的准确值是未知的，但我们可以观测到关于 $g(w_t)$ 的带有噪音的观测值 $\tilde{g}(w_t,\eta_t)$</li></ul><p>通过RM算法求解方程的解依赖于数据</p><ul><li>输入序列：$\{w_{t}\}$</li><li>带噪音的输出序列：$\{\widetilde{g}(w_{t},\eta_{t})\}$</li></ul><h4 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h4><h5 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h5><p>估计 $g(w)=\tanh(w-1)=0$ 的解，初始 $w_1=3,a_t=\frac{1}{t},\eta_t\equiv0$ (为简化计算，假设无噪音干扰)</p><p><img src="/posts/1184554208/Figure_1.png" alt="Figure_1"></p><p>有解析解 $w^*=1$</p><p>由于噪音是0，所以本例中 $\tilde{g}(w_t,\eta_t)=g(w_t)$ ，基于以下公式计算</p><script type="math/tex;mode=display">w_{t+1}=w_t-\alpha_tg(w_t)</script><p><img src="/posts/1184554208/image-20240225111350459.png" alt="image-20240225111350459"></p><p>$w_2=w_1=3$ ，$w_3=w_2-a_2g(w_2)=3-\frac{1}{2}\tanh(3)=3-\frac{0.9950547536867305}{2}=2.5024726231566348$</p><p>直观上看，$w_{t+1}$ 会随着 $t$ 的迭代不断接近 $w^*$</p><ul><li>当 $w_t&gt;w^*$ 时，有 $g(w_t)&gt;0$ ， $w_{t+1}=w_t-\alpha_tg(w_t)&lt;w_t$</li><li>当 $w_t<w^*$ 时，有 $g(w_t)<0$ ， $w_{t+1}="w_t-\alpha_tg(w_t)">w_t$</li></ul><p>因此，$w_{t+1}$ 比 $w_t$ 更靠近 $w^*$</p><h5 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h5><p>RM算法 $w_{t+1}=w_{t}-\alpha_t\widetilde{g}(w_{t},\eta_{t})$​ 收敛的充分条件是：</p><blockquote><ul><li>$0&lt; c_1\le \bigtriangledown_w g(w)\le c_2,\forall w$</li><li>$\sum\limits_{t=1}^{\infty}\alpha_t=\infty$ 且 $\sum\limits_{t=1}^{\infty}\alpha_t^2&lt;\infty$</li><li>$E[\eta_t\vert \mathcal{H}_t]=0$ 且 $E[\eta_t^2\vert \mathcal{H}_t]&lt;\infty$ ，$\mathcal{H}_t=\{w_t,w_{t-1,\cdots}\}$</li></ul><p>则 $w_t$ 依概率收敛于 $w^<em>$ 为方程的解 $g(w^</em>)=0$</p></blockquote><p><strong>1. $0&lt; c_1\le \bigtriangledown_w g(w)\le c_2,\forall w$ 的条件分析</strong></p><ul><li>导数大于0的条件：确保单调增，一定会有0点</li><li>导数有界，避免无穷的情况</li></ul><p>为什么单调增的 $g(w)$ 是可以接受的：随机近似常用于最优化方程的求解，即 $g(w)=\bigtriangledown_w J(w)=0$ ，对于凸优化问题，$J’’(w)\ge 0\iff g’(w)&gt;0$ ，所以是可接受的</p><p><strong>2. $E[\eta_t\vert \mathcal{H}_t]=0$ 且 $E[\eta_t^2\vert \mathcal{H}_t]&lt;\infty,\mathcal{H}_t=\{w_t,w_{t-1,\cdots}\}$</strong></p><p>噪音 $\{\eta_t\}$ 是一个独立同分布的随机序列，其均值应该为0($E[\eta_t\vert \mathcal{H}_t]=0$)，且方差应是有界的($E[\eta_t^2\vert \mathcal{H}_t]&lt;\infty,\mathcal{H}_t=\{w_t,w_{t-1,\cdots}\}$)，且 $\eta$ 并不需要服从高斯分布</p><p><strong>3. $\sum\limits_{t=1}^{\infty}\alpha_t=\infty$ 且 $\sum\limits_{t=1}^{\infty}\alpha_t^2&lt;\infty$ 的条件分析</strong></p><ul><li><p>$\sum\limits_{t=1}^{\infty}\alpha_t^2&lt;\infty$ 确保 $\alpha_t\xrightarrow{t\rightarrow \infty}0$</p><p>由于 $w_{t+1}-w_{t}=-\alpha_t\tilde{g}(w_{t},\eta_{t})$ ，若 $\alpha_t\rightarrow 0\Rightarrow \alpha_t\tilde{g}(w_{t},\eta_{t})\rightarrow 0\Rightarrow w_{t+1}-w_{t}\rightarrow 0$ ，若想要 $w_t$ 收敛，则 $w_{t+1}-w_{t}\rightarrow 0$ 是一个充分条件条件</p><p>若 $w_t\rightarrow w^*$，则有 $g(w_t)\rightarrow 0$ 并且 $\tilde{g}(w_{t},\eta_{t})$ 受 $\eta_t$ 扰动</p></li><li><p>$\sum\limits_{t=1}^{\infty}\alpha_t=\infty$ 确保 $a_t$ 趋于0的速度不会过快</p><p>由上，$w_2=w_1-a_1\tilde{g}(w_1,\eta_1)$ ，$w_3=w_2-a_2\tilde{g}(w_2,\eta_2)$ ，$\cdots ,w_{t+1}=w_t-\alpha_t\tilde{g}(w_t,\eta_t)$ ，等号左右分别求和</p><script type="math/tex;mode=display">w_{t+1}-w_1=\sum\limits_{t=1}^{t}-a_t\tilde{g}(w_t,\eta_t)</script><p>若 $w_{\infty}=w^<em>$ ，假设 $\sum\limits_{t=1}^{\infty}\alpha_t&lt;\infty$ ，则 $\sum\limits_{t=1}^{\infty}\alpha_t\tilde{g}(w_t,\eta_t)$ 是有界的，则 $w^</em>$ 与 $w_{1}$ 的差距是有界的，意味着不能随便选择初始的 $w_1$ 。若选择的 $w_1$ 与精确值间距过大，超过这个界，则等式不成立，即 $w_{\infty}$ 不是方程的解</p><p>当 $\sum\limits_{t=1}^{\infty}\alpha_t=\infty$ ，表示选择的初始 $w_1$ 不管离 $w^*$ 有多远，最后都能收敛到精确值</p></li></ul><h5 id="满足系数的条件的-a-t"><a href="#满足系数的条件的-a-t" class="headerlink" title="满足系数的条件的 $a_t$"></a>满足系数的条件的 $a_t$</h5><p>典型的是 $a_t=\frac{1}{t}$ ，级数 $\sum\limits_{t=1}^n\frac{1}{t}$ 在 $n\rightarrow \infty$ 时是发散的，且 $\sum\limits_{t=1}^{\infty}\frac{1}{t^2}=\frac{\pi^2}{6}&lt;\infty$</p><p>在强化学习中，$\alpha_t$ 会选择足够小的常量，不会使用 $\frac{1}{t}$ ，这样会使 $\sum\limits_{t=1}^{\infty}\alpha_t^2&lt;\infty$ 不成立，但算法依然会是有效的</p><ul><li>后续的数据，随 $t$ 的增大，其作用会非常小。在实际问题中，我们希望后续的数据也有用，所以不会让 $\alpha_t\xrightarrow{t\rightarrow \infty}0$ ，而让其 $\alpha_t\xrightarrow{t\rightarrow \infty}\varepsilon$ ，$\varepsilon$ 是一个非常小的数</li></ul><h4 id="RM算法应用于均值估计"><a href="#RM算法应用于均值估计" class="headerlink" title="RM算法应用于均值估计"></a>RM算法应用于均值估计</h4><script type="math/tex;mode=display">w_{t+1}=w_t-\alpha_t(w_t-x_t)</script><p>我们知道若 $\alpha_t=\frac{1}{t}$ ，则 $w_{t+1}=\frac{1}{t}\sum\limits_{i=1}^tx_i\xrightarrow{t\rightarrow \infty}E[X]$ 。但 $\alpha_t\neq\frac{1}{t}$ 时，$w_t$ 是否仍能收敛与期望 $E[X]$ 目前并未分析过。基于RM算法，只需证明 <strong>广义增量式均值估计</strong> 属于RM算法即可</p><p>在均值估计中，$g(w)=w_t-E[X]=0$ ，观测量 $\tilde{g}(w,\eta)=w-x$</p><script type="math/tex;mode=display">\begin{aligned}
\tilde{g}(w_t,\eta_t)&=w_t-x_t=w_t-E[X]+E[X]-x_t\\
&=(w_t-E[X])+(E[X]-x_t)=g(w_t)+\eta_t
\end{aligned}</script><p>可见，广义增量式均值估计属于RM算法</p><h3 id="3-2-3-随机梯度下降"><a href="#3-2-3-随机梯度下降" class="headerlink" title="3.2.3 随机梯度下降"></a>3.2.3 随机梯度下降</h3><p><strong>问题定义</strong></p><script type="math/tex;mode=display">\min\limits_{w} J(w)=\min\limits_{w}E[f(w,X)]</script><p>$w$ 是最优化参数，$X$ 是随机变量，优化目标是以 $w$ 为参数最小化 $f(w,X)$ 的期望</p><ul><li>期望可以看做带权求和的函数，待解决的问题可以看做多元函数最优化问题</li></ul><p><strong>方法</strong></p><ol><li><p>梯度下降法（gtradient descent,GD）</p><p>找目标函数的最小值，所以使用梯度下降法</p><script type="math/tex;mode=display">w_{t+1}=w_t-\alpha_t\underbrace{\bigtriangledown_w E[f(w,X)]}_{\bigtriangledown_w J(w)}\xlongequal{期望本质上是求和}w_t-\alpha_tE[\bigtriangledown_wf(w,X)]</script><p>沿梯度方向，函数值减少最快，可以更快地收敛到最小值</p><p>缺点：期望的计算</p><ul><li>有模型，用公式求</li><li>无模型，用数据求</li></ul></li><li><p>批量梯度下降（batch gradient descent, BGD）</p><p>MC方法： $E[\bigtriangledown_wf(w,X)]\approx\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_i,x_i)$</p><script type="math/tex;mode=display">w_{t+1}=w_t-a_t\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_i,x_i)</script><p>缺点：每次迭代 $w_t$ 都需要采样很多次数 $N$ 次</p></li><li><p>随机梯度下降（stochastic gradient descent, SGD）</p><script type="math/tex;mode=display">w_{t+1}=w_t-a_t\bigtriangledown_wf(w_t,x_t)</script><ul><li>将 <strong>梯度下降法</strong> 中，真实的梯度替换为对梯度的采样 $\bigtriangledown_w f(w_t,x_t)$</li><li>将 <strong>批量梯度下降法</strong> 中，令 $n=1$</li></ul></li></ol><h4 id="算法示例"><a href="#算法示例" class="headerlink" title="算法示例"></a>算法示例</h4><p>最小化目标 $w$ 与变量 $X$ 距离的期望</p><script type="math/tex;mode=display">\min\limits_{w}J(w)=\min\limits_{w}E[f(w,X)]=\min\limits_{w}E\left[\frac{1}{2}\Vert w-X \Vert^2\right]</script><p><strong>1. 最优解 $w^*$ 为 $E[X]$</strong></p><p>求解最小化 $\min\limits_{w}J(w)$ ，即令 $\bigtriangledown_wJ(w)=0$</p><script type="math/tex;mode=display">\bigtriangledown_wJ(w)=0\Rightarrow \bigtriangledown_wE[f(w,X)]=0\Rightarrow E[\bigtriangledown_wf(w,X)]=0\Rightarrow E[w^*-X]=0\Rightarrow w^*=E[X]</script><p><strong>2. 梯度下降法求解最小值</strong></p><script type="math/tex;mode=display">\begin{aligned}
w_{t+1}&=w_t-\alpha_t\bigtriangledown_wJ(w_t)\\
&=w_t-\alpha_tE[\bigtriangledown_w f(w_t,X)]\\
&=w_t-\alpha_tE[w_t-X]\\
&=w_t-\alpha_tw_t+a_tE[X]
\end{aligned}</script><p>每轮 $w_k$ 都需要估计一次期望 $E[X]$</p><p><strong>3. 随机梯度下降法求解最小值</strong></p><script type="math/tex;mode=display">w_{t+1}=w_t-\alpha_t\bigtriangledown_wf(w_t,x_t)=w_t-\alpha_t(w_t-x_t)</script><h4 id="收敛性-1"><a href="#收敛性-1" class="headerlink" title="收敛性"></a>收敛性</h4><blockquote><p>在梯度下降法中，目标是求解最小值，所以沿着 $w_t$ 梯度方向会更快地下降到 $w_t$ 方向上的最小值，我们知道 $w_{t}\xrightarrow{t\rightarrow \infty}w^*$；而随机梯度下降法，用 $(w_t,x_t)$ 处的梯度代替取 $w=w_t$ 时所有点 $(w_t,x_i),\forall i$ 的梯度均值，即一个数据代替数据整体的梯度</p><script type="math/tex;mode=display">GD:w_{t+1}=w_t-\alpha_tE[\bigtriangledown_wf(w_t,X)]\\
\Downarrow\\
SGD:w_{t+1}=w_t-\alpha_t\bigtriangledown_wf(w_t,x_t)</script><p>因为 $\bigtriangledown_wf(w_t,x_t)\neq E[\bigtriangledown_wf(w_t,X)]$ ，需要证明基于SGD，$w_t\xrightarrow{\infty}w^*$ 是否成立</p></blockquote><p>证明思路： <strong>随机梯度下降法</strong> 是一个特殊的 <strong>随机近似算法</strong> ，且满足随机近似收敛的充分条件，则 <strong>随机梯度下降法</strong> 也会收敛</p><p>优化目标是最小化期望</p><script type="math/tex;mode=display">\min\limits_wJ(w)=\min\limits_{w} E[f(w,X)]</script><p>最优化问题使用梯度下降法，等价于求解方程 $\bigtriangledown_w J(w)=E[\bigtriangledown_wf(w_t,X)]=0$ 的解 $w^*$</p><p>对于这么一个未知方程的求解问题，可以用RM算法求解，其观测量为 $\tilde{g}(w,\eta)$</p><script type="math/tex;mode=display">\begin{aligned}
\tilde{g}(w,\eta)&=\bigtriangledown_w f(w,x)\\
&=\underbrace{E[\bigtriangledown_wf(w,X)]}_{g(w)}+\underbrace{\bigtriangledown_wf(w,x)-E[\bigtriangledown_wf(w,X)]}_{\eta}
\end{aligned}</script><ul><li>$\bigtriangledown_wf(w_t,x_t)$ 可以看做 $E[\bigtriangledown_wf(w_t,X)]$ 的一个噪音度量</li></ul><p>因此，SGD是一种特殊的 RM 算法。若满足RM算法收敛的三个条件</p><ul><li>$0&lt;c_1\le \bigtriangledown^2_w f(w,X)\le c_2$</li><li>$\sum\limits_{t=1}^{\infty}\alpha_t=\infty$ 且 $\sum\limits_{t=1}^{\infty}\alpha_t^2&lt;\infty$</li><li>$\{x_t\}_{t=1}^{\infty}$ 是独立同分布的</li></ul><p>则SGD一定收敛于 $w^*$</p><h5 id="SGD收敛的性质"><a href="#SGD收敛的性质" class="headerlink" title="SGD收敛的性质"></a>SGD收敛的性质</h5><p>SGD中，用随机梯度代替真实梯度，梯度的随机性并不会造成收敛的随机性或慢收敛，即初始值的选择 $w_0$ 是否会影响收敛速度</p><ul><li>当 $w_t$ 与 $w^*$ 差距大时 ，SGD表现与GD相同，因为相对误差比较小</li><li>当 $w_t$ 与 $w^*$ 差距小时，SGD才会呈现较大的随机性</li></ul><p>引入随机梯度与批量梯度的相对误差</p><script type="math/tex;mode=display">\delta_t=\frac{\vert \bigtriangledown_wf(w_t,x_t)- E[\bigtriangledown_w f(w_t,X)]\vert}{\vert E[\bigtriangledown_w f(w_t,X)]\vert}</script><p>对于最优解 $w^<em>$ 满足梯度 $E[\bigtriangledown_w f(w^</em>,X)]=0$ ，进而有</p><script type="math/tex;mode=display">\delta_t=\frac{\big\vert \bigtriangledown_wf(w_t,x_t)- E[\bigtriangledown_w f(w_t,X)]\vert}{\vert E[\bigtriangledown_w f(w_t,X)]-E[\bigtriangledown_w f(w^*,X)]\vert}=\frac{\vert \bigtriangledown_wf(w_t,x_t)- E[\bigtriangledown_w f(w_t,X)]\vert}{\big\vert E[\left(\bigtriangledown_w f(w_t,X)-\bigtriangledown_wf(w^*,X)\right)]\big\vert}\xlongequal{中值定理}\frac{\vert \bigtriangledown_wf(w_t,x_t)- E[\bigtriangledown_w f(w_t,X)]\vert}{\big\vert E[\bigtriangledown_w^2 f(\widetilde{w}_t,X)(w_t-w^*)]\big\vert},\widetilde{w}_t\in [w_t,w^*]</script><p>假设 $f$ 是一个严格凸函数，$\bigtriangledown_w^2 f\ge c&gt;0,\forall w,X$ ，则相对误差的分母为</p><script type="math/tex;mode=display">\begin{aligned}
\big\vert E[\bigtriangledown_w^2 f(\widetilde{w}_t,X)(w_t-w^*)]\big\vert&=\big\vert E[\bigtriangledown_w^2 f(\widetilde{w}_t,X)](w_t-w^*)\big\vert\\
&=\big\vert E[\bigtriangledown_w^2 f(\widetilde{w}_t,X)]\big\vert\big\vert(w_t-w^*)\big\vert\ge c\vert(w_t-w^*)\vert
\end{aligned}</script><p>代入相对误差</p><script type="math/tex;mode=display">\delta_t\le \frac{\vert \overbrace{\overbrace{\bigtriangledown_wf(w_t,x_t)}^{随机梯度}- \overbrace{E[\bigtriangledown_w f(w_t,X)]}^{真实梯度}}^{绝对误差}\vert}{c\underbrace{\vert(w_t-w^*)\vert}_{与最优解的间距}}</script><p>相对误差 $\delta_k$ 反比于与最优解的间距：</p><ul><li>分母大的时候，相对误差上界相对小，绝对误差也相对小</li><li>分母小的时候，相对误差上界相对大，绝对误差也相对大</li></ul><p>当 $w_t$ 与 $w^*$ 距离远时，SGD的参数调整方向与GD的方向大致相同</p><h4 id="SGD的确定性公式"><a href="#SGD的确定性公式" class="headerlink" title="SGD的确定性公式"></a>SGD的确定性公式</h4><p>常见的最优化问题是</p><script type="math/tex;mode=display">\min_w J(w)=\min_w\frac{1}{n}\sum\limits_{i=1}^nf(w,x_i)</script><p>$x_i$ 并不是一个随机变量，只是来源于独立同分布的实数集 $\{x_i\}_{i=1}^n$ ，使用梯度下降法最优化这样的带参函数 $f(w,x_i)$</p><script type="math/tex;mode=display">w_{t+1}=w_t-a_t\bigtriangledown_w J(w)=w_t-a_t\sum\limits_{i=1}^n\frac{1}{n}\bigtriangledown_wf(w,x_i)</script><p>假设数据集很大，即 $n$ 很大，则可使用增量式梯度下降——SGD确定性公式</p><script type="math/tex;mode=display">w_{t+1}=w_t-\alpha_t\bigtriangledown_w J(w)=w_t-\alpha_t\bigtriangledown_wf(w_t,x_t)</script><p>这样的梯度下降公式虽然与 SGD相似，但它不含期望与随机变量。通过引入一个随机变量将确定性最优化问题变为随机最优化问题，假设 $X$ 是一个由 $\{x_i\}_{i=1}^n$ 定义的随机变量，且服从均匀分布 $P(X=x_i)=\frac{1}{n}$</p><script type="math/tex;mode=display">\min\limits_{w}J(w)=\min\limits_{w}\frac{1}{n}\sum\limits_{i=1}^nf(w,x_i)=\min_{w}E[f(w,X)]</script><p>所以也可以用SGD求解最优化，即上述SGD确定性公式</p><ul><li>关于 $x_t$ 的选取：从 $\{x_i\}_{i=1}^n$ 中有放回地随机取出 $x_t$</li></ul><h3 id="3-3-4-BGD、SGD、MBGD对比"><a href="#3-3-4-BGD、SGD、MBGD对比" class="headerlink" title="3.3.4 BGD、SGD、MBGD对比"></a>3.3.4 BGD、SGD、MBGD对比</h3><p>对于同一个最优化问题 $\min\limits_{w}J(w)=\min_{w}E[f(w,X)]$ ，随机变量 $X$ 的样本集为 $\{x_i\}_{i=1}^n$</p><script type="math/tex;mode=display">\begin{array}{ll}
w_{t+1}=w_t-\alpha_t\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_t,x_i)&(BGD)\\
w_{t+1}=w_t-\alpha_t\frac{1}{m}\sum\limits_{x_i\in \mathcal{I}_t}\bigtriangledown_wf(w_t,x_i)&(MBGD)\\
w_{t+1}=w_t-\alpha_t\frac{1}{n}\bigtriangledown_wf(w_t,x_t)&(SGD)\\
\end{array}</script><p>BGD，每轮迭代需要 $n$ 个数据，最优化结果最接近真实的最优解</p><p>MBGD，从 $n$ 个数据中采集 $m&lt;n$ 个数据生成数据集 $\mathcal{I}_k$ ，作为第 $k$ 轮迭代的数据，基于这个子集的梯度计算最优解</p><p>SGD，每轮迭代随机选择一个数据，计算其梯度</p><p>相较于SGD，MBGD使用了更多数据将噪音等测量平均掉，所以随机性更小</p><p>相较于BGD，MBGD不需要使用全部数据，所以更加灵活和高效</p><ul><li><p>当 $m=1$ ，MBGD变为SGD</p></li><li><p>当 $m=n$ ，MBGD变为BGD</p><p>BGD用到 $n$ 个数据，虽然 MBGD也会用 $n$ 个数据，但是在BGD的 $n$ 个数据中有重复采样</p></li></ul><h4 id="算法示例-1"><a href="#算法示例-1" class="headerlink" title="算法示例"></a>算法示例</h4><p>对于给定的数据集 $\{x_i\}_{i=1}^{n}$ ，目标是最优化 $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^nx_i$ ，数学公式表示为</p><script type="math/tex;mode=display">\min\limits_{w}J(w)=\min\limits_{w}\frac{1}{2n}\sum\limits_{i=1}^n\Vert w-x_i\Vert^2</script><p>使用梯度下降法求解，其中 $\bigtriangledown_wJ(w)=\frac{1}{n}\sum\limits_{i=1}^n(w-x_i)$</p><script type="math/tex;mode=display">\begin{array}{ll}
w_{t+1}=w_t-\alpha_t\frac{1}{n}\sum\limits_{i=1}^n(w_t-x_i)=w_t-\alpha_t(w_t-\overline{x})&(BGD)\\
w_{t+1}=w_t-\alpha_t\frac{1}{m}\sum\limits_{i=1}^{m}(w_t-x_i)=w_t-\alpha_t(w_t-\overline{x}_t^{(m)})&(MBGD)\\
w_{t+1}=w_t-\alpha_t(w_t-x_t)&(SGD)\\
\end{array}</script><h5 id="收敛速度对比"><a href="#收敛速度对比" class="headerlink" title="收敛速度对比"></a>收敛速度对比</h5><p>$X\in \R^2$ 表示一个平面中的随机位置，其分布服从 $U(20,20)$ 的均匀分布，则真实均值 $E[X]=\mathbf{0}$，基于100个独立同分布的样本 $\{x_i\}_{i=1}^{100}$ ，令 $\alpha_t=\frac{1}{t}$，求均值</p><p><img src="/posts/1184554208/image-20240227163416632.png" alt="image-20240227163416632"></p><p><img src="/posts/1184554208/image-20240227163505422.png" alt="image-20240227163505422"></p><p>观察橙色线（基于随机梯度下降的最优化），尽管初始值与真实期望差距很大，但SGD朝着最优解的方向收敛，当接近最优解时，呈现随机性，但仍逐渐收敛于真实值</p><p>若 $\alpha_t=\frac{1}{t}$</p><script type="math/tex;mode=display">\begin{array}{ll}
w_{t+1}=w_t-\frac{1}{t}(w_t-\overline{x})\xlongequal{迭代推导}\frac{1}{t}\sum\limits_{i=1}^tx_i=\frac{1}{t}\sum\limits_{i=1}^t\overline{x}=\overline{x}&(BGD)\\
w_{t+1}=\frac{1}{t}\sum\limits_{i=1}^t\overline{x}_t^{(m)}=\overline{x}_t^{(m)}&(MBGD)\\
w_{t+1}=\frac{t-1}{k}w_t+\frac{1}{t}x_t=\frac{1}{t}\sum\limits_{i=1}^tx_i=\frac{1}{t}\overline{x}&(SGD)  
\end{array}</script><p>BGD的每一步迭代都是最优解 $w^*=\overline{x}$</p><p>由于 $\overline{x}_t^{(m)}$ 已经是一个均值，所以MBGD收敛于均值的速度快于SGD，且 $m$ 越大，MBGD的收敛速度越快</p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/1184554208/" title="3-基于价值表格的免模型学习">https://amostian.github.io/posts/1184554208/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 强化学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3236386842/" rel="prev" title="0-强化学习资料"><i class="fa fa-chevron-left"></i> 0-强化学习资料</a></div><div class="post-nav-item"><a href="/posts/3003864197/" rel="next" title="函数型方法">函数型方法 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%85%8D%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.1 蒙特卡洛方法——免模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-text">3.1.1 蒙特卡洛基本思想</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MC%E6%96%B9%E6%B3%95%E7%90%86%E8%AE%BA%E4%BE%9D%E6%8D%AE%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B"><span class="nav-text">MC方法理论依据——大数定律</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-MC-Basic"><span class="nav-text">3.1.2 MC Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90"><span class="nav-text">策略迭代分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-Basic%E7%AE%97%E6%B3%95"><span class="nav-text">MC Basic算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-text">伪代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-Basic%E4%BE%8B%E5%AD%90"><span class="nav-text">MC Basic例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-Basic%E7%89%B9%E7%82%B9"><span class="nav-text">MC Basic特点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E5%9B%9E%E5%90%88%E9%95%BF%E5%BA%A6%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">关于回合长度的例子</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">3.1.3 探索性开始</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="nav-text">探索性开始的必要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E9%AB%98%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E7%9A%84%E6%95%88%E7%8E%87"><span class="nav-text">提高策略评估的效率</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E9%AB%98%E6%95%88%E5%88%A9%E7%94%A8"><span class="nav-text">数据的高效利用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0%E6%97%B6%E6%9C%BA%E2%80%94%E2%80%94%E7%A6%BB%E7%BA%BF%E5%8F%98%E5%9C%A8%E7%BA%BF"><span class="nav-text">策略更新时机——离线变在线</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81%EF%BC%9AMC%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">伪代码：MC探索性开始</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-MC%E6%97%A0%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">3.1.4 MC无探索性开始</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#varepsilon-%E8%B4%AA%E5%BF%83"><span class="nav-text">$\varepsilon-贪心$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%86-varepsilon-%E8%B4%AA%E5%BF%83-%E5%BA%94%E7%94%A8%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">将 $\varepsilon-贪心$ 应用于强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81-1"><span class="nav-text">伪代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E7%82%B9"><span class="nav-text">特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90"><span class="nav-text">例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#varepsilon-%E8%B6%8A%E5%A4%A7%EF%BC%8C%E6%8E%A2%E7%B4%A2%E6%80%A7%E8%B6%8A%E5%BC%BA"><span class="nav-text">$\varepsilon$ 越大，探索性越强</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%BA%E7%89%B2%E6%9C%80%E4%BC%98%E6%80%A7"><span class="nav-text">牺牲最优性</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E9%9A%8F%E6%9C%BA%E8%BF%91%E4%BC%BC%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">3.2 随机近似与随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-%E5%9D%87%E5%80%BC%E8%BF%91%E4%BC%BC%E7%9A%84%E4%B8%8D%E5%90%8C%E7%90%86%E8%A7%A3"><span class="nav-text">3.2.1 均值近似的不同理解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E5%9D%87%E5%80%BC%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-text">增量式均值计算方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E7%82%B9-1"><span class="nav-text">特点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E5%A2%9E%E9%87%8F%E5%BC%8F%E5%9D%87%E5%80%BC%E4%BC%B0%E8%AE%A1"><span class="nav-text">广义增量式均值估计</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-RM%E7%AE%97%E6%B3%95"><span class="nav-text">3.2.2 RM算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%80%A7"><span class="nav-text">收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90-1"><span class="nav-text">例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%81%E6%98%8E"><span class="nav-text">证明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BB%A1%E8%B6%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E6%9D%A1%E4%BB%B6%E7%9A%84-a-t"><span class="nav-text">满足系数的条件的 $a_t$</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RM%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E4%BA%8E%E5%9D%87%E5%80%BC%E4%BC%B0%E8%AE%A1"><span class="nav-text">RM算法应用于均值估计</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">3.2.3 随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%A4%BA%E4%BE%8B"><span class="nav-text">算法示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%80%A7-1"><span class="nav-text">收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SGD%E6%94%B6%E6%95%9B%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-text">SGD收敛的性质</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%85%AC%E5%BC%8F"><span class="nav-text">SGD的确定性公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-4-BGD%E3%80%81SGD%E3%80%81MBGD%E5%AF%B9%E6%AF%94"><span class="nav-text">3.3.4 BGD、SGD、MBGD对比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%A4%BA%E4%BE%8B-1"><span class="nav-text">算法示例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94"><span class="nav-text">收敛速度对比</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">355</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">58</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">74</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">803.3k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">33:30</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>