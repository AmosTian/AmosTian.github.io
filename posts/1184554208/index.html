<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="3-基于价值的决策"><meta property="og:url" content="https://amostian.github.io/posts/1184554208/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240123174813238.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240220233518921.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240220234156707.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240220234539493.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240220234817666.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240124152431886.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240123173100717.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221202657024.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221204003812.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221204628110.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221205930439.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221210420304.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240124154153825.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240124154228790.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240124154658184.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240124155225933.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240124155802457.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221225836947.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221230035376.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240221230121741.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223105012439.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223155537861.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223165057968.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223165703357.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240223165756497.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240222174121229.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224102256712.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224094801791.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240222233322816.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224101526505.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240224101639774.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/Figure_1.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240225111350459.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240227163416632.png"><meta property="og:image" content="https://amostian.github.io/posts/1184554208/image-20240227163505422.png"><meta property="article:published_time" content="2024-02-20T10:18:40.000Z"><meta property="article:modified_time" content="2024-02-27T15:35:33.414Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="强化学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/1184554208/image-20240123174813238.png"><link rel="canonical" href="https://amostian.github.io/posts/1184554208/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>3-基于价值的决策 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">58</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">356</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/1184554208/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">3-基于价值的决策</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-02-20 18:18:40" itemprop="dateCreated datePublished" datetime="2024-02-20T18:18:40+08:00">2024-02-20</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-02-27 23:35:33" itemprop="dateModified" datetime="2024-02-27T23:35:33+08:00">2024-02-27</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">4.9k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>13 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h2 id="3-0-策略的获取"><a href="#3-0-策略的获取" class="headerlink" title="3.0 策略的获取"></a>3.0 策略的获取</h2><h3 id="3-0-1-价值函数的计算"><a href="#3-0-1-价值函数的计算" class="headerlink" title="3.0.1 价值函数的计算"></a>3.0.1 价值函数的计算</h3><ul><li>动态规划法(dynamic programming, DP)——有模型学习, (Model-based Reinforcement Learning, MBRL)</li><li>蒙特卡洛方法(Monte Carlo, MC)——免模型学习(Model-free Reinforcement Learning, MFRL)</li><li>时序差分学习(temporal-difference learning, TD learning)，前两者的结合</li></ul><blockquote><p>强化学习中的 <strong>模型</strong> ：若环境的动态特性($P(s’,r\vert s,a)$)已知，则认为环境已知。智能体对环境的建模称为模型</p></blockquote><p>有模型学习常用的价值函数计算方法有</p><ul><li>蒙特卡洛方法</li><li>动态规划方法<ul><li>价值迭代</li><li>策略迭代</li></ul></li><li>用数据估计出一个环境模型，进而基于动态规划方法计算价值函数</li></ul><p>免模型学习的价值函数计算方法</p><ul><li><p>蒙特卡洛方法：将策略迭代中，基于模型的部分替换为免模型部分</p><p>更新时间：2024-2-24 10:28:04</p></li><li><p>时序差分方法</p><p>更新时间：2024-2-27 23:34:53</p></li></ul><h3 id="3-0-2-最优策略的获取"><a href="#3-0-2-最优策略的获取" class="headerlink" title="3.0.2 最优策略的获取"></a>3.0.2 最优策略的获取</h3><p>当获取最优价值函数后，可以通过取使 $V(s)$ 最大的动作来得到最佳策略</p><script type="math/tex;mode=display">\pi^*(a\vert s)=\begin{cases}
1,&a=\mathop{\mathrm{argmax}}\limits_{a}Q(s,a)\\
0,&其他
\end{cases}</script><p>最简单的策略搜索办法是 <strong>穷举</strong> 。假设动作和状态都是有限的，则对于确定性策略空间有 $\vert \mathcal{A}\vert ^{\vert \mathcal{S}\vert}$ 个可能的策略，基于每个策略，算出状态价值函数，取最大状态价值对应的策略即可。</p><p>但穷举法效率低，可采取其他更好地最佳策略搜索方法</p><h2 id="3-1-动态规划方法——有模型"><a href="#3-1-动态规划方法——有模型" class="headerlink" title="3.1 动态规划方法——有模型"></a>3.1 动态规划方法——有模型</h2><ul><li>策略迭代</li><li><p>价值迭代</p></li><li><p>截断策略迭代</p></li></ul><h3 id="3-1-1-价值迭代"><a href="#3-1-1-价值迭代" class="headerlink" title="3.1.1 价值迭代"></a>3.1.1 价值迭代</h3><h4 id="最优性原理"><a href="#最优性原理" class="headerlink" title="最优性原理"></a>最优性原理</h4><p>在策略 $\pi(a\vert s)$ 下，一个状态 $s$ 达到了最优价值( $V_\pi(s)=V^<em>(s)$ ) ，当且仅当对于任何能够从 $s$ 到达的状态 $s’$ ，都已经达到了最优价值，即对于所有的 $s’$ ，有 $V_\pi(s’)=V^</em>(s’)$ 恒成立——最优子结构</p><h4 id="确定性价值迭代"><a href="#确定性价值迭代" class="headerlink" title="确定性价值迭代"></a>确定性价值迭代</h4><p>由贝尔曼最优方程可知</p><script type="math/tex;mode=display">\begin{aligned}
V^{(k+1)}(s)&=f(V^{(k)})=\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right),k=0,1,2,3,\cdots\\
&=\max\limits_{\pi\in\Pi}\sum\limits_{a}\pi(a\vert s)Q(s,a)\\
&\xlongequal{Q=f(V)}\max\limits_{\pi\in\Pi}\sum\limits_{a}\pi(a\vert s)\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V^*(s')\right)
\end{aligned}</script><p>为了得到每个状态下的最优状态价值 $V^*$ ，可直接将贝尔曼最优方程作为更新规则进行迭代，迭代多次后，价值函数就会收敛，这种价值迭代算法也被称为确定性价值迭代</p><h5 id="迭代算法"><a href="#迭代算法" class="headerlink" title="迭代算法"></a>迭代算法</h5><p>初始化：令 $k=0$ ，对所有状态 $s$ ，$V^{(0)}(s)=0$</p><ol><li><p>第 $k$ 轮迭代，$k=0,1,2,\cdots$：</p><p>计算 $Q^{(k)}$</p><script type="math/tex;mode=display">\mathbf{Q}^{(k)}(s,a)=\mathbf{R}(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot \mathbf{V}^{(k)}(s')\\</script><p><img src="/posts/1184554208/image-20240123174813238.png" alt="image-20240123174813238"></p><p>策略更新</p><script type="math/tex;mode=display">\pi^{(k+1)}(s)=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)Q^{(k)}(s,a)\\
\Updownarrow\\
\pi^{(k+1)}(a\vert s)=\begin{cases}
1&a=a_{*}^{(k)}\\
0&a\neq a_{*}^{(k)}
\end{cases}\quad,a_{*}^{(k)}=\mathop{\mathrm{argmax}}\limits_a\mathbf{Q}^{(k)}(s,a)</script></li><li><p>价值更新</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{V}^{(k+1)}(s)&=\mathbf{R}_{\pi^{(k+1)}}(s,a)+\gamma \sum\limits_{s'\in S}P_{\pi^{(k+1)}}(s'\vert s,a)\cdot \mathbf{V}^{(k)}(s')\\
&=\sum\limits_{a}\pi^{(k+1)}(a\vert s)\underbrace{\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'}P(s'\vert s,a)V^{(k)}(s')\right)}_{Q^{(k)}(s,a)}
\end{aligned}</script><p>由于采用确定性策略，所以最优动作的概率 $\pi(a^*\vert s)=1,\pi(a^{others}\vert s)=0$ 。相当于</p><script type="math/tex;mode=display">V^{(k+1)}=\max\limits_{a}Q^{(k)}(a,s)</script></li></ol><p>在收敛后提取最优策略，$H$是让 $V(s)$ 收敛所需的迭代次数</p><script type="math/tex;mode=display">\pi^*(s)=\mathop{\mathrm{argmax}}_a\left[R(s,a)+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)V^{(H+1)}(s')\right]</script><h6 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h6><script type="math/tex;mode=display">V^{(0)}(s),V^{(k)}(s)\rightarrow Q^{(k)}(s,a)\rightarrow 贪心策略 \pi^{(k+1)}(a\vert s)\rightarrow V^{(k+1)}=\max\limits_{a}Q^{(k)}(s,a)</script><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：已知P(r\vert s,a)和P(s'\vert s,a)，用随机值初始化V^{(0)}\\
&目标：求解贝尔曼最优方程进而得出最优状态价值与最优策略\\
&第k轮迭代,当\mathbf{V}^{(k)}未收敛时(\Vert V^{(k)}-V^{(k-1)}\Vert>\varepsilon)，进行：\\
&\quad对于 \forall s\in \mathcal{S}:\\
&\qquad 对于 \forall a\in \mathcal{A}(s):\\
&\quad \qquad 计算Q值：Q^{(k)}(s,a)=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'}P(s'\vert s,a)V^{(k-1)}(s')\\
&\qquad 求最优动作：a_{*}^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q^{(k)}(s,a)\\
&\qquad 策略更新：\pi^{(k+1)}(a_{*}^{(k)}\vert s)=1,\pi^{(k+1)}(a^{others}\vert s)=0\\
&\qquad 价值更新：V^{(k+1)}=\max\limits_{a}Q^{(k)}(a,s)\\
\hline
\end{array}</script><h6 id="价值迭代例子"><a href="#价值迭代例子" class="headerlink" title="价值迭代例子"></a>价值迭代例子</h6><p>条件：1个禁止区域 $s_2$ ，1个目标区域 $s_4$ 。奖励设置为 $r_{boundary}=r_{forbidden}=-1,r_{target}=1,r_{others}=0$ ，$a_1,a_2,a_3,a_4=上,右,下,左$</p><p><img src="/posts/1184554208/image-20240220233518921.png" alt="image-20240220233518921"></p><p>根据贝尔曼期望方程计算状态价值，由于采取确定性策略且状态转移是确定的，所以 $\pi(a^*\vert s)=1,P(s’\vert s,a)=1$</p><script type="math/tex;mode=display">\begin{aligned}
V(s)&=\sum\limits_{a\in A}\pi(a\vert s)\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_\pi(s')\right)\\
&=R(s,a)+\gamma V_{\pi}(s')
\end{aligned}</script><p><img src="/posts/1184554208/image-20240220234156707.png" alt="image-20240220234156707"></p><ul><li><p>k=0：</p><p><img src="/posts/1184554208/image-20240220234539493.png" alt="image-20240220234539493"></p></li><li><p>k=1：</p><p><img src="/posts/1184554208/image-20240220234817666.png" alt="image-20240220234817666"></p></li></ul><h4 id="价值迭代分析"><a href="#价值迭代分析" class="headerlink" title="价值迭代分析"></a>价值迭代分析</h4><h5 id="价值迭代理解"><a href="#价值迭代理解" class="headerlink" title="价值迭代理解"></a>价值迭代理解</h5><p>价值迭代每次迭代只能影响与之直接相关的状态，工作过程类似于价值的反向传播，每次迭代做进一步传播，如果子问题的价值变好了，当前价值也会变得更好。</p><p><strong>以最短路径为例</strong> ：对于每个状态，都可以将其看最一个终点。每轮迭代，从每个状态开始根据贝尔曼最优方程重新计算价值。左上角为终点，每多走一步，价值越小。只有当 $V_7$ 收敛后，才能基于得到的最优价值来提取最佳策略</p><p><img src="/posts/1184554208/image-20240124152431886.png" alt="image-20240124152431886"></p><h5 id="中间状态价值-策略无意义"><a href="#中间状态价值-策略无意义" class="headerlink" title="中间状态价值/策略无意义"></a>中间状态价值/策略无意义</h5><p>在贝尔曼方程中，$V^{(k+1)}$ 与 $V^{(k)}$ 都是状态价值；而在值迭代算法中，$V^{(k)}$ 可以是任意的值，不是状态价值</p><p>在未将每个状态的最优价值未传递给其他所有状态之前，中间的几个价值只是一种暂存的不完整的数据，不能代表每个状态的价值，所以基于中间过程的价值函数所生成的策略没有意义</p><h3 id="3-1-2-策略迭代"><a href="#3-1-2-策略迭代" class="headerlink" title="3.1.2 策略迭代"></a>3.1.2 策略迭代</h3><h4 id="策略迭代算法"><a href="#策略迭代算法" class="headerlink" title="策略迭代算法"></a>策略迭代算法</h4><blockquote><p>通过解决预测问题进而解决控制问题</p></blockquote><ol><li><p>策略评估(policy evaluation,PE)：利用贝尔曼公式评估给定策略 $\pi^{(k)}$ 的状态价值</p><script type="math/tex;mode=display">\mathbf{V}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}</script></li><li><p>策略改进(policy improvement,PI)：使用 $\mathop{\mathrm{argmax}}$ 进行策略改进</p><script type="math/tex;mode=display">\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi\in\Pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi}\right)</script></li></ol><h5 id="策略评估-预测问题"><a href="#策略评估-预测问题" class="headerlink" title="策略评估(预测问题)"></a>策略评估(预测问题)</h5><p>利用贝尔曼方程计算状态价值</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{V}_{\pi^{(k)}}(s)&=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}(s),s\in \mathcal{S}
\end{aligned}</script><p>其解析解为</p><script type="math/tex;mode=display">\mathbf{V}_{\pi^{(k)}}(s)=(\mathbf{I}-\gamma P_{\pi^{(k)}})^{-1}\mathbf{R}_{\pi^{(k)}}</script><p>数值解为</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)&=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}^{(j)}(s)\\
&=\sum\limits_{a\in A}\pi^{(k)}(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k)}}^{(j)}(s')\right),s\in \mathcal{S},j=0,1,2,\cdots
\end{aligned}</script><ul><li>终止条件是 $\Vert \mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)-\mathbf{V}_{\pi^{(k)}}^{(j)}(s)\Vert$ 足够小</li><li>通过自举法求解贝尔曼方程，相当于在策略迭代的大迭代过程中小迭代求解状态价值</li></ul><h5 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h5><blockquote><p>由 <strong>策略改进定理</strong> ，可知 $argmax$ 可以得到单调递增的 $V$</p><p>若 $\pi’=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi}\right)$ ，则 $\mathbf{V}_{\pi’}\ge \mathbf{V}_{\pi}$</p></blockquote><script type="math/tex;mode=display">\begin{aligned}
\pi^{(k+1)}&=\mathop{\mathrm{argmax}}\limits_{\pi\in\Pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi}\right)\\
&=\mathop{\mathrm{argmax}}\limits_{\pi\in\Pi}\sum\limits_{a}\pi(a\vert s)\underbrace{\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\right)}_{Q_{\pi^{(k)}}(s,a)},s\in \mathcal{S},s\in \mathcal{S}
\end{aligned}</script><p>$Q_{\pi^{(k)}}(s,a)$ 为基于策略 $\pi^{(k)}$ 下在状态 $s$ 下的动作价值，最优动作 $a_{*}^{(k)}$</p><script type="math/tex;mode=display">a_{*}^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)</script><p>因此，贪心策略为</p><script type="math/tex;mode=display">\pi^{(k+1)}(a\vert s)=\begin{cases}
1&a=a_{*}^{(k)}(s)\\
0&a\neq a_{*}^{(k)}(s)
\end{cases}</script><h5 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h5><p>变量序列为</p><script type="math/tex;mode=display">\pi^{(0)}\xrightarrow{PE}V_{\pi^{(0)}}\xrightarrow{PI}\pi^{(1)}\xrightarrow{PE}V_{\pi^{(1)}}\xrightarrow{PI}\cdots</script><p>上述步骤经过多轮迭代，状态价值函数和策略都会收敛</p><p><img src="/posts/1184554208/image-20240123173100717.png" alt="image-20240123173100717"></p><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化:动态特性与状态转移P(r\vert s,a)与P(s'\vert s,a)，随机初始化\pi^{(0)}\\
&目标：寻找最优状态价值与最优策略\\
&当策略未收敛，第 k轮迭代:\\
&\quad 策略评估:\\
&\qquad 初始化：一个随机初始化的状态价值\mathbf{V}^{(0)}_{\pi^{(k)}}\\
&\qquad 当\mathbf{V}^{(j)}_{\pi^{(k)}} 未收敛，进行第j+1轮迭代：\\
&\quad\qquad 对每个状态 s\in\mathcal{S}:\\
&\qquad\qquad \mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)=\sum\limits_{a\in A}\pi^{(k)}(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k)}}^{(j)}(s')\right)\\
&\quad 策略改进:\\
&\qquad 对于每个状态s\in \mathcal{S}:\\
&\qquad \quad 对于每个动作 a\in \mathcal{A}(s):\\
&\qquad \qquad Q_{\pi^{(k)}}(s,a)=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\\
&\qquad \quad a_*^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)\\
&\quad \qquad 若a_*^{(k)}(s)==a ,则\pi^{(k+1)}(a\vert s)=1，否则\pi^{(k+1)}(a\vert s)=0\\
\hline
\end{array}</script><h4 id="策略迭代收敛性"><a href="#策略迭代收敛性" class="headerlink" title="策略迭代收敛性"></a>策略迭代收敛性</h4><p>策略迭代算法最终会得到收敛于最优策略的策略</p><p>在策略迭代过程中，会产生状态价值序列 $\{\mathbf{V}_{\pi^{(0)}},\mathbf{V}_{\pi^{(1)}},\cdots,\mathbf{V}_{\pi^{(k)}},\cdots\}$ 与策略序列 $\{\pi^{(0)},\pi^{(1)},\cdots,\pi^{(k)},\cdots\}$</p><p>由 <strong>策略改进定理</strong> ，状态价值是单调增的；而由于我们知道其存在解析解 $\mathbf{V}^*$ ，故有</p><script type="math/tex;mode=display">\mathbf{V}_{\pi^{(0)}}\le \mathbf{V}_{\pi^{(1)}}\le \cdots\le \mathbf{V}_{\pi^{(k)}}\le \cdots\le \mathbf{V}^*</script><p>当 $k\rightarrow \infty$ 时，$\mathbf{V}^{(\infty)}=\mathbf{V}^*$ ，即状态价值会收敛于最优状态价值。因此，策略序列 $\{\pi^{(k)}\}_{k=0}^{\infty}$ 收敛于最优策略</p><h5 id="策略迭代的收敛速度快于价值迭代"><a href="#策略迭代的收敛速度快于价值迭代" class="headerlink" title="策略迭代的收敛速度快于价值迭代"></a>策略迭代的收敛速度快于价值迭代</h5><p>策略迭代的状态价值序列为 $\{\mathbf{V}_{\pi^{(k)}}\}_{k=0}^{\infty}$ ，而价值迭代的状态价值序列为 $\{\mathbf{V}^{(k)}\}_{k=0}^{\infty}$</p><script type="math/tex;mode=display">\mathbf{V}^{(k+1)}=f(\mathbf{V}^{(k)})=\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right)</script><p>同理，我们也知道对于任意的初始值 $V^{(0)}$ ，有 $V^{(k)}\xrightarrow{k\rightarrow \infty} V^*$</p><p>若要证明 <strong>策略迭代的迭代轮数 $k$ 小</strong>，只需证明对于任一 $k$ ，有 $\mathbf{V}^{(k)}\le \mathbf{V}_{\pi^{(k)}}\le \mathbf{V}^*$</p><p>采用归纳法，假设对于 $k\ge 0$ ，有 $\mathbf{V}^{(k)}\le \mathbf{V}_{\pi^{(k)}}$ ，对于 $k+1$</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{V}_{\pi^{(k+1)}}-\mathbf{V}^{(k+1)}&=\left(\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}_{\pi^{(k+1)}}\right)-\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right)\\
&\ge \left(\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}_{\pi^{(k)}}\right)-\max\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)}\right)\\
&\qquad(由于\mathbf{V}_{\pi^{(k+1)}}\ge \mathbf{V}_{\pi^{(k)}})\\
&=\left(\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}_{\pi^{(k)}}\right)-\left(\mathbf{R}_{\pi'_{(k)}}+\gamma P_{\pi'_{(k)}}\mathbf{V}_{\pi'_{(k)}}\right)\\
&\qquad 令 \pi'_{(k)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}^{(k)})\\
&\ge \left(\mathbf{R}_{\pi'_{(k)}}+\gamma P_{\pi'_{(k)}}\mathbf{V}_{\pi^{(k)}}\right)-\left(\mathbf{R}_{\pi'_{(k)}}+\gamma P_{\pi'_{(k)}}\mathbf{V}_{\pi'_{(k)}}\right)\\
&\qquad 由于 \pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}}\right)，大于任意策略值\\
&=\gamma P_{\pi'_{(k)}}\left(\mathbf{V}_{\pi^{(k)}}-\mathbf{V}^{(k)}\right)
\end{aligned}</script><p>因此，$\mathbf{V}_{\pi^{(k+1)}}\ge\mathbf{V}^{(k+1)}$ 成立</p><p>故可知 $\mathbf{V}^{(k)}\le \mathbf{V}_{\pi^{(k)}}\le \mathbf{V}^*$ ，即策略迭代的收敛速度快</p><h4 id="策略迭代例子"><a href="#策略迭代例子" class="headerlink" title="策略迭代例子"></a>策略迭代例子</h4><p><img src="/posts/1184554208/image-20240221202657024.png" alt="image-20240221202657024"></p><p>状态空间有两个状态，$s_1,s_2\in \mathcal{S}$ ，动作空间 $\mathcal{A}=\{a_l,a_0,a_r\}$ 分别代表向左、不动、向右</p><p>奖励设置：$r_{boundary}=-1,r_{target}=1$ ，折扣因子 $\gamma=0.9$</p><p>初始策略 $\pi^{(0)}(s_1)=a_l,\pi^{(0)}(s_2)=a_l$</p><p><strong>$k=0$</strong></p><ul><li><p>策略评估：根据贝尔曼方程计算状态价值 $\mathbf{V}_{\pi^{(0)}}$</p><script type="math/tex;mode=display">\begin{cases}
V_{\pi^{(0)}}(s_1)=R_{\pi^{(0)}}(s_1)+\gamma V_{\pi^{(0)}}(s_1)\\
V_{\pi^{(0)}}(s_2)=R_{\pi^{(0)}}(s_2)+\gamma V_{\pi^{(0)}}(s_1)
\end{cases}=\begin{cases}
V_{\pi^{(0)}}(s_1)=-1+\gamma V_{\pi^{(0)}}(s_1)\\
V_{\pi^{(0)}}(s_2)=0+\gamma V_{\pi^{(0)}}(s_1)
\end{cases}\xRightarrow{解析解}\begin{cases}
V_{\pi^{(0)}}(s_1)=-10\\
V_{\pi^{(0)}}(s_2)=-9
\end{cases}</script><p>若使用自举法求数值解，随机初始 $V_{\pi^{(0)}}^{(0)}(s_1)=V_{\pi^{(0)}}^{(0)}(s_2)=0$</p><p><img src="/posts/1184554208/image-20240221204003812.png" alt="image-20240221204003812"></p><p>随着迭代次数 $j$ 的增多，$V_{\pi^{(0)}}^{(j)}(s_1)\rightarrow V_{\pi^{(0)}}(s_1)=-10,V_{\pi^{(0)}}^{(j)}(s_2)\rightarrow V_{\pi^{(0)}}(s_2)=-9$</p></li><li><p>策略改进，</p><p>| $Q_{\pi^{(k)}}(s,a)$ | $a_l$ | $a_0$ | $a_r$ |<br>| —————————— | ——————————————— | ——————————————- | ——————————————— |<br>| $s_1$ | $-1+\gamma V_{\pi^{(k)}}(s_1)$ | $0+\gamma V_{\pi^{(k)}}(s_1)$ | $1+\gamma V_{\pi^{(k)}}(s_2)$ |<br>| $s_2$ | $0+\gamma V_{\pi^{(k)}}(s_1)$ | $1+\gamma V_{\pi^{(k)}}(s_2)$ | $-1+\gamma V_{\pi^{(k)}}(s_2)$ |</p><p>当 $k=0$ 时，计算 $Q_{\pi^{(0)}}(s,a)$ 表</p><p><img src="/posts/1184554208/image-20240221204628110.png" alt="image-20240221204628110"></p><p>策略改进为</p><script type="math/tex;mode=display">\pi^{(1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(0)}}(s,a)=\begin{cases}
\pi^{(1)}(a_r\vert s_1)=1,\pi^{(1)}(a_0\vert s_1)=\pi^{(1)}(a_l\vert s_1)=0\\
\pi^{(1)}(a_0\vert s_2)=1,\pi^{(0)}(a_0\vert s_1)=\pi^{(r)}(a_0\vert s_1)=0
\end{cases}</script><p><img src="/posts/1184554208/image-20240221205930439.png" alt="image-20240221205930439"></p></li></ul><p><strong>$k=1,2,\cdots$</strong></p><p>策略评估发现，$V_{\pi^{(k+1)}}(s_1)-V_{\pi^{(k)}}(s_1)\le \varepsilon,V_{\pi^{(k+1)}}(s_2)-V_{\pi^{(k)}}(s_2)\le \varepsilon$ ，则策略收敛</p><h4 id="通过策略迭代例子理解强化"><a href="#通过策略迭代例子理解强化" class="headerlink" title="通过策略迭代例子理解强化"></a>通过策略迭代例子理解强化</h4><p>$r_{boundary}=-1,r_{forbidden}=-10,r_{targer}=1,\gamma=0.9$</p><p><img src="/posts/1184554208/image-20240221210420304.png" alt="image-20240221210420304"></p><ul><li><p>$\gamma=0.9$ 更重视未来奖励，可能会造成选择短期负奖励，但可以更快地到达目标状态</p><p>但再加上 $r_{forbidden}=-10$ ，加大了负奖励的惩罚，所以最优路径会绕过负奖励区域</p></li><li><p>离目标区域近的状态，其最优策略收敛早于远离目标区域的状态。<br>只有当距离较近的状态能够找到到达目标的轨迹时，距离较远的状态才能找到穿过距离较近的状态到达目标的轨迹；若较近状态的策略并不是最优策略，较远状态做出的最优决策也是没有意义的</p></li><li><p>靠近目标的状态具有更大的状态值。这种模式的原因是，一个智能体从更远的状态出发，必须走很多步才能获得积极的奖励。这样的奖励将严重打折扣，因此相对较小。</p></li></ul><h3 id="3-1-3-截断策略迭代"><a href="#3-1-3-截断策略迭代" class="headerlink" title="3.1.3 截断策略迭代"></a>3.1.3 截断策略迭代</h3><h4 id="网格世界中的策略迭代与价值迭代"><a href="#网格世界中的策略迭代与价值迭代" class="headerlink" title="网格世界中的策略迭代与价值迭代"></a>网格世界中的策略迭代与价值迭代</h4><p>网格世界初始化界面</p><p><img src="/posts/1184554208/image-20240124154153825.png" alt="image-20240124154153825"></p><p><strong>策略迭代</strong></p><p><img src="/posts/1184554208/image-20240124154228790.png" alt="image-20240124154228790"></p><p>初始策略：每个状态采取固定的随机策略，每个状态可采取的各个动作等概率</p><p>第一次策略评估：每个状态都有一个价值函数</p><p>第一次策略改进：有些状态的策略已经发生改变</p><p><img src="/posts/1184554208/image-20240124154658184.png" alt="image-20240124154658184"></p><p>第二次策略评估：在多次策略评估后，所有状态的价值函数收敛</p><p>第二次策略更新：每个状态会选择最佳策略来执行，不再是随机策略</p><p><img src="/posts/1184554208/image-20240124155225933.png" alt="image-20240124155225933"></p><p>直至每个格子的值不再变化，说明整个MDP已经收敛</p><p><strong>价值迭代</strong></p><p><img src="/posts/1184554208/image-20240124155802457.png" alt="image-20240124155802457"></p><p>当价值迭代使状态价值收敛后，可提取最佳策略</p><p>价值迭代提取出的最佳策略与策略迭代得出的最佳策略一致</p><h4 id="价值迭代与策略迭代对比"><a href="#价值迭代与策略迭代对比" class="headerlink" title="价值迭代与策略迭代对比"></a>价值迭代与策略迭代对比</h4><h5 id="变量序列"><a href="#变量序列" class="headerlink" title="变量序列"></a>变量序列</h5><ul><li><p>策略迭代</p><p>策略评估，PE：$\mathbf{V}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}$</p><p>策略改进，PI：$\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}})$</p></li><li><p>价值迭代</p><p>策略更新，PU：$\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}})$</p><p>价值更新，VU：$\mathbf{V}^{(k+1)}=\mathbf{R}_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}}\mathbf{V}^{(k)}$</p></li></ul><script type="math/tex;mode=display">\begin{aligned}
&策略迭代:\pi^{(0)}\xrightarrow{PE}&\mathbf{V}_{\pi^{(0)}}\xrightarrow{PI}\pi^{(1)}\xrightarrow{PE}\mathbf{V}_{\pi^{(1)}}\xrightarrow{PI}\pi^{(2)}\xrightarrow{PE}\mathbf{V}_{\pi^{(2)}}\xrightarrow{PI}\cdots\\
&价值迭代:&\mathbf{V}^{(0)}\xrightarrow{PU}\mu^{(1)}\xrightarrow{VU}\mathbf{V}^{(1)}\xrightarrow{PU}\mu^{(2)}\xrightarrow{VU}\mathbf{V}^{(2)}\xrightarrow{PU}\cdots
\end{aligned}</script><h5 id="算法对比"><a href="#算法对比" class="headerlink" title="算法对比"></a>算法对比</h5><div class="table-container"><table><thead><tr><th></th><th>策略迭代</th><th>价值迭代</th><th>备注</th></tr></thead><tbody><tr><td>Policy</td><td>$\pi^{(0)}$</td><td>无 $\mu^{(0)}$</td><td>初始：策略迭代从策略 $\pi^{(0)}$ 出发，价值迭代从价值 $V^{(0)}$ 出发</td></tr><tr><td>Value</td><td>$\mathbf{V}_{\pi^{(0)}}=\mathbf{R}_{\pi^{(0)}}+\gamma P_{\pi^{(0)}}\mathbf{V}_{\pi^{(0)}}$</td><td>令 $V^{(0)}=V_{\pi^{(0)}}$</td><td>为便于比较(控制变量)，将价值迭代初始值设为贝尔曼方程的解</td></tr><tr><td>Policy</td><td>$\pi^{(1)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(0)}})$</td><td>$\mu^{(1)}=\mathop{\mathrm{argmax}}\limits_{\mu}(\mathbf{R}_{\mu}+\gamma P_{\mu}\mathbf{V}^{(0)})$</td><td></td></tr><tr><td>Value</td><td>$\mathbf{V}_{\pi^{(1)}}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}$</td><td>$\mathbf{V}^{(1)}=\mathbf{R}_{\mu^{(1)}}+\gamma P_{\mu^{(1)}}\mathbf{V}^{(0)}$</td><td>由策略改进定理，$\mathbf{V}_{\pi^{(1)}}\ge \mathbf{V}_{\pi^{(1)}}=V^{(0)}\Rightarrow \mathbf{V}_{\pi^{(1)}}\ge \mathbf{V}^{(1)}$</td></tr><tr><td>Policy</td><td>$\pi^{(2)}=\mathop{\mathrm{argmax}}\limits_{\pi}(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(1)}})$</td><td>$\mu^{(2)}=\mathop{\mathrm{argmax}}\limits_{\mu}(\mathbf{R}_{\mu}+\gamma P_{\mu}\mathbf{V}^{(1)})$</td><td></td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><p>在价值迭代方法中，在价值更新步骤，只计算一轮就将 $V^{(1)}$ 代入下一轮策略更新</p><p>在策略迭代方法中，在价值评估步骤，利用贝尔曼方程求解当前策略下逼近解析解的数值解 $\mathbf{V}_{\pi^{(1)}}^{(j)}\xlongequal{j\rightarrow \infty}\mathbf{V}_{\pi^{(1)}}\rightarrow \mathbf{V}^*_{\pi^{(1)}}$ ；由策略改进定理，有 $\mathbf{V}_{\pi^{(1)}}\ge V^{(1)}$</p><script type="math/tex;mode=display">\begin{array}{rlll}
&&&\mathbf{V}_{\pi^{(1)}}^{(0)}=\mathbf{V}^{(0)}\\
价值迭代法&\leftarrow \mathbf{V}^{(1)}&\leftarrow&\mathbf{V}_{\pi^{(1)}}^{(1)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(0)}\\
&&&\mathbf{V}_{\pi^{(1)}}^{(2)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(1)}\\
&&&\vdots\\
截断策略迭代法&\leftarrow\overline{\mathbf{V}^{(1)}}&\leftarrow&\mathbf{V}_{\pi^{(1)}}^{(j)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(j-1)}\\
&&&\vdots\\
策略迭代法&\leftarrow \mathbf{V}_{\pi^{(1)}}&\leftarrow&\mathbf{V}_{\pi^{(1)}}^{(\infty)}=\mathbf{R}_{\pi^{(1)}}+\gamma P_{\pi^{(1)}}\mathbf{V}_{\pi^{(1)}}^{(\infty)}
\end{array}</script><ul><li>因为策略评估需要经过无穷多步骤，所以策略迭代法只在理论上存在</li><li>实际编程实现的都是截断策略迭代算法，当相邻两次的价值函数差距小于一个阈值，则认为收敛</li></ul><h5 id="其他方面对比"><a href="#其他方面对比" class="headerlink" title="其他方面对比"></a>其他方面对比</h5><div class="table-container"><table><thead><tr><th></th><th>策略迭代</th><th>价值迭代</th></tr></thead><tbody><tr><td>贝尔曼方程</td><td>贝尔曼期望方程</td><td>贝尔曼最优方程</td></tr><tr><td>策略提取</td><td>不管在那个状态，都可以利用状态对应的最佳策略到达可以获得最多奖励的状态</td><td>基于中间过程的价值函数所生成的策略没有意义</td></tr><tr><td></td><td>实时策略，在线学习</td><td>非实时策略，离线学习</td></tr></tbody></table></div><h4 id="截断策略迭代伪代码"><a href="#截断策略迭代伪代码" class="headerlink" title="截断策略迭代伪代码"></a>截断策略迭代伪代码</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化:动态特性与状态转移P(r\vert s,a)与P(s'\vert s,a)，随机初始化\pi^{(0)}\\
&目标：寻找最优状态价值与最优策略\\
&当策略未收敛，第 k轮迭代:\\
&\quad 策略评估:\\
&\qquad 初始化：一个随机初始化的状态价值\mathbf{V}^{(0)}_{\pi^{(k)}}=V_{\pi^{(k-1)}}，设定迭代次数超参数j_{truncate}\\
&\qquad 当j<j_{truncate}，进行第j+1轮迭代：\\
&\quad\qquad 对每个状态 s\in\mathcal{S}:\\
&\qquad\qquad \mathbf{V}_{\pi^{(k)}}^{(j+1)}(s)=\sum\limits_{a\in A}\pi^{(k)}(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma\sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k)}}^{(j)}(s')\right)\\
&\qquad 令V_{\pi^{(k)}}=V_{\pi^{(k)}}^{(j_{truncate})}
&\quad 策略改进:\\
&\qquad 对于每个状态s\in \mathcal{S}:\\
&\qquad \quad 对于每个动作 a\in \mathcal{A}(s):\\
&\qquad \qquad Q_{\pi^{(k)}}(s,a)=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\\
&\qquad \quad a_*^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)\\
&\quad \qquad 若a_*^{(k)}(s)==a ,则\pi^{(k+1)}(a\vert s)=1，否则\pi^{(k+1)}(a\vert s)=0\\
\hline
\end{array}</script><h4 id="截断策略迭代收敛性证明"><a href="#截断策略迭代收敛性证明" class="headerlink" title="截断策略迭代收敛性证明"></a>截断策略迭代收敛性证明</h4><p>策略评估中，迭代公式为</p><script type="math/tex;mode=display">\mathbf{V}^{(j+1)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(j)}_{\pi^{(k)}},j=0,1,2,\cdots</script><p>对于任一的初始值 $\mathbf{V}^{(0)}_{\pi^{(k)}}=V_{\pi^{(k-1)}}$ ，有</p><script type="math/tex;mode=display">\mathbf{V}^{(j+1)}_{\pi^{(k)}}\ge \mathbf{V}^{(j)}_{\pi^{(k)}},j=0,1,2,\cdots</script><p>证明：</p><p>由于 $\mathbf{V}^{(j+1)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(j)}_{\pi^{(k)}}$ 且 $\mathbf{V}^{(j)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(j-1)}_{\pi^{(k)}}$ ，则有</p><script type="math/tex;mode=display">\mathbf{V}^{(j+1)}_{\pi^{(k)}}-\mathbf{V}^{(j)}_{\pi^{(k)}}=\gamma P_{\pi^{(k)}}\left(\mathbf{V}^{(j)}_{\pi^{(k)}}-\mathbf{V}^{(j-1)}_{\pi^{(k)}}\right)=\cdots=\gamma^j P_{\pi^{(k)}}^j\left(\mathbf{V}^{(1)}_{\pi^{(k)}}-\mathbf{V}^{(0)}_{\pi^{(k)}}\right)</script><p>另外，由于 $\mathbf{V}^{(0)}_{\pi^{(k)}}=V_{\pi^{(k-1)}}$</p><script type="math/tex;mode=display">\mathbf{V}^{(1)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}^{(0)}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}V_{\pi^{(k-1)}}\ge \mathbf{R}_{\pi^{(k-1)}}+\gamma P_{\pi^{(k-1)}}V_{\pi^{(k-1)}}\xlongequal{贝尔曼方程}\mathbf{V}_{\pi^{(k-1)}}=\mathbf{V}^{(0)}_{\pi^{(k)}}</script><p>故可得，$\mathbf{V}^{(j+1)}_{\pi^{(k)}}\ge \mathbf{V}^{(j)}_{\pi^{(k)}},j=0,1,2,\cdots$</p><p><img src="/posts/1184554208/image-20240221225836947.png" alt="image-20240221225836947"></p><h4 id="截断策略迭代例子"><a href="#截断策略迭代例子" class="headerlink" title="截断策略迭代例子"></a>截断策略迭代例子</h4><p><img src="/posts/1184554208/image-20240221230035376.png" alt="image-20240221230035376"></p><p>策略评估求状态价值的迭代次数对收敛速度的影响</p><p><img src="/posts/1184554208/image-20240221230121741.png" alt="image-20240221230121741"></p><p>可见，在策略评估中，求解状态价值时，越多的迭代次数 $j_{truncate}$ ，则最终策略迭代会更快地收敛</p><p>但随着迭代次数的增加，带来的收敛加速度越来越小</p><p>在实践中，使用较小的迭代次数 $j_{truncate}$</p><h2 id="3-2-蒙特卡洛方法——免模型"><a href="#3-2-蒙特卡洛方法——免模型" class="headerlink" title="3.2 蒙特卡洛方法——免模型"></a>3.2 蒙特卡洛方法——免模型</h2><blockquote><p>基于蒙特卡洛方法的免模型学习是一种 <strong>广义策略迭代</strong> (generalized policy iteration, GPI)</p><ul><li>不断地在 <em>策略评估, PE</em> 与 <em>策略改进, PI</em> 之间切换</li><li>策略评估不需要精确的 $Q$ 值与 $V$ 值</li></ul></blockquote><ul><li><p>MC基本思想</p></li><li><p>MC Basic：实现从有模型学习到无模型学习的转化</p></li><li>MC探索性开始<ul><li>提高数据的使用效率</li></ul></li><li>去除探索性开始条件<ul><li>MC $\varepsilon-贪心$</li></ul></li></ul><h3 id="3-2-1-蒙特卡洛基本思想"><a href="#3-2-1-蒙特卡洛基本思想" class="headerlink" title="3.2.1 蒙特卡洛基本思想"></a>3.2.1 蒙特卡洛基本思想</h3><p>掷硬币，用随机变量 $X$ 表示结果，$+1,-1$ 表示两种结果，求期望 $E[X]$</p><ul><li><p>基于模型的方法</p><p>当我们已知硬币是均匀的，或任何能保证得出 $P(X=+1)=P(X=-1)=0.5$ 的条件，在这种先验知识的条件下，我们可以对实验环境进行建模，用 $P(X=+1)=P(X=-1)=0.5$ 表示环境模型，进而可基于模型的方法估计期望</p><p>$E[X]=\sum\limits_{x}xP(x)=0$</p></li><li><p>基于MC的方法：<strong>采样取平均</strong></p><p>在不知道任何先验知识的前提下，就无法对实验环境进行建模</p><p>无模型时用数据(经验,experience)：通过多次实验，可以得到样本集 $\{x_1,x_2,\cdots,x_N\}$</p><script type="math/tex;mode=display">E[X]\approx \overline{X}=\frac{1}{N}\sum\limits_{j=1}^Nx_j</script><p><img src="/posts/1184554208/image-20240223105012439.png" alt="image-20240223105012439"></p><p>可见，随着实验次数的增多，$\overline{X}\xrightarrow{N\rightarrow \infty}E[X]$</p></li></ul><h4 id="MC方法理论依据——大数定律"><a href="#MC方法理论依据——大数定律" class="headerlink" title="MC方法理论依据——大数定律"></a>MC方法理论依据——大数定律</h4><p>对于一个随机变量 $X$ ，假设 $\{x_j\}_{j=1}^{N}$ 是独立同分布样本集，令 $\overline{X}=\frac{1}{N}\sum\limits_{j=1}^Nx_j$ ，则有，</p><script type="math/tex;mode=display">E[\overline{X}]=E[X]\\
Var[\overline{X}]=\frac{1}{N}Var[X]</script><p>即 $\overline{X}$ 是 $E[X]$ 的无偏估计，且方差随着实验次数的增多减小</p><h3 id="3-2-2-MC-Basic"><a href="#3-2-2-MC-Basic" class="headerlink" title="3.2.2 MC Basic"></a>3.2.2 MC Basic</h3><h4 id="策略迭代分析"><a href="#策略迭代分析" class="headerlink" title="策略迭代分析"></a>策略迭代分析</h4><p>PE：$\mathbf{V}_{\pi^{(k)}}=\mathbf{R}_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}\mathbf{V}_{\pi^{(k)}}$</p><p>PI：$\pi^{(k+1)}=\mathop{\mathrm{argmax}}\limits_{\pi}\left(\mathbf{R}_{\pi}+\gamma P_{\pi}\mathbf{V}_{\pi^{(k)}}\right)$</p><p>对于策略改进步骤，针对每个状态 $s\in \mathcal{S}$</p><script type="math/tex;mode=display">\begin{aligned}
\pi^{(k+1)}(a\vert s)&=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)\left[\sum\limits_{r'}P(r'\vert s,a)r'+\sum\limits_{s'}P(s'\vert s,a)V_{\pi^{(k)}}(s')\right]\\
&=\mathop{\mathrm{argmax}}\limits_{\pi}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(k)}}(s,a)
\end{aligned}</script><p>若环境动态特性 $P(s’,r\vert s,a)$ 已知，可通过对动态规划方法计算 $Q_{\pi^{(k)}}(s,a)$</p><p>若环境动态特性 $P(s’,r\vert s,a)$ 已知，则采用蒙特卡洛方法计算</p><script type="math/tex;mode=display">Q_{\pi^{(k)}}(s,a)=E_{\pi^{(k)}}[G_{t}\vert s_t=s,a_t=a]\\
\downarrow\\
均值估计问题\leftarrow MC方法</script><h4 id="MC-Basic算法"><a href="#MC-Basic算法" class="headerlink" title="MC Basic算法"></a>MC Basic算法</h4><ol><li><p>从 $(s,a)$ 开始，基于策略 $\pi^{(k)}$ 生成轨迹 $\tau$</p></li><li><p>计算 $\tau$ 的回报 $g(s,a)$</p><script type="math/tex;mode=display">g^{(j)}(s,a)=r(s,a)+\gamma r(s_{t+1},a_{t+1})+\gamma^2r(s_{t+2},a_{t+2})+\cdots</script><ul><li>若采用确定性策略，且环境也是确定的。若估计 $Q(s_1,a_1)$ ，则基于当前策略 $\pi^{(k)}$ 从 $(s_1,a_1)$ 出发，不管采样多少次，得到的轨迹都是同一条，故只需要采样一次。因此估计整个Q表只需要45条轨迹，即 $N=1$</li><li>若采用随机性策略或确定性策略但环境是随机的。若估计 $Q(s_1,a_1)$ ，则基于当前策略 $\pi^{(k)}$ 从 $(s_1,a_1)$ 出发，需要采集从$(s_1,a_1)$ 出发的 $N$ 条轨迹才能估计其状态价值 $Q(s_1,a_1)$ 。因此估计整个Q表共需计算 $45\times N$ 条轨迹的回报，即 $N$ 为足够大的数值</li></ul></li><li><p>执行 $N$ 次 1,2，可得到一个回报集 $\{g^{(j)(s,a)}\}_{j=1}^N$ ，计算动作价值</p><script type="math/tex;mode=display">Q_{\pi^{(k)}}(s,a)=E_{\pi^{(k)}}[G_{t}\vert s_t=s,a_t=a]\approx\overline{g}(s,a)=\frac{1}{N}\sum\limits_{j=1}^Ng^{(j)}(s,a)</script></li></ol><h5 id="伪代码-2"><a href="#伪代码-2" class="headerlink" title="伪代码"></a>伪代码</h5><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：初始化策略 \pi^{(0)}\\
&目标：寻找最优策略\\
&当价值函数未收敛，进行第 k 轮迭代:\\
&\quad 对于每个状态 s\in \mathcal{S}:\\
&\qquad 对于每个动作a\in \mathcal{A}(s):\\
&\qquad \quad MC-based 策略评估:\\
&\qquad \qquad 以(s,a) 为起点，基于策略\pi^{(k)} 生成足够多的回合，计算这些回合的回报g(s,a)\\
&\qquad \qquad Q_{\pi^{(k)}}(s,a)=\overline{g}(s,a)\\
&\qquad MC-based 策略改进：\\
&\quad \qquad a^{(k)}_*=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)\\
&\quad \qquad 若a==a^{(k)}_*,则\pi^{(k+1)}(a\vert s)=1，否则\pi^{(k+1)}(a\vert s)=0\\
\hline
\end{array}</script><h4 id="MC-Basic例子"><a href="#MC-Basic例子" class="headerlink" title="MC Basic例子"></a>MC Basic例子</h4><p><img src="/posts/1184554208/image-20240223155537861.png" alt="image-20240223155537861"></p><ul><li>初始策略 $\pi^{(0)}$</li><li>奖励设置 $r_{boundary}=-1,r_{forbidden}=-1,r_{target}=1,\gamma=0.9$</li></ul><p>对于当前策略 $\pi^{(k)}$</p><ol><li><p>策略评估：计算 $Q_{\pi^{(k)}}(s,a)$</p><p>对于示例，有9个状态，每个状态由5个动作，即 $Q$ 表需要计算 $9\times 5=45$ 个 $Q$ 值，由于此例采用确定性策略，所以只需要45条轨迹</p></li><li><p>策略改进：贪心算法选择最优决策</p><script type="math/tex;mode=display">a_*^{(k)}(s)=\mathop{\mathrm{argmax}}\limits_{a}Q_{\pi^{(k)}}(s,a)</script></li></ol><p>以 $Q_{\pi^{(0)}}(s_1,a)$ 的计算为例</p><p><strong>1. 策略评估</strong></p><p>从 $(s_1,a_1)$ 开始，轨迹为 $s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_1)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{1}{1-\gamma}</script><p>从 $(s_1,a_2)$ 开始，轨迹为 $s_1\xrightarrow{a_2}s_2\xrightarrow{a_3}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9\xrightarrow{a_5}s_9\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_2)=0+\gamma(0)+\gamma^2(0)+\gamma^3(1)+\cdots=\frac{\gamma^3}{1-\gamma}</script><p>从 $(s_1,a_3)$ 开始，轨迹为 $s_1\xrightarrow{a_3}s_4\xrightarrow{a_2}s_5\xrightarrow{a_3}s_8\xrightarrow{a_2}s_9\xrightarrow{a_5}s_9\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_3)=0+\gamma(0)+\gamma^2(0)+\gamma^3(1)+\cdots=\frac{\gamma^3}{1-\gamma}</script><p>从 $(s_1,a_4)$ 开始，轨迹为 $s_1\xrightarrow{a_4}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_4)=-1+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{1}{1-\gamma}</script><p>从 $(s_1,a_5)$ 开始，轨迹为 $s_1\xrightarrow{a_5}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}s_1\xrightarrow{a_1}\cdots$ ，动作价值为</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_5)=0+\gamma(-1)+\gamma^2(-1)+\cdots=-\frac{\gamma}{1-\gamma}</script><p><strong>2. 策略改进</strong></p><p>通过观察动作价值，可知在 $s_1$ 时，最佳动作是 $a_2,a_3$</p><script type="math/tex;mode=display">Q_{\pi^{(0)}}(s_1,a_2)=Q_{\pi^{(0)}}(s_1,a_3)</script><p>因此，策略改进为</p><script type="math/tex;mode=display">\pi^{(1)}(a_2\vert s_1)或\pi^{(1)}(a_3\vert s_1)=1</script><h4 id="MC-Basic特点"><a href="#MC-Basic特点" class="headerlink" title="MC Basic特点"></a>MC Basic特点</h4><p>MCBasic算法是MC-based RL的核心，但由于其数据利用效率低，所以并不实用</p><p>因为策略迭代算法是收敛的，所以有足够回合数据的MC Basic也是收敛的</p><p>回合长度，<strong>直观上可以理解为探索半径</strong>。理论上越长越好，可以使计算出的回报更为精确，但实际上不可能无穷长，只要能让所有状态到达目标状态即可</p><h5 id="关于回合长度的例子"><a href="#关于回合长度的例子" class="headerlink" title="关于回合长度的例子"></a>关于回合长度的例子</h5><ul><li>奖励设置 $r_{boundary}=-1,r_{forbidden}=-10,r_{target}=1,\gamma=0.9$</li></ul><p>通过当前策略可到达目标状态的当前状态，其状态价值是正数；0表示未到达目标，若为负数，则表示有进入禁入区域或越界的中间状态</p><p><img src="/posts/1184554208/image-20240223165057968.png" alt="image-20240223165057968"></p><p>当回合长度为1时，只有目标状态一步之内的状态的最佳策略是正确的</p><p>当回合长度短时，只有离目标状态近的状态才有非0的状态价值，进而找到最优的策略；随着回合长度的增加，离目标状态远的状态才能到达目标状态，从而找到最优策略</p><p><img src="/posts/1184554208/image-20240223165703357.png" alt="image-20240223165703357"></p><p><img src="/posts/1184554208/image-20240223165756497.png" alt="image-20240223165756497"></p><h3 id="3-2-3-探索性开始"><a href="#3-2-3-探索性开始" class="headerlink" title="3.2.3 探索性开始"></a>3.2.3 探索性开始</h3><h4 id="探索性开始的必要性"><a href="#探索性开始的必要性" class="headerlink" title="探索性开始的必要性"></a>探索性开始的必要性</h4><p><strong>探索</strong>：确保每个 $(s,a)$ 都能被访问到</p><ul><li>需要保证从一个访问出发有多个回合，只有这样才能用后边的奖励来估计回报，进一步估计动作价值</li><li>同时，为了不遗漏最优决策，需要保证每个$(s,a)$ 都能够被访问到</li></ul><p><strong>开始</strong> ：要生成以 $(s,a)$ 开始的回合</p><ul><li><p>若要计算以 $(s,a)$ 开始的回合的回报，有两种方法</p><ul><li>start：计算以(s,a) 开始的回合</li></ul></li></ul><ul><li><p>visit 从其他 (s’,a’) 开始，中间经过 (s,a) ，将回合前截断，计算(s,a) 之后的回报</p><p>但由于轨迹的生成依赖于策略与环境，无法保证以(s,a) 开始仍能获取后续轨迹，即不保证可复现</p></li></ul><h4 id="提高策略评估的效率"><a href="#提高策略评估的效率" class="headerlink" title="提高策略评估的效率"></a>提高策略评估的效率</h4><blockquote><p>访问：在每个回合中，每出现一次 $(s_t,a_t)$ 都为对其的一次访问</p></blockquote><ul><li>数据的高效利用</li><li>策略更新时机</li></ul><h5 id="数据的高效利用"><a href="#数据的高效利用" class="headerlink" title="数据的高效利用"></a>数据的高效利用</h5><p>一个以 $(s_1,a_2)$ 开始的回合，仅用于计算 $Q(s_1,a_2)$ ，并为充分利用这一经验</p><p><strong>一个回合可以作为多个Q值的经验</strong></p><p><img src="/posts/1184554208/image-20240222174121229.png" alt="image-20240222174121229"></p><p>采用 <em>first-visit</em> 方法，对于出现在同一个回合中的多个同一访问 $(s_t,a_t)$ ，只将其第一次出现之后的轨迹作为 $(s_t,a_t)$ 的一条轨迹</p><p>采用 <em>every-visit</em> 方法，对于出现在同一个回合中的多个同一访问 $(s_t,a_t)$ ， 将每个 $(s_t,a_t)$ 之后的轨迹都作为 $(s_t,a_t)$ 的一条轨迹</p><h5 id="策略更新时机——离线变在线"><a href="#策略更新时机——离线变在线" class="headerlink" title="策略更新时机——离线变在线"></a>策略更新时机——离线变在线</h5><ul><li><p>在策略评估中，若评估 $(s_1,a_1)$ 的动作价值 $Q(s_1,a_1)$ ，则需要收集 $N$ 条以 $(s_1,a_1)$ 开始的轨迹，然后计算这些轨迹的回报均值去近似动作价值</p><p>被 MC Basic 算法采用，缺点是需要等所有的回合收集结束后再开始计算均值</p></li><li><p>使用一个回合的回报在线更新动作价值</p><p>优点：可以在线逐回合改进策略</p></li></ul><h4 id="伪代码：MC探索性开始"><a href="#伪代码：MC探索性开始" class="headerlink" title="伪代码：MC探索性开始"></a>伪代码：MC探索性开始</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：初始化策略\pi^{(0)},Q(s,a),R(s,a)=0,Num(s,a)=0,\forall s\in \mathcal{S},a\in \mathcal{A}(s)\\
&目标：寻找最优策略\\
&\\
&对于每个回合:\\
&\quad生成回合:随机选择回合起点(s_0,a_0)，并确保所有的访问都有可能被选择到。\\
&\quad \qquad\qquad 基于当前策略 \pi^{(k)}生成回合长度为T的回合:s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1}\\
&\quad 初始化:g\leftarrow 0\\
&\quad 对于一个回合的每一步，t=T-1,T-2,\cdots,0:\\
&\qquad g\leftarrow \gamma g+r_{t+1}\\
&\qquad 使用first-visit方法:\\
&\quad \qquad若(s_t,a_t)未出现在前子轨迹(s_0,a_0,s_1,a_1,\cdots,s_{t_1},a_{t-1}),则:\\
&\qquad \qquad R(s_t,a_t)\leftarrow R(s_t,a_t)+g //采用第一次访问策略，只有第一次出现才算(s_t,a_t)的经验\\
&\qquad \qquad Num(s_t,a_t)\leftarrow Num(s_t,a_t)+1\\
&\qquad \qquad 策略评估：\\
&\quad\qquad \qquad Q(s_t,a_t)=average(R(s_t,a_t))\\
&\qquad \qquad 策略改进：\\
&\quad \qquad \qquad \pi(a^*\vert s_t)=1,其中a^*=\mathop{\mathrm{argmax}}\limits_{a}Q(s_t,a) 
\\
\hline
\end{array}</script><h3 id="3-2-4-MC无探索性开始"><a href="#3-2-4-MC无探索性开始" class="headerlink" title="3.2.4 MC无探索性开始"></a>3.2.4 MC无探索性开始</h3><p>实际中，探索性开始很难实现。以每个 $(s,a)$ 为开始开始的回合收集成本很高</p><blockquote><p>软策略,soft policy：每个动作都有概率被采用的策略，称为soft policy</p><script type="math/tex;mode=display">\begin{cases}
确定性策略\leftarrow 贪心算法\\
随机性策略\leftarrow 软策略
\end{cases}</script></blockquote><p>soft policies：去除探索性开始的条件</p><ul><li>使用软策略，可以确保一些回合足够长，使得所有的 $(s,a)$ 都能出现足够的次数，只需要从一个或几个 $(s,a)$ 出发，就能覆盖到其他的访问，从而不需要大量的回合数据</li></ul><h4 id="varepsilon-贪心"><a href="#varepsilon-贪心" class="headerlink" title="$\varepsilon-贪心$"></a>$\varepsilon-贪心$</h4><p>在强化学习中，使用的软策略为 $\varepsilon-贪心$ 算法</p><script type="math/tex;mode=display">\begin{aligned}
&\pi(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,对于最优动作\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,对于其他 \vert \mathcal{A}(s)\vert-1个非最优动作
\end{cases}\\
&其中，\varepsilon\in[0,1]，\vert \mathcal{A}\vert(s)为状态s的动作数
\end{aligned}</script><ul><li>若 $\varepsilon=0.2$ ，$\vert \mathcal{A}\vert(s)=5$ ，则非最优动作的概率为 $\frac{0.2}{5}=0.04$ ，最优动作的概率为 $1-0.04\times 4=0.84$</li></ul><p>当前状态 $s$ 下的最优决策有最大可能被选择，同时其他非最优决策也有一定的概率被选择到</p><p>最优决策被选择的概率比非最优决策被选择的概率大</p><ul><li>$1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)=1-\varepsilon+\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}\ge \frac{\varepsilon}{\vert \mathcal{A}\vert(s)}$</li></ul><p>$\varepsilon-贪心$ 可以更好地平衡探索与利用</p><ul><li>$\varepsilon=0$ ，变为贪心策略，探索少但利用多</li><li>$\varepsilon=1$ ，每个动作都被平等探索，探索多但利用少</li></ul><h4 id="将-varepsilon-贪心-应用于强化学习"><a href="#将-varepsilon-贪心-应用于强化学习" class="headerlink" title="将 $\varepsilon-贪心$ 应用于强化学习"></a>将 $\varepsilon-贪心$ 应用于强化学习</h4><p>在MC Basic与MC探索性开始算法中，将策略改进步骤的贪心算法改为 $\varepsilon-贪心$ 算法</p><ul><li><p>贪心算法 $\pi^{(k+1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(k)}}(s,a)$</p><script type="math/tex;mode=display">\pi^{(k+1)}(a\vert s)=\begin{cases}
1,&a=a^{(k)}_*\\
0,&a\neq a^{(k)}_*
\end{cases}\\
其中,a^{(k)}_*=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi}Q_{\pi^{(k)}}(s,a)</script></li><li><p>$\varepsilon-贪心$ 算法 $\pi^{(k+1)}(a\vert s)=\mathop{\mathrm{argmax}}\limits_{\pi\in \Pi_{\varepsilon}}\sum\limits_{a}\pi(a\vert s)Q_{\pi^{(k)}}(s,a)$</p><script type="math/tex;mode=display">\pi^{(k+1)}(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,a=a^{(k)}_*\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,a\neq a^{(k)}_*
\end{cases}</script></li></ul><p>关于 $\Pi$ 与 $\Pi_{\varepsilon}$ 的理解</p><ul><li>$\Pi_{\varepsilon}$ 表示除最优决策外其他决策等可能的策略</li><li>$\Pi$ 表示所有可能的策略</li></ul><h5 id="伪代码-3"><a href="#伪代码-3" class="headerlink" title="伪代码"></a>伪代码</h5><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：初始化策略\pi^{(0)},\varepsilon\in [0,1],,Q(s,a),R(s,a)=0,Num(s,a)=0,\forall s\in \mathcal{S},a\in \mathcal{A}(s)\\
&目标：寻找最优策略\\
&\\
&对于每个回合:\\
&\quad 生成回合:随机选择回合起点(s_0,a_0),基于当前策略 \pi^{(k)}生成回合长度为T的回合:s_0,a_0,r_0,\cdots,s_{T-1},a_{T-1},r_{T-1}\\
&\quad 初始化:g\leftarrow 0\\
&\quad 对于一个回合的每一步，t=T-1,T-2,\cdots,0:\\
&\qquad g\leftarrow \gamma g+r_{t+1}\\
&\qquad 使用every-visit方法:\\
&\qquad \qquad R(s_t,a_t)\leftarrow R(s_t,a_t)+g\\
&\qquad \qquad Num(s_t,a_t)\leftarrow Num(s_t,a_t)+1\\
&\qquad \qquad 策略评估:\\
&\quad\qquad \qquad Q(s_t,a_t)=average(R(s_t,a_t))\\
&\qquad \qquad 策略改进:\\
&\quad\qquad \qquad 令a_*^{(k)}=\mathop{\mathrm{argmax}}\limits_{a}Q(s_t,a)，且\pi^{(k+1)}(a\vert s)=\begin{cases}
1-\frac{\varepsilon}{\vert \mathcal{A}(s)\vert}(\vert \mathcal{A}(s)\vert-1)&,a=a_*^{(k)}\\
\frac{\varepsilon}{\vert \mathcal{A}\vert(s)}&,a\neq a_*^{(k)}
\end{cases}\\
\hline
\end{array}</script><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>优点： $\varepsilon$ 越大，探索性越强，避开了探索性开始的条件</p><p>缺点：生成的最优策略的最优性越差</p><ul><li><p>由于 $\varepsilon-贪心$ 生成的最优策略 $\pi^*_{\varepsilon}$ 仅是属于 $\Pi_{\varepsilon}$ 中的最优策略</p></li><li><p><strong>$\varepsilon$不能太大</strong> ：当 $\varepsilon$ 小时，探索性小， $\varepsilon-贪心$ 就趋于贪心，用 $\varepsilon-贪心$ 找到的最优策略接近于贪心算法找到的最优策略</p><p><strong>实际中，开始时 $\varepsilon$ 比较大，让其具有较强的探索能力；之后让 $\varepsilon$ 趋向于0确保得到的策略具有最优性</strong></p></li></ul><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><h5 id="varepsilon-越大，探索性越强"><a href="#varepsilon-越大，探索性越强" class="headerlink" title="$\varepsilon$ 越大，探索性越强"></a>$\varepsilon$ 越大，探索性越强</h5><p><img src="/posts/1184554208/image-20240224102256712.png" alt="image-20240224102256712"></p><p>共125个 $(s,a)$，$\varepsilon$ 较大，探索性强，一个100 0000步的回合就探索了很多(s,a)</p><p>两个1000000步的回合就能找到基于 $\varepsilon-贪心$ 的最优策略</p><p><img src="/posts/1184554208/image-20240224094801791.png" alt="image-20240224094801791"></p><h5 id="牺牲最优性"><a href="#牺牲最优性" class="headerlink" title="牺牲最优性"></a>牺牲最优性</h5><p>$\varepsilon-贪心$ 通过探索性得到了一些好处，但同时牺牲了一些状态的最优性</p><p>$r_{boundary}=-1,r_{forbidden}=-10,r_{target}=1,\gamma=0.9$</p><p><img src="/posts/1184554208/image-20240222233322816.png" alt="image-20240222233322816"></p><p>一致性：在每个状态下，采取最优动作的可能性最大</p><p><img src="/posts/1184554208/image-20240224101526505.png" alt="image-20240224101526505"></p><p><img src="/posts/1184554208/image-20240224101639774.png" alt="image-20240224101639774"></p><p>最优策略是基于最优状态价值定义的，所以最优状态价值在反映策略的好坏</p><p>虽然所有的策略都与最优策略保持一致，但由于还可能采取其他非最优动作，所以状态价值会比贪心算法的最优状态价值小，其最优性越来越差</p><p>在实际应用中，我们会将基于 $\varepsilon-贪心$ 算法的最优策略转为基于贪心算法的策略，希望这个策略与最优策略保持一致。但随着 $\varepsilon$ 的增大，二者已不具有一致性，所以 $\varepsilon$ 不能太大。一个技巧就是$\varepsilon$ 先大后小</p><h2 id="3-3-随机近似与随机梯度下降"><a href="#3-3-随机近似与随机梯度下降" class="headerlink" title="3.3 随机近似与随机梯度下降"></a>3.3 随机近似与随机梯度下降</h2><blockquote><p><strong>时序差分方法</strong> 是随机近似的一种特殊情况</p></blockquote><ol><li><p>均值近似的不同理解</p><p>使用数据集 $\{x_k\}$ 估计均值 $E[X]$</p><script type="math/tex;mode=display">w_{k+1}=w_k-\frac{1}{k}(w_k-x_k)</script></li><li><p>Robbins-Monro 算法，RM算法</p><p>使用观测集 $\{\tilde{g}(w_k,\eta_k)\}$ 估计未知方程 $g(w)=0$ 的解</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\tilde{g}(w_k,\eta_k)</script><ul><li>算法描述</li><li>示例</li><li>收敛性分析</li><li>应用于均值近似</li></ul></li><li><p>随机梯度下降——RM算法的一种特殊情况</p><p>使用梯度集 $\{\bigtriangledown_wf(w_k,x_k)\}$ 求解最优化问题 $\min\limits_{w}J(w)=\min\limits_{w}E[f(w,x)]$ 的最优解，令 $g(w)=E[\bigtriangledown_wf(w,x)]=0$ ，梯度 $\bigtriangledown_wf(w_k,\eta_k)$ 相当于期望的带误差观测</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,\eta_k)</script><ul><li>算法描述</li><li>示例</li><li>收敛性</li><li>确定性公式</li><li>BGD，MBGD，SGD</li></ul></li></ol><h3 id="3-3-1-均值近似的不同理解"><a href="#3-3-1-均值近似的不同理解" class="headerlink" title="3.3.1 均值近似的不同理解"></a>3.3.1 均值近似的不同理解</h3><blockquote><p>对于一个随机变量，目标是估计其期望 $E[X]$ ，假设有一组独立同分布的数据集 $\{x_i\}_{i=1}^N$ ，期望可以用其均值近似 $E[X]\approx \overline{X}=\frac{1}{N}\sum\limits_{i=1}^Nx_i$</p></blockquote><p>强化学习中有很多期望，除价值函数外，还有很多量需要用数据去估计</p><h4 id="增量式均值计算方法"><a href="#增量式均值计算方法" class="headerlink" title="增量式均值计算方法"></a>增量式均值计算方法</h4><ul><li>收集所有的轨迹，再计算其均值：必须等 $N$ 条数据全部都收集完后，才能取平均</li><li>增量式(迭代式)</li></ul><p>对于非增量式计算</p><script type="math/tex;mode=display">w_{k+1}=\frac{1}{k}\sum\limits_{i=1}^kx_i,k=1,2,\cdots\\
w_{k}=\frac{1}{k-1}\sum\limits_{i=1}^{k-1}x_i,k=2,3\cdots</script><p>因此有</p><script type="math/tex;mode=display">\begin{aligned}
w_{k+1}&=\frac{1}{k}\sum\limits_{i=1}^kx_i=\frac{1}{k}\left(\sum\limits_{i=1}^{k-1}x_i+x_k\right)\\
&=\frac{1}{k}((k-1)w_{k}+x_k)\\
&=w_k-\frac{1}{k}(w_k-x_k)
\end{aligned}</script><p>利用上式可以增量式计算均值 $\overline{X}$</p><script type="math/tex;mode=display">\begin{array}{ll}
当有一个数据 &w_1=x_1\\
当有一个数据 &w_2=x_1\\
当有两个数据 &w_3=w_2-\frac{1}{2}(w_2-x_2)=\frac{1}{2}(x_1+x_2)\\
当有三个数据 &w_4=w_3-\frac{1}{3}(w_3-x_3)=\frac{1}{3}(x_1+x_2+x_3)\\
&\vdots\\
当有k个数据 &w_{k+1}=w_k-\frac{1}{k}(w_k-x_k)=\frac{1}{k}(x_1+x_2+\cdots+x_k)=\frac{1}{k}\sum\limits_{i=1}^kx_i
\end{array}</script><h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul><li>增量式均值估计每采样一次都能更新一次</li><li>均值估计一开始由于数据量不充分，所以并不精确 $(w_{k+1\neq E[X]})$ 。但随着样本数的增多，估计会越来越准确 $(w_{k+1}\xrightarrow{k\rightarrow N}E[X])$</li></ul><h4 id="广义增量式均值估计"><a href="#广义增量式均值估计" class="headerlink" title="广义增量式均值估计"></a>广义增量式均值估计</h4><script type="math/tex;mode=display">w_{k+1}=w_{k}-a_{k}(w_{k}-x_k)</script><p>将 $\frac{1}{k}$ 替换为 $\alpha_k$ ，仍可收敛于 $E[X]$</p><ul><li>属于特殊的 <strong>随机近似算法</strong></li><li>属于 <strong>随机梯度下降算法</strong></li></ul><h3 id="3-3-2-RM算法"><a href="#3-3-2-RM算法" class="headerlink" title="3.3.2 RM算法"></a>3.3.2 RM算法</h3><p>随机近似算法 (stochastic approximation, SA) ：用于方程求解或最优化问题的一类随机的、迭代的算法</p><ul><li>相较于其他方程求解算法(梯度上升/下降算法)，随机近似算法不需要知道方程的具体形式<ul><li>方程的表达式 $g$ 是已知的，可以通过很多数值算法求解</li><li>方程的表达式 $g$ 是未知的（神经网络：已知输入输出，但不知道具体的网络结构）</li></ul></li></ul><p>Robbins-Monro算法：是SA中的一种代表性算法</p><ul><li>随机梯度下降算法、增量式均值估计都属于RM算法</li></ul><p>方程求解问题应用于最优化求解：求解 $J(w)$ 的最优化问题，利用梯度下降法求解 $g(w)=\bigtriangledown_w J(w)=0$ 也是一个方程求解问题</p><p><strong>问题定义</strong></p><p>求解表达式未知的方程的根</p><script type="math/tex;mode=display">g(w)=0</script><p>其中，$w$ 为变量，$g(\cdot)$ 为表示未知方程的函数，$w^*$ 为方程的解</p><p><strong>求解</strong></p><p>RM算法是一种迭代式算法</p><script type="math/tex;mode=display">\begin{aligned}
w_{k+1}&=w_{k}-a_k\widetilde{g}(w_{k},\eta_{k})\\
&=w_k-a_k[g(w_{k})+\eta_{k}]
\end{aligned}</script><ul><li>$w_{k}$ 表示对 $w$ 的第 $k$ 次迭代</li><li>函数 $g(\cdot)$ 是一个黑盒函数， $g(w_k)$ 的准确值是未知的，但我们可以观测到关于 $g(w_k)$ 的带有噪音的观测值 $\tilde{g}(w_k,\eta_k)$</li></ul><p>通过RM算法求解方程的解依赖于数据</p><ul><li>输入序列：$\{w_{k}\}$</li><li>带噪音的输出序列：$\{\widetilde{g}(w_{k},\eta_{k})\}$</li></ul><h4 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h4><h5 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h5><p>估计 $g(w)=\tanh(w-1)=0$ 的解，初始 $w_1=3,a_k=\frac{1}{k},\eta_k\equiv0$ (为简化计算，假设无噪音干扰)</p><p><img src="/posts/1184554208/Figure_1.png" alt="Figure_1"></p><p>有解析解 $w^*=1$</p><p>由于噪音是0，所以本例中 $\tilde{g}(w_k,\eta_k)=g(w_k)$ ，基于以下公式计算</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_kg(w_k)</script><p><img src="/posts/1184554208/image-20240225111350459.png" alt="image-20240225111350459"></p><p>$w_2=w_1=3$ ，$w_3=w_2-a_2g(w_2)=3-\frac{1}{2}\tanh(3)=3-\frac{0.9950547536867305}{2}=2.5024726231566348$</p><p>直观上看，$w_{k+1}$ 会随着 $k$ 的迭代不断接近 $w^*$</p><ul><li>当 $w_k&gt;w^*$ 时，有 $g(w_k)&gt;0$ ， $w_{k+1}=w_k-a_kg(w_k)&lt;w_k$</li><li>当 $w_k<w^*$ 时，有 $g(w_k)<0$ ， $w_{k+1}="w_k-a_kg(w_k)">w_k$</li></ul><p>因此，$w_{k+1}$ 比 $w_k$ 更靠近 $w^*$</p><h5 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h5><p>RM算法 $w_{k+1}=w_{k}-a_k\widetilde{g}(w_{k},\eta_{k})$​ 收敛的充分条件是：</p><blockquote><ul><li>$0&lt; c_1\le \bigtriangledown_w g(w)\le c_2,\forall w$</li><li>$\sum\limits_{k=1}^{\infty}a_k=\infty$ 且 $\sum\limits_{k=1}^{\infty}a_k^2&lt;\infty$</li><li>$E[\eta_k\vert \mathcal{H}_k]=0$ 且 $E[\eta_k^2\vert \mathcal{H}_k]&lt;\infty$ ，$\mathcal{H}_k=\{w_k,w_{k-1,\cdots}\}$</li></ul><p>则 $w_k$ 依概率收敛于 $w^<em>$ 为方程的解 $g(w^</em>)=0$</p></blockquote><p><strong>1. $0&lt; c_1\le \bigtriangledown_w g(w)\le c_2,\forall w$ 的条件分析</strong></p><ul><li>导数大于0的条件：确保单调增，一定会有0点</li><li>导数有界，避免无穷的情况</li></ul><p>为什么单调增的 $g(w)$ 是可以接受的：随机近似常用于最优化方程的求解，即 $g(w)=\bigtriangledown_w J(w)=0$ ，对于凸优化问题，$J’’(w)\ge 0\iff g’(w)&gt;0$ ，所以是可接受的</p><p><strong>2. $E[\eta_k\vert \mathcal{H}_k]=0$ 且 $E[\eta_k^2\vert \mathcal{H}_k]&lt;\infty,\mathcal{H}_k=\{w_k,w_{k-1,\cdots}\}$</strong></p><p>噪音 $\{\eta_k\}$ 是一个独立同分布的随机序列，其均值应该为0($E[\eta_k\vert \mathcal{H}_k]=0$)，且方差应是有界的($E[\eta_k^2\vert \mathcal{H}_k]&lt;\infty,\mathcal{H}_k=\{w_k,w_{k-1,\cdots}\}$)，且 $\eta$ 并不需要服从高斯分布</p><p><strong>3. $\sum\limits_{k=1}^{\infty}a_k=\infty$ 且 $\sum\limits_{k=1}^{\infty}a_k^2&lt;\infty$ 的条件分析</strong></p><ul><li><p>$\sum\limits_{k=1}^{\infty}a_k^2&lt;\infty$ 确保 $a_k\xrightarrow{k\rightarrow \infty}0$</p><p>由于 $w_{k+1}-w_{k}=-a_k\tilde{g}(w_{k},\eta_{k})$ ，若 $a_k\rightarrow 0\Rightarrow a_k\tilde{g}(w_{k},\eta_{k})\rightarrow 0\Rightarrow w_{k+1}-w_{k}\rightarrow 0$ ，若想要 $w_k$ 收敛，则 $w_{k+1}-w_{k}\rightarrow 0$ 是一个充分条件条件</p><p>若 $w_k\rightarrow w^*$，则有 $g(w_k)\rightarrow 0$ 并且 $\tilde{g}(w_{k},\eta_{k})$ 受 $\eta_k$ 扰动</p></li><li><p>$\sum\limits_{k=1}^{\infty}a_k=\infty$ 确保 $a_k$ 趋于0的速度不会过快</p><p>由上，$w_2=w_1-a_1\tilde{g}(w_1,\eta_1)$ ，$w_3=w_2-a_2\tilde{g}(w_2,\eta_2)$ ，$\cdots ,w_{k+1}=w_k-a_k\tilde{g}(w_k,\eta_k)$ ，等号左右分别求和</p><script type="math/tex;mode=display">w_{k+1}-w_1=\sum\limits_{k=1}^{k}-a_k\tilde{g}(w_k,\eta_k)</script><p>若 $w_{\infty}=w^<em>$ ，假设 $\sum\limits_{k=1}^{\infty}a_k&lt;\infty$ ，则 $\sum\limits_{k=1}^{\infty}a_k\tilde{g}(w_k,\eta_k)$ 是有界的，则 $w^</em>$ 与 $w_{1}$ 的差距是有界的，意味着不能随便选择初始的 $w_1$ 。若选择的 $w_1$ 与精确值间距过大，超过这个界，则等式不成立，即 $w_{\infty}$ 不是方程的解</p><p>当 $\sum\limits_{k=1}^{\infty}a_k=\infty$ ，表示选择的初始 $w_1$ 不管离 $w^*$ 有多远，最后都能收敛到精确值</p></li></ul><h5 id="满足系数的条件的-a-k"><a href="#满足系数的条件的-a-k" class="headerlink" title="满足系数的条件的 $a_k$"></a>满足系数的条件的 $a_k$</h5><p>典型的是 $a_k=\frac{1}{k}$ ，级数 $\sum\limits_{k=1}^n\frac{1}{k}$ 在 $n\rightarrow \infty$ 时是发散的，且 $\sum\limits_{k=1}^{\infty}\frac{1}{k^2}=\frac{\pi^2}{6}&lt;\infty$</p><p>在强化学习中，$a_k$ 会选择足够小的常量，不会使用 $\frac{1}{k}$ ，这样会使 $\sum\limits_{k=1}^{\infty}a_k^2&lt;\infty$ 不成立，但算法依然会是有效的</p><ul><li>后续的数据，随 $k$ 的增大，其作用会非常小。在实际问题中，我们希望后续的数据也有用，所以不会让 $a_k\xrightarrow{k\rightarrow \infty}0$ ，而让其 $a_k\xrightarrow{k\rightarrow \infty}\varepsilon$ ，$\varepsilon$ 是一个非常小的数</li></ul><h4 id="RM算法应用于均值估计"><a href="#RM算法应用于均值估计" class="headerlink" title="RM算法应用于均值估计"></a>RM算法应用于均值估计</h4><script type="math/tex;mode=display">w_{k+1}=w_k-a_k(w_k-x_k)</script><p>我们知道若 $a_k=\frac{1}{k}$ ，则 $w_{k+1}=\frac{1}{k}\sum\limits_{i=1}^kx_i\xrightarrow{k\rightarrow \infty}E[X]$ 。但 $a_k\neq\frac{1}{k}$ 时，$w_k$ 是否仍能收敛与期望 $E[X]$ 目前并未分析过。基于RM算法，只需证明 <strong>广义增量式均值估计</strong> 属于RM算法即可。</p><p>在均值估计中，$g(w)=w_k-E[X]=0$ ，观测量 $\tilde{g}(w,\eta)=w-x$</p><script type="math/tex;mode=display">\begin{aligned}
\tilde{g}(w_k,\eta_k)&=w_k-x_k=w_k-E[X]+E[X]-x_k\\
&=(w_k-E[X])+(E[X]-x_k)=g(w_k)+\eta_k
\end{aligned}</script><h3 id="3-3-3-随机梯度下降"><a href="#3-3-3-随机梯度下降" class="headerlink" title="3.3.3 随机梯度下降"></a>3.3.3 随机梯度下降</h3><p><strong>问题定义</strong></p><script type="math/tex;mode=display">\min\limits_{w} J(w)=\min\limits_{w}E[f(w,X)]</script><p>$w$ 是最优化参数，$X$ 是随机变量，优化目标是以 $w$ 为参数最小化 $f(w,X)$ 的期望</p><ul><li>期望可以看做带权求和的函数，待解决的问题可以看做多元函数最优化问题</li></ul><p><strong>方法</strong></p><ol><li><p>梯度下降法（gtradient descent,GD）</p><p>找目标函数的最小值，所以使用梯度下降法</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\underbrace{\bigtriangledown_w E[f(w,X)]}_{\bigtriangledown_w J(w)}\xlongequal{期望本质上是求和}w_k-a_kE[\bigtriangledown_wf(w,X)]</script><p>沿梯度方向，函数值减少最快，可以更快地收敛到最小值</p><p>缺点：期望的计算</p><ul><li>有模型，用公式求</li><li>无模型，用数据求</li></ul></li><li><p>批量梯度下降（batch gradient descent, BGD）</p><p>MC方法： $E[\bigtriangledown_wf(w,X)]\approx\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_i,x_i)$</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_i,x_i)</script><p>缺点：每次迭代 $w_k$ 都需要采样很多次数 $N$ 次</p></li><li><p>随机梯度下降（stochastic gradient descent, SGD）</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,x_k)</script><ul><li>将 <strong>梯度下降法</strong> 中，真实的梯度替换为对梯度的采样 $\bigtriangledown_w f(w_k,x_k)$</li><li>将 <strong>批量梯度下降法</strong> 中，令 $n=1$</li></ul></li></ol><h4 id="算法示例"><a href="#算法示例" class="headerlink" title="算法示例"></a>算法示例</h4><p>相当于最小化目标 $w$ 与变量 $X$ 距离的期望</p><script type="math/tex;mode=display">\min\limits_{w}J(w)=\min\limits_{w}E[f(w,X)]=\min\limits_{w}E\left[\frac{1}{2}\Vert w-X \Vert^2\right]</script><p><strong>1. 最优解 $w^*$ 为 $E[X]$</strong></p><p>求解最小化 $\min\limits_{w}J(w)$ ，即令 $\bigtriangledown_wJ(w)=0$</p><script type="math/tex;mode=display">\bigtriangledown_wJ(w)=0\Rightarrow \bigtriangledown_wE[f(w,X)]=0\Rightarrow E[\bigtriangledown_wf(w,X)]=0\Rightarrow E[w^*-X]=0\Rightarrow w^*=E[X]</script><p><strong>2. 梯度下降法求解最小值</strong></p><script type="math/tex;mode=display">\begin{aligned}
w_{k+1}&=w_k-a_k\bigtriangledown_wJ(w_k)\\
&=w_k-a_kE[\bigtriangledown_w f(w_k,X)]\\
&=w_k-a_kE[w_k-X]\\
&=w_k-a_kw_k+a_kE[X]
\end{aligned}</script><p>每轮 $w_k$ 都需要估计一次期望 $E[X]$</p><p><strong>3. 随机梯度下降法求解最小值</strong></p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,x_k)=w_k-a_k(w_k-x_k)</script><h4 id="收敛性-1"><a href="#收敛性-1" class="headerlink" title="收敛性"></a>收敛性</h4><blockquote><p>在梯度下降法中，目标是求解最小值，所以沿着 $w_k$ 梯度方向会更快地下降到 $w_k$ 方向上的最小值，我们知道 $w_{k}\xrightarrow{k\rightarrow \infty}w^*$；而随机梯度下降法，用 $(w_k,x_k)$ 处的梯度代替取 $w=w_k$ 时所有点 $(w_k,x_i),\forall i$ 处的梯度均值，即一个数据代替数据整体的梯度</p><script type="math/tex;mode=display">GD:w_{k+1}=w_k-a_kE[\bigtriangledown_wf(w_k,X)]\\
\Downarrow\\
SGD:w_{k+1}=w_k-a_k\bigtriangledown_wf(w_k,x_k)</script><p>因为 $\bigtriangledown_wf(w_k,x_k)\neq E[\bigtriangledown_wf(w_k,X)]$ ，需要证明基于SGD，$w_k\xrightarrow{\infty}w^*$ 是否成立</p></blockquote><p>证明思路： <strong>随机梯度下降法</strong> 是一个特殊的 <strong>随机近似算法</strong> ，且满足随机近似收敛的充分条件，则 <strong>随机梯度下降法</strong> 也会收敛</p><p>优化目标是最小化期望</p><script type="math/tex;mode=display">\min\limits_wJ(w)=\min\limits_{w} E[f(w,X)]</script><p>最优化问题使用梯度下降法，等价于求解方程 $\bigtriangledown_w J(w)=E[\bigtriangledown_wf(w_k,X)]=0$ 的解 $w^*$</p><p>对于这么一个未知方程的求解问题，可以用RM算法求解，其观测量为 $\tilde{g}(w,\eta)$</p><script type="math/tex;mode=display">\begin{aligned}
\tilde{g}(w,\eta)&=\bigtriangledown_w f(w,x)\\
&=\underbrace{E[\bigtriangledown_wf(w,X)]}_{g(w)}+\underbrace{\bigtriangledown_wf(w,x)-E[\bigtriangledown_wf(w,X)]}_{\eta}
\end{aligned}</script><ul><li>$\bigtriangledown_wf(w_k,x_k)$ 可以看做 $E[\bigtriangledown_wf(w_k,X)]$ 的一个噪音度量</li></ul><p>因此，SGD是一种特殊的 RM 算法。若满足RM算法收敛的三个条件</p><ul><li>$0&lt;c_1\le \bigtriangledown^2_w f(w,X)\le c_2$</li><li>$\sum\limits_{k=1}^{\infty}a_k=\infty$ 且 $\sum\limits_{k=1}^{\infty}a_k^2&lt;\infty$</li><li>$\{x_k\}_{k=1}^{\infty}$ 是独立同分布的</li></ul><p>则SGD一定收敛于 $w^*$</p><h5 id="SGD收敛的性质"><a href="#SGD收敛的性质" class="headerlink" title="SGD收敛的性质"></a>SGD收敛的性质</h5><p>SGD中，用随机梯度代替真实梯度，梯度的随机性并不会造成收敛的随机性或慢收敛，即初始值的选择 $w_0$ 是否会影响收敛速度</p><ul><li>当 $w_k$ 与 $w^*$ 差距大时 ，SGD表现与GD相同，因为相对误差比较小</li><li>当 $w_k$ 与 $w^*$ 差距小时，SGD才会呈现较大的随机性</li></ul><p>引入随机梯度与批量梯度的相对误差</p><script type="math/tex;mode=display">\delta_k=\frac{\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\vert E[\bigtriangledown_w f(w_k,X)]\vert}</script><p>对于最优解 $w^<em>$ 满足梯度 $E[\bigtriangledown_w f(w^</em>,X)]=0$ ，进而有</p><script type="math/tex;mode=display">\delta_k=\frac{\big\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\vert E[\bigtriangledown_w f(w_k,X)]-E[\bigtriangledown_w f(w^*,X)]\vert}=\frac{\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\big\vert E[\left(\bigtriangledown_w f(w_k,X)-\bigtriangledown_wf(w^*,X)\right)]\big\vert}\xlongequal{中值定理}\frac{\vert \bigtriangledown_wf(w_k,x_k)- E[\bigtriangledown_w f(w_k,X)]\vert}{\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)(w_k-w^*)]\big\vert},\widetilde{w_k}\in [w_k,w^*]</script><p>假设 $f$ 是一个严格凸函数，$\bigtriangledown_w^2 f\ge c&gt;0,\forall w,X$ ，则相对误差的分母为</p><script type="math/tex;mode=display">\begin{aligned}
\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)(w_k-w^*)]\big\vert&=\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)](w_k-w^*)\big\vert\\
&=\big\vert E[\bigtriangledown_w^2 f(\widetilde{w_k},X)]\big\vert\big\vert(w_k-w^*)\big\vert\ge c\vert(w_k-w^*)\vert
\end{aligned}</script><p>代入相对误差</p><script type="math/tex;mode=display">\delta_k\le \frac{\vert \overbrace{\overbrace{\bigtriangledown_wf(w_k,x_k)}^{随机梯度}- \overbrace{E[\bigtriangledown_w f(w_k,X)]}^{真实梯度}}^{绝对误差}\vert}{c\underbrace{\vert(w_k-w^*)\vert}_{与最优解的间距}}</script><p>相对误差 $\delta_k$ 反比于与最优解的间距：</p><ul><li>分母大的时候，相对误差上界相对小，绝对误差也相对小</li><li>分母小的时候，相对误差上界相对大，绝对误差也相对大</li></ul><p>当 $w_k$ 与 $w^*$ 距离远时，SGD的参数调整方向与GD的方向大致相同</p><h4 id="SGD的确定性公式"><a href="#SGD的确定性公式" class="headerlink" title="SGD的确定性公式"></a>SGD的确定性公式</h4><p>常见的最优化问题是</p><script type="math/tex;mode=display">\min_w J(w)=\min_w\frac{1}{n}\sum\limits_{i=1}^nf(w,x_i)</script><p>$x_i$ 并不是一个随机变量，只是来源于独立同分布的实数集 $\{x_i\}_{i=1}^n$ ，使用梯度下降法最优化这样的带参函数 $f(w,x_i)$</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\bigtriangledown_w J(w)=w_k-a_k\sum\limits_{i=1}^n\frac{1}{n}\bigtriangledown_wf(w,x_i)</script><p>假设数据集很大，即 $n$ 很大，则可使用增量式梯度下降——SGD确定性公式</p><script type="math/tex;mode=display">w_{k+1}=w_k-a_k\bigtriangledown_w J(w)=w_k-a_k\bigtriangledown_wf(w_k,x_k)</script><p>这样的梯度下降公式虽然与 SGD相似，但它不含期望与随机变量。通过引入一个随机变量将确定性最优化问题变为随机最优化问题，假设 $X$ 是一个由 $\{x_i\}_{i=1}^n$ 定义的随机变量，且服从均匀分布 $P(X=x_i)=\frac{1}{n}$</p><script type="math/tex;mode=display">\min\limits_{w}J(w)=\min\limits_{w}\frac{1}{n}\sum\limits_{i=1}^nf(w,x_i)=\min_{w}E[f(w,X)]</script><p>所以也可以用SGD求解最优化，即上述SGD确定性公式</p><ul><li>关于 $x_k$ 的选取：从 $\{x_i\}_{i=1}^n$ 中有放回地随机取出 $x_k$</li></ul><h3 id="3-3-4-BGD、SGD、MBGD对比"><a href="#3-3-4-BGD、SGD、MBGD对比" class="headerlink" title="3.3.4 BGD、SGD、MBGD对比"></a>3.3.4 BGD、SGD、MBGD对比</h3><p>对于同一个最优化问题 $\min\limits_{w}J(w)=\min_{w}E[f(w,X)]$ ，随机变量 $X$ 的样本集为 $\{x_i\}_{i=1}^n$</p><script type="math/tex;mode=display">\begin{array}{ll}
w_{k+1}=w_k-a_k\frac{1}{n}\sum\limits_{i=1}^n\bigtriangledown_wf(w_k,x_i)&(BGD)\\
w_{k+1}=w_k-a_k\frac{1}{m}\sum\limits_{x_i\in \mathcal{I}_k}\bigtriangledown_wf(w_k,x_i)&(MBGD)\\
w_{k+1}=w_k-a_k\frac{1}{n}\bigtriangledown_wf(w_k,x_k)&(SGD)\\
\end{array}</script><p>BGD，每轮迭代需要 $n$ 个数据，最优化结果最接近真实的最优解</p><p>MBGD，从 $n$ 个数据中采集 $m&lt;n$ 个数据生成数据集 $\mathcal{I}_k$ ，作为第 $k$ 轮迭代的数据，基于这个子集的梯度计算最优解</p><p>SGD，每轮迭代随机选择一个数据，计算其梯度</p><p>相较于SGD，MBGD使用了更多数据将噪音等测量平均掉，所以随机性更小</p><p>相较于BGD，MBGD不需要使用全部数据，所以更加灵活和高效</p><ul><li><p>当 $m=1$ ，MBGD变为SGD</p></li><li><p>当 $m=n$ ，MBGD变为BGD</p><p>BGD用到 $n$ 个数据，虽然 MBGD也会用 $n$ 个数据，但是在BGD的 $n$ 个数据中有重复采样</p></li></ul><h4 id="算法示例-1"><a href="#算法示例-1" class="headerlink" title="算法示例"></a>算法示例</h4><p>对于给定的数据集 $\{x_i\}_{i=1}^{n}$ ，目标是最优化 $\overline{X}=\frac{1}{n}\sum\limits_{i=1}^nx_i$ ，数学公式表示为</p><script type="math/tex;mode=display">\min\limits_{w}J(w)=\min\limits_{w}\frac{1}{2n}\sum\limits_{i=1}^n\Vert w-x_i\Vert^2</script><p>使用梯度下降法求解，其中 $\bigtriangledown_wJ(w)=\frac{1}{n}\sum\limits_{i=1}^n(w-x_i)$</p><script type="math/tex;mode=display">\begin{array}{ll}
w_{k+1}=w_k-a_k\frac{1}{n}\sum\limits_{i=1}^n(w_k-x_i)=w_k-a_k(w_k-\overline{x})&(BGD)\\
w_{k+1}=w_k-a_k\frac{1}{m}\sum\limits_{i=1}^{m}(w_k-x_i)=w_k-a_k(w_k-\overline{x}_k^{(m)})&(MBGD)\\
w_{k+1}=w_k-a_k(w_k-x_k)&(SGD)\\
\end{array}</script><h5 id="收敛速度对比"><a href="#收敛速度对比" class="headerlink" title="收敛速度对比"></a>收敛速度对比</h5><p>$X\in \R^2$ 表示一个平面中的随机位置，其分布服从 $U(20,20)$ 的均匀分布，则真实均值 $E[X]=\mathbf{0}$，基于100个独立同分布的样本 $\{x_i\}_{i=1}^{100}$ ，令 $a_k=\frac{1}{k}$，求均值</p><p><img src="/posts/1184554208/image-20240227163416632.png" alt="image-20240227163416632"></p><p><img src="/posts/1184554208/image-20240227163505422.png" alt="image-20240227163505422"></p><p>观察橙色线（基于随机梯度下降的最优化），尽管初始值与真实期望差距很大，但SGD朝着最优解的方向收敛，当接近最优解时，呈现随机性，但仍逐渐收敛于真实值</p><p>若 $a_k=\frac{1}{k}$</p><script type="math/tex;mode=display">\begin{array}{ll}
w_{k+1}=w_k-\frac{1}{k}(w_k-\overline{x})\xlongequal{迭代推导}\frac{1}{k}\sum\limits_{i=1}^kx_i=\frac{1}{k}\sum\limits_{i=1}^k\overline{x}=\overline{x}&(BGD)\\
w_{k+1}=\frac{1}{k}\sum\limits_{i=1}^k\overline{x}_k^{(m)}=\overline{x}_k^{(m)}&(MBGD)\\
w_{k+1}=\frac{k-1}{k}w_k+\frac{1}{k}x_k=\frac{1}{k}\sum\limits_{i=1}^kx_i=\frac{1}{k}\overline{x}&(SGD)  
\end{array}</script><p>BGD的每一步迭代都是最优解 $w^*=\overline{x}$</p><p>由于 $\overline{x}_k^{(m)}$ 已经是一个均值，所以MBGD收敛于均值的速度快于SGD，且 $m$ 越大，MBGD的收敛速度越快</p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/1184554208/" title="3-基于价值的决策">https://amostian.github.io/posts/1184554208/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 强化学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3236386842/" rel="prev" title="0-强化学习资料"><i class="fa fa-chevron-left"></i> 0-强化学习资料</a></div><div class="post-nav-item"><a href="/posts/0/" rel="next"><i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-0-%E7%AD%96%E7%95%A5%E7%9A%84%E8%8E%B7%E5%8F%96"><span class="nav-text">3.0 策略的获取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-0-1-%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">3.0.1 价值函数的计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-0-2-%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5%E7%9A%84%E8%8E%B7%E5%8F%96"><span class="nav-text">3.0.2 最优策略的获取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E6%9C%89%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.1 动态规划方法——有模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">3.1.1 价值迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86"><span class="nav-text">最优性原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AE%E5%AE%9A%E6%80%A7%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">确定性价值迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-text">迭代算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-text">伪代码</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%BE%8B%E5%AD%90"><span class="nav-text">价值迭代例子</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90"><span class="nav-text">价值迭代分析</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%90%86%E8%A7%A3"><span class="nav-text">价值迭代理解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%AD%E9%97%B4%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC-%E7%AD%96%E7%95%A5%E6%97%A0%E6%84%8F%E4%B9%89"><span class="nav-text">中间状态价值&#x2F;策略无意义</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-text">3.1.2 策略迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-text">策略迭代算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="nav-text">策略评估(预测问题)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B"><span class="nav-text">策略改进</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81-1"><span class="nav-text">伪代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E6%94%B6%E6%95%9B%E6%80%A7"><span class="nav-text">策略迭代收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%9A%84%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E5%BF%AB%E4%BA%8E%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">策略迭代的收敛速度快于价值迭代</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E4%BE%8B%E5%AD%90"><span class="nav-text">策略迭代例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E4%BE%8B%E5%AD%90%E7%90%86%E8%A7%A3%E5%BC%BA%E5%8C%96"><span class="nav-text">通过策略迭代例子理解强化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-%E6%88%AA%E6%96%AD%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-text">3.1.3 截断策略迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E6%A0%BC%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E4%B8%8E%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">网格世界中的策略迭代与价值迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E5%AF%B9%E6%AF%94"><span class="nav-text">价值迭代与策略迭代对比</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E5%BA%8F%E5%88%97"><span class="nav-text">变量序列</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="nav-text">算法对比</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%96%B9%E9%9D%A2%E5%AF%B9%E6%AF%94"><span class="nav-text">其他方面对比</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%88%AA%E6%96%AD%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-text">截断策略迭代伪代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%88%AA%E6%96%AD%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E6%94%B6%E6%95%9B%E6%80%A7%E8%AF%81%E6%98%8E"><span class="nav-text">截断策略迭代收敛性证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%88%AA%E6%96%AD%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E4%BE%8B%E5%AD%90"><span class="nav-text">截断策略迭代例子</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%85%8D%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.2 蒙特卡洛方法——免模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-text">3.2.1 蒙特卡洛基本思想</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MC%E6%96%B9%E6%B3%95%E7%90%86%E8%AE%BA%E4%BE%9D%E6%8D%AE%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B"><span class="nav-text">MC方法理论依据——大数定律</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-MC-Basic"><span class="nav-text">3.2.2 MC Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90"><span class="nav-text">策略迭代分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-Basic%E7%AE%97%E6%B3%95"><span class="nav-text">MC Basic算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81-2"><span class="nav-text">伪代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-Basic%E4%BE%8B%E5%AD%90"><span class="nav-text">MC Basic例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MC-Basic%E7%89%B9%E7%82%B9"><span class="nav-text">MC Basic特点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E5%9B%9E%E5%90%88%E9%95%BF%E5%BA%A6%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">关于回合长度的例子</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">3.2.3 探索性开始</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="nav-text">探索性开始的必要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E9%AB%98%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E7%9A%84%E6%95%88%E7%8E%87"><span class="nav-text">提高策略评估的效率</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E9%AB%98%E6%95%88%E5%88%A9%E7%94%A8"><span class="nav-text">数据的高效利用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0%E6%97%B6%E6%9C%BA%E2%80%94%E2%80%94%E7%A6%BB%E7%BA%BF%E5%8F%98%E5%9C%A8%E7%BA%BF"><span class="nav-text">策略更新时机——离线变在线</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81%EF%BC%9AMC%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">伪代码：MC探索性开始</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-4-MC%E6%97%A0%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">3.2.4 MC无探索性开始</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#varepsilon-%E8%B4%AA%E5%BF%83"><span class="nav-text">$\varepsilon-贪心$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%86-varepsilon-%E8%B4%AA%E5%BF%83-%E5%BA%94%E7%94%A8%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">将 $\varepsilon-贪心$ 应用于强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81-3"><span class="nav-text">伪代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E7%82%B9"><span class="nav-text">特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90"><span class="nav-text">例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#varepsilon-%E8%B6%8A%E5%A4%A7%EF%BC%8C%E6%8E%A2%E7%B4%A2%E6%80%A7%E8%B6%8A%E5%BC%BA"><span class="nav-text">$\varepsilon$ 越大，探索性越强</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%BA%E7%89%B2%E6%9C%80%E4%BC%98%E6%80%A7"><span class="nav-text">牺牲最优性</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E9%9A%8F%E6%9C%BA%E8%BF%91%E4%BC%BC%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">3.3 随机近似与随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E5%9D%87%E5%80%BC%E8%BF%91%E4%BC%BC%E7%9A%84%E4%B8%8D%E5%90%8C%E7%90%86%E8%A7%A3"><span class="nav-text">3.3.1 均值近似的不同理解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E5%9D%87%E5%80%BC%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-text">增量式均值计算方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E7%82%B9-1"><span class="nav-text">特点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E5%A2%9E%E9%87%8F%E5%BC%8F%E5%9D%87%E5%80%BC%E4%BC%B0%E8%AE%A1"><span class="nav-text">广义增量式均值估计</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-RM%E7%AE%97%E6%B3%95"><span class="nav-text">3.3.2 RM算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%80%A7"><span class="nav-text">收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90-1"><span class="nav-text">例子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%81%E6%98%8E"><span class="nav-text">证明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BB%A1%E8%B6%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E6%9D%A1%E4%BB%B6%E7%9A%84-a-k"><span class="nav-text">满足系数的条件的 $a_k$</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RM%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%E4%BA%8E%E5%9D%87%E5%80%BC%E4%BC%B0%E8%AE%A1"><span class="nav-text">RM算法应用于均值估计</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">3.3.3 随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%A4%BA%E4%BE%8B"><span class="nav-text">算法示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%80%A7-1"><span class="nav-text">收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SGD%E6%94%B6%E6%95%9B%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="nav-text">SGD收敛的性质</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%85%AC%E5%BC%8F"><span class="nav-text">SGD的确定性公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-4-BGD%E3%80%81SGD%E3%80%81MBGD%E5%AF%B9%E6%AF%94"><span class="nav-text">3.3.4 BGD、SGD、MBGD对比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%A4%BA%E4%BE%8B-1"><span class="nav-text">算法示例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E5%AF%B9%E6%AF%94"><span class="nav-text">收敛速度对比</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">356</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">58</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">74</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">770.3k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">32:11</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>