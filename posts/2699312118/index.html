<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="9.动手学深度学习-经典循环神经网络"><meta property="og:url" content="https://amostian.github.io/posts/2699312118/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240413113046624.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240401155051159.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407101355042.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407103134627.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407103538566.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407134733719.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240405013034969.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407145619349.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240405014237128.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407145450768.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407160511029.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407161153837.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407162021676.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407162638655.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407165124433.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407165135826.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240407232518186.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408015730339.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/86390fcfb156fee777a636ec9f70f01a.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408145702650.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408163046075.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408173857167.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408173400912.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408174035997.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240413184724630.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240413194025317.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240408221457210.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240409103816847.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240409103914543.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240409104505573.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410001959811.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410012002938.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410102345041.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410102423598.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410102154589.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/d3fb1230446f4f08e0313ce89f703b75a59a3e58.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410001959811.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410104005425.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410165521289.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410170543002.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20240410173453704.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013103557927.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013104135009.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013104732726.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013105018503.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013105058398.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013124904362.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231013130523936.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231015121736584.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231015121856862.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231015121943840.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231015122048994.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231015122131583.png"><meta property="og:image" content="https://amostian.github.io/posts/2699312118/image-20231015122142678.png"><meta property="article:published_time" content="2024-04-07T02:17:20.000Z"><meta property="article:modified_time" content="2024-04-13T15:38:25.000Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/2699312118/image-20240413113046624.png"><link rel="canonical" href="https://amostian.github.io/posts/2699312118/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>9.动手学深度学习-经典循环神经网络 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">65</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">82</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">218</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/2699312118/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">9.动手学深度学习-经典循环神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-04-07 10:17:20" itemprop="dateCreated datePublished" datetime="2024-04-07T10:17:20+08:00">2024-04-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-04-13 23:38:25" itemprop="dateModified" datetime="2024-04-13T23:38:25+08:00">2024-04-13</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">16.7k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>45 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h2 id="9-1-RNN改进"><a href="#9-1-RNN改进" class="headerlink" title="9.1 RNN改进"></a>9.1 RNN改进</h2><p>由于循环神经网络使用的非线性激活函数为 $Logistic$ 函数或 $Tanh$ 函数，其导数值都小于1，并且权重矩阵 $\Vert \mathbf{W}_{hh}\Vert$ 也不会太大，因此如果时间间隔 $T-u$ 过大，$\delta_{T,u}$ 会趋于0，因而经常出现梯度消失问题</p><p>若将模型修改为</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{h}_{t}&=\mathbf{h}_{t-1}+g\left(\mathbf{x}_{t};\theta\right)\\
&=\mathbf{h}_{t-1}+\sigma\left(\mathbf{x}_{t}\mathbf{W}_{hx}^{\mathsf{T}}+\mathbf{b}_h\right)
\end{aligned}</script><p>$\mathbf{h}_{t}$ 与 $\mathbf{h}_{t-1}$ 为线性依赖关系，且权重系数为 $\mathbf{I}$ ，这样可以消除梯度爆炸或梯度消失问题。但同时丢失了隐状态神经元在自反馈边上的非线性激活性质。因此，将模型进一步修改为</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{h}_{t}&=\mathbf{h}_{t-1}+g\left(\mathbf{x}_{t},\mathbf{h}_{t-1};\theta\right)\\
&=\mathbf{h}_{t-1}+\sigma\left(\mathbf{x}_{t}\mathbf{W}_{hx}^{\mathsf{T}}+\mathbf{h}_{t-1}\mathbf{W}_{hh}^{\mathsf{T}}+\mathbf{b}_h\right)
\end{aligned}</script><p><img src="/posts/2699312118/image-20240413113046624.png" alt="image-20240413113046624"></p><ul><li><p>类似于残差网络的形式</p><p><img src="/posts/2699312118/image-20240401155051159.png" alt="image-20240401155051159"></p></li></ul><p>同样，在计算梯度时，仍存在梯度爆炸问题</p><p>此外，还存在 <strong>记忆容量</strong> 问题：</p><ul><li>随着 $h_t$ 的不断累计存储，会发生饱和线性</li><li>若 $g(\cdot)$ 为 $Logistic$ 函数，随时间 $t$ 的增长，$h_t$ 会变得越来越大，从而导致 $h$ 变得饱和。随着记忆单元存储的内容越来越多，其(“随机”)丢失的信息也越来越多</li></ul><h3 id="9-1-1-基于门控机制的循环神经网络GRU"><a href="#9-1-1-基于门控机制的循环神经网络GRU" class="headerlink" title="9.1.1 基于门控机制的循环神经网络GRU"></a>9.1.1 基于门控机制的循环神经网络GRU</h3><p>为改善长程依赖问题，使用一种基于 <strong>残差网络思想</strong> 的模型修改方法，但梯度爆炸、记忆容量问题仍存在，通过引入门控机制进一步改进模型</p><ul><li><p>门控机制：控制信息的累计速度（有选择地加入新信息，并选择性地遗忘之前累计的信息）</p><p>在数字电路中，<strong>门</strong> 为一个二值变量 $\{0,1\}$</p><ul><li>$0$ 表示关闭状态，不允许任何信息通过</li><li>$1$ 表示开放状态，允许所有信息通过</li></ul></li></ul><p>这类网络称为 <strong>基于门控的循环神经网络 (Gated RNN)</strong></p><h4 id="遗忘门与更新门"><a href="#遗忘门与更新门" class="headerlink" title="遗忘门与更新门"></a>遗忘门与更新门</h4><h5 id="门控的实现"><a href="#门控的实现" class="headerlink" title="门控的实现"></a>门控的实现</h5><p>门的输入：当前时间步的输入 $\mathbf{X}_{t}\in \mathbb{R}^{B\times V}$ 和上一时刻隐状态 $\mathbf{H}_{t-1}\in \mathbb{R}^{B\times h}$ ，两个门的经过 <em>siogmoid</em> 函数输出</p><p><img src="/posts/2699312118/image-20240407101355042.png" alt="image-20240407101355042"></p><p>重置门 $\mathbf{R}_{t}\in \mathbb{R}^{B\times h}$ ，更新门 $\mathbf{Z}_{t}\in \mathbb{R}^{B\times h}$</p><script type="math/tex;mode=display">\mathbf{R}_t = \sigma\left(\mathbf{X}_{t} \mathbf{W}_{rx}^{\mathsf{T}} + \mathbf{H}_{t-1} \mathbf{W}_{rh}^{\mathsf{T}} + \mathbf{b}_r\right),\\
\mathbf{Z}_t = \sigma\left(\mathbf{X}_{t} \mathbf{W}_{zx}^{\mathsf{T}} + \mathbf{H}_{t-1} \mathbf{W}_{zh}^{\mathsf{T}} + \mathbf{b}_z\right)</script><p>其中，$\mathbf{W}_{rx}\in \mathbb{R}^{h\times V},\mathbf{W}_{zx}\in \mathbb{R}^{h\times V}$ ，$\mathbf{W}_{rh},\mathbf{W}_{zh}\in \mathbb{R}^{h\times h}$ ，$\mathbf{b}_r,\mathbf{b}_h\in \mathbb{R}^{1\times h}$ 是可学习的参数</p><h4 id="候选隐状态"><a href="#候选隐状态" class="headerlink" title="候选隐状态"></a>候选隐状态</h4><p><img src="/posts/2699312118/image-20240407103134627.png" alt="image-20240407103134627"></p><script type="math/tex;mode=display">\tilde{\mathbf{H}}_t=\tanh\left[\mathbf{X}_{t}\mathbf{W}_{hx}^{\mathsf{T}}+\left(\mathbf{R}_t\odot \mathbf{H}_{t-1}\right)\mathbf{W}_{hh}^{\mathsf{T}}+\mathbf{b}_h\right]</script><p>重置门，控制当前时刻隐状态对历史隐状态的响应程度</p><ul><li>当 $\mathbf{R}_t=\mathbf{I}$ 时，对历史隐状态响应程度最大，保留全部的历史信息，变为RNN；</li><li>当 $\mathbf{R}_t=\mathbf{0}$ 时，对历史隐状态响应程度最小，丢弃全部的历史信息，从当前时刻重置为输入 $\mathbf{X}_{t}$ 的状态；</li></ul><h4 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h4><p><img src="/posts/2699312118/image-20240407103538566.png" alt="image-20240407103538566"></p><p>$\mathbf{Z}_t$ 决定当前时刻的隐状态 $\mathbf{H}_{t}\in \mathbb{R}^{B\times h}$ 对上一时刻隐状态 $\mathbf{H}_{t-1}$ 与新候选状态 $\tilde{\mathbf{H}}_{t}$ 的响应程度</p><ul><li>$\mathbf{Z}_t=\mathbf{I}$ ，对历史隐状态响应程度大，候选隐状态响应程度小，所以对当前时刻的隐状态的更新程度小</li><li>$\mathbf{Z}_t=\mathbf{0}$ ，对历史隐状态响应程度小，候选隐状态响应程度大，所以对当前时刻的隐状态的更新程度大</li></ul><script type="math/tex;mode=display">\mathbf{H}_{t} = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_{t}</script><h5 id="门的作用"><a href="#门的作用" class="headerlink" title="门的作用"></a>门的作用</h5><p>在一个子序列中，不是每个观测值都同等重要</p><ul><li><p>一些状态并没有观测值或者与目标输出状态无关，我们希望有一些机制从隐状态中 <strong>遗忘</strong> 这些状态</p><p>序列之间各部分存在逻辑中断，最好能有一种办法来 <strong>重置</strong> 内部状态</p></li><li><p>存在早期观测值对预测未来所有观测值有非常重要意义：如第一个观测值包含整个序列的校验和</p></li></ul><p>重置门(reset gate)、遗忘门：控制历史隐状态的保留程度，若当前输入信息比较重要，则下一个时间步尽量不响应历史隐状态</p><ul><li>$R_t$ 越小，对历史隐状态的重置(遗忘)程度越大，<strong>有助于捕获序列中的短期依赖</strong></li></ul><p>更新门(update gate)：控制隐状态的更新程度，即对过去隐状态更新多少成为新的隐状态，跳过不相关的状态</p><ul><li>$Z_t$ 越小，对隐状态的更新程度越大 ，<strong>有助于捕获序列中的长期依赖</strong></li></ul><p>相当于新引进了两种极端情况</p><ul><li>只关注当下输入（短期依赖），即 $R_t=0,Z_t=0$</li><li>只关注历史信息（长期依赖），即 $R_t=1,Z_t=1$</li></ul><p>通过对门参数的学习，使得网络对子序列时序信息的学习介于这两种极端情况之间</p><p>可学习参数是RNN的三倍</p><script type="math/tex;mode=display">\mathbf{R}_t = \sigma\left(\mathbf{X}_{t} \mathbf{W}_{rx}^{\mathsf{T}} + \mathbf{H}_{t-1} \mathbf{W}_{rh}^{\mathsf{T}} + \mathbf{b}_r\right),\\
\tilde{\mathbf{H}}=\tanh\left[\mathbf{X}_{t}\mathbf{W}_{hx}^{\mathsf{T}}+\left(\mathbf{R}_t\odot \mathbf{H}_{t-1}\right)\mathbf{W}_{hh}^{\mathsf{T}}+\mathbf{b}_h\right]\\
\mathbf{Z}_t = \sigma\left(\mathbf{X}_{t} \mathbf{W}_{zx}^{\mathsf{T}} + \mathbf{H}_{t-1} \mathbf{W}_{zh}^{\mathsf{T}} + \mathbf{b}_z\right)\\
\mathbf{H}_{t} = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_{t}</script><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p><strong>参数初始化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line">    <span class="comment"># 对可学习参数的通用初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><p><strong>起始隐状态初始化</strong></p><p>返回一个形状为 $(B,h)$ 的全零张量作为起始隐状态的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p><strong>门控循环神经网络正向传播</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><p><strong>训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240407134733719.png" alt="image-20240407134733719"></p><ul><li><p>循环神经网络</p><p><img src="/posts/2699312118/image-20240405013034969.png" alt="image-20240405013034969"></p></li></ul><blockquote><p>理论上GRU相对于RNN计算量多了进3倍，所以此处GRU用时是RNN的3倍</p></blockquote><h4 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="comment"># 定义输出层，即 Y=tanh(hW_&#123;hq&#125;)+b_q</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        <span class="comment"># inputs的形状(批量大小,时间步长,词表大小)</span></span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Y.shape输出的是 (时间步长,批量大小,词表大小) ，Y.shape[-1]为词表大小</span></span><br><span class="line">        <span class="comment">#  全连接输出层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># 完成隐变量的初始化</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line">                        batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line"><span class="comment"># 参数列表:循环层的(一个输入词元神经元数,一个词元相应的隐藏层神经元数)</span></span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line"><span class="comment"># RNNModel(rnn层,一个输出词元的神经元数)</span></span><br><span class="line">model = d2l.RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240407145619349.png" alt="image-20240407145619349"></p><ul><li><p>RNN</p><p><img src="/posts/2699312118/image-20240405014237128.png" alt="image-20240405014237128"></p></li></ul><blockquote><p>虽然理论上GRU相对于RNN计算量多了进3倍，但基于torch框架的实际训练速度相差不大</p><p>因为同一层的参数，如 $R_t$ 与 $Z_t$ ，$H_t$ 的计算形式上相似，所以基于torch框架的RNN模型会将这三组矩阵并行计算，所以性能上相差不大</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> gru_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数名:<span class="subst">&#123;name&#125;</span>-参数形状:<span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数名:weight_ih_l0-参数形状:torch.Size([768, 28])</span></span><br><span class="line"><span class="comment"># 参数名:weight_hh_l0-参数形状:torch.Size([768, 256])</span></span><br><span class="line"><span class="comment"># 参数名:bias_ih_l0-参数形状:torch.Size([768])</span></span><br><span class="line"><span class="comment"># 参数名:bias_hh_l0-参数形状:torch.Size([768])</span></span><br></pre></td></tr></table></figure><p>$B=32,T=35,h=256$ ，两个门控参数和一个连接参数，即 $\begin{aligned}\mathbf{W}_{hx}\\\mathbf{W}_{zx}\\\mathbf{W}_{rx}\end{aligned}\in \mathbb{R}^{(3\times 256)\times 28}$</p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1bM41127U4/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=260d5bbbf395fd4a9b3e978c7abde437">https://www.bilibili.com/video/BV1bM41127U4/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=260d5bbbf395fd4a9b3e978c7abde437</a></p><p><img src="/posts/2699312118/image-20240407145450768.png" alt="image-20240407145450768"></p></blockquote><h3 id="9-1-2-长短期记忆网络LSTM"><a href="#9-1-2-长短期记忆网络LSTM" class="headerlink" title="9.1.2 长短期记忆网络LSTM"></a>9.1.2 长短期记忆网络LSTM</h3><p>隐变量模型存在 <strong>信息长期保存</strong> 和 <strong>输入短期缺失</strong> 的问题。最早的解决方法是长短期存储器（long short-term memory）。</p><p>为每个输入、输出、隐状态增加记忆单元，为控制记忆单元增加了许多门</p><ul><li>输入门：决定何时将数据读入记忆单元</li><li>输出门：从记忆单元读出</li><li>遗忘门：重置记忆单元内容</li></ul><h4 id="门机制"><a href="#门机制" class="headerlink" title="门机制"></a>门机制</h4><p>输入信息：当前时间步的输入 $\mathbf{X}_{t}\in \mathbb{R}^{B\times V}$ 和上一时刻隐状态 $\mathbf{H}_{t-1}\in \mathbb{R}^{B\times h}$</p><p><img src="/posts/2699312118/image-20240407160511029.png" alt="image-20240407160511029"></p><p>输入门 $\mathbf{I}_t\in \mathbb{R}^{B\times h}$ ，遗忘门 $\mathbf{F}_t\in \mathbb{R}^{B\times h}$ ，输出门 $\mathbf{O}_t\in \mathbb{R}^{B\times h}$</p><script type="math/tex;mode=display">\mathbf{I}_t=\sigma\left(\mathbf{X}_{t}\mathbf{W}_{ix}^{\mathsf{T}}+\mathbf{H}_{t-1}\mathbf{W}_{ih}^{\mathsf{T}}+\mathbf{b}_i\right)\\
\mathbf{F}_t=\sigma\left(\mathbf{X}_{t}\mathbf{W}_{fx}^{\mathsf{T}}+\mathbf{H}_{t-1}\mathbf{W}_{fh}^{\mathsf{T}}+\mathbf{b}_f\right)\\
\mathbf{O}_t=\sigma\left(\mathbf{X}_{t}\mathbf{W}_{ox}^{\mathsf{T}}+\mathbf{H}_{t-1}\mathbf{W}_{oh}^{\mathsf{T}}+\mathbf{b}_i\right)</script><p>其中，$\mathbf{W}_{ix},\mathbf{W}_{fx},\mathbf{W}_{ox}\in \mathbb{R}^{h\times V}$ ，$\mathbf{W}_{ih},\mathbf{W}_{fh},\mathbf{W}_{oh}\in \mathbb{R}^{h\times h}$ ，$\mathbf{b}_i,\mathbf{b}_f，\mathbf{b}_o\in \mathbb{R}^{1\times h}$ 是可学习参数</p><h4 id="候选记忆元"><a href="#候选记忆元" class="headerlink" title="候选记忆元"></a>候选记忆元</h4><p><img src="/posts/2699312118/image-20240407161153837.png" alt="image-20240407161153837"></p><p>候选记忆元 $\tilde{\mathbf{C}}_{t}\in \mathbb{R}^{B\times h}$</p><script type="math/tex;mode=display">\tilde{\mathbf{C}}_{t}=\tanh\left(\mathbf{X}_{t}\mathbf{W}_{cx}^{\mathsf{T}}+\mathbf{H}_{t-1}\mathbf{W}_{ch}^{\mathsf{T}}+\mathbf{b}_c\right)</script><p>其中，$\mathbf{W}_{cx}\in\R^{h\times V}$ ，$\mathbf{W}_{ch}\in \mathbb{R}^{h\times h}$ ，$\mathbf{b}_c\in \mathbb{R}^{1\times h}$ 都是可学习的参数</p><h4 id="记忆元"><a href="#记忆元" class="headerlink" title="记忆元"></a>记忆元</h4><p><img src="/posts/2699312118/image-20240407162021676.png" alt="image-20240407162021676"></p><p>类似于GRU的重置门(遗忘门)和更新门，LSTM中，遗忘门 $\mathbf{F}_{t}$ 控制遗忘(保留)多少历史信息，输入门 $\mathbf{I}_t$ 控制更新多少状态信息</p><script type="math/tex;mode=display">\mathbf{C}_{t}=\mathbf{F}_t\odot \mathbf{C}_{t-1}+\mathbf{I}_t\odot \tilde{\mathbf{C}}_{t}</script><ul><li><p>区别于GRU的是</p><p>由于 $\mathbf{F}_{t}$ 与 $\mathbf{I}_{t}$ 是独立的，当 $\mathbf{F}_{t},\mathbf{I}_{t}\rightarrow 1$ 时，长期记忆既可以保留历史记忆，又能更新历史记忆；</p><p>而GRU的 $\mathbf{Z}_{t},1-\mathbf{Z}_{t}$ 分别控制保留历史状态和更新历史状态，只能二选一</p></li></ul><h4 id="隐状态-1"><a href="#隐状态-1" class="headerlink" title="隐状态"></a>隐状态</h4><p><img src="/posts/2699312118/image-20240407162638655.png" alt="image-20240407162638655"></p><p>隐状态 $\mathbf{H}_{t}\in\R^{B\times h}$ ，</p><script type="math/tex;mode=display">\mathbf{H}_{t}=\mathbf{O}_t\odot \tanh\left(\mathbf{C}_{t}\right)</script><ul><li><p>输出门 $\mathbf{O}_t$ 的作用相当于GRU 的重置门，当 $\mathbf{O}_t\rightarrow 0$ ，则忽略历史信息</p></li><li><p>关于 $\tanh\left(\mathbf{C}_{t}\right)$ 的理解，确保 $\mathbf{H}_{t}$ 压缩在 $[-1,1]$</p><p>$\mathbf{F}_t$ 和 $\mathbf{I}_t$ 都在 $[0,1]$ 区间内，所以 $\mathbf{C}_{t}\in [-2,2]$ ，为了将 $\mathbf{H}_{t}$ 恢复到 $[-1,1]$ 内，使用 $\tanh$</p></li></ul><p>$C$ 长期记忆：因为取值范围更大，所以可以存储更多的时序特征</p><p>$H$ 短期记忆：取值范围在 $[-1,1]$ ，存储的时序信息少一些</p><p>实际上，LSTM与GRU都是在 <em>极远视</em> 和 <em>极近视</em> 两种极端情况的平衡，实际的效果差不多</p><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><p><strong>加载数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p><strong>参数初始化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,</span><br><span class="line">              b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><p><strong>起始隐状态初始化</strong></p><p>长短期记忆网络的隐状态需要返回一个额外的记忆元，长期记忆 $\mathbf{C}_{t}$ ，短期记忆 $\mathbf{H}_{t}$ ，形状为 $(B,h)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure><p><strong>LSTM前向传播</strong></p><p>只有隐状态 $\mathbf{H}_{t}$ 才会传递到输出层，$\mathbf{C}_{t}$ 不直接参与输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure><p><strong>训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240407165124433.png" alt="image-20240407165124433"></p><h4 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240407165135826.png" alt="image-20240407165135826"></p><h3 id="9-1-3-深度循环神经网络"><a href="#9-1-3-深度循环神经网络" class="headerlink" title="9.1.3 深度循环神经网络"></a>9.1.3 深度循环神经网络</h3><p>隐变量与观测值之间的关系不仅是单向非线性的，实际上隐变量和观测值与具体的函数形式的交互方式是多种多样的</p><p>深度RNN用多个隐藏层获得更多的非线性信息</p><p><img src="/posts/2699312118/image-20240407232518186.png" alt="image-20240407232518186"></p><p>若深度RNN有 $L$ 个隐藏层构成</p><p>在 $t$ 步有一个小批量的输入数据 $\mathbf{X}_t\in \mathbb{R}^{B\times V}$ ，第 $l$ 个隐藏层的隐状态 $\mathbf{H}^{(l)}_t\in \mathbb{R}^{B\times h}$ ，输出层变量 $\mathbf{O}_t\in \mathbb{R}^{B\times q}$ 。可认为 $\mathbf{H}_t^{(0)}=\mathbf{X}_t$ ，第 $l$ 个隐藏层的隐状态使用的激活函数为 $\phi_l(\cdot)$</p><script type="math/tex;mode=display">\mathbf{H}_t^{(l)}=\phi_l\left(\mathbf{H}_t^{(l-1)}\cdot \mathbf{W}_{hx}^{(l)}+\mathbf{H}^{(l)}_{t}\cdot\mathbf{W}_{hh}^{(l)}+\mathbf{b}^{(l)}_h\right)</script><p>其中，</p><ul><li>当 $l\ge 2$ 时，$\mathbf{W}_{hx}^{(l)}\in \mathbb{R}^{h\times h}$ ，$\mathbf{W}_{hh}^{(l)}\in \mathbb{R}^{h\times h}$ ，$\mathbf{b}_h^{(l)}\in \mathbb{R}^{1\times h}$ 是第 $l$ 层可学习的参数</li><li>当 $l=1$​ 时，$\mathbf{W}_{hx}^{(l)}\in \mathbb{R}^{h\times V}$ ，$\mathbf{W}_{hh}^{(l)}\in \mathbb{R}^{h\times h}$，$\mathbf{b}_h^{(l)}\in \mathbb{R}^{1\times h}$ 是第 $1$ 层可学习的参数</li></ul><p>输出层仅基于第 $L$ 层的隐状态</p><script type="math/tex;mode=display">\mathbf{O}_t=\mathbf{H}_t^{(L)}\cdot\mathbf{W}_{qh}+\mathbf{b}_q</script><p>其中，$\mathbf{W}_{qh}^{(L)}\in \mathbb{R}^{q\times h}$ ，$\mathbf{b}_q\in \mathbb{R}^{1\times q}$ 都是输出层可学习的参数</p><p><strong>RNN不会用很深，一般2，3层</strong></p><p>同时，GRU和LSTM也可以堆叠多个隐藏层</p><h4 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h4><p><strong>加载数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p><strong>初始化参数</strong></p><p><code>num_layers</code> ：$L=2$ ，有2个隐藏层</p><p><code>num_inputs</code> ：输入层神经元数为词表大小 $V$</p><p><code>lstm</code> 循环层：输入神经元数为 $V$ ，隐藏层神经元数都为 $h$</p><p><code>d2l.RNNModel(lstm_layer, len(vocab))</code> 输出层：输出层输入神经元数量为 $h$ ，输出神经元数量为 $V$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">input = torch.randn(5, 3, 10)</span></span><br><span class="line"><span class="string">rnn = nn.LSTM(10, 20, 2)</span></span><br><span class="line"><span class="string"># 隐状态形状 num_directions * num_layers,batch_size, self.num_hiddens</span></span><br><span class="line"><span class="string">#   在train_epoch_ch8中，初始化工作在每个批量子序列开始迭代前完成，即begin_state()的调用</span></span><br><span class="line"><span class="string">h0 = torch.randn(2, 3, 20)</span></span><br><span class="line"><span class="string">c0 = torch.randn(2, 3, 20) </span></span><br><span class="line"><span class="string"># lstm有两个隐状态，长期记忆c和短期记忆h</span></span><br><span class="line"><span class="string">output, (hn, cn) = rnn(input, (h0, c0))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><p><strong>训练和预测</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr*<span class="number">1.0</span>, num_epochs, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240408015730339.png" alt="image-20240408015730339"></p><p>由于模型容量大，所以迭代轮数很少就收敛</p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数名:<span class="subst">&#123;name&#125;</span>,参数形状:<span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#	参数名:weight_ih_l0,参数形状:torch.Size([1024, 28])</span></span><br><span class="line"><span class="comment">#	参数名:weight_hh_l0,参数形状:torch.Size([1024, 256])</span></span><br><span class="line"><span class="comment">#	参数名:bias_ih_l0,参数形状:torch.Size([1024])</span></span><br><span class="line"><span class="comment">#	参数名:bias_hh_l0,参数形状:torch.Size([1024])</span></span><br><span class="line"><span class="comment">#	参数名:weight_ih_l1,参数形状:torch.Size([1024, 256])</span></span><br><span class="line"><span class="comment">#	参数名:weight_hh_l1,参数形状:torch.Size([1024, 256])</span></span><br><span class="line"><span class="comment">#	参数名:bias_ih_l1,参数形状:torch.Size([1024])</span></span><br><span class="line"><span class="comment">#	参数名:bias_hh_l1,参数形状:torch.Size([1024])</span></span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/86390fcfb156fee777a636ec9f70f01a.png" alt="img"></p><p>第 $0$ 隐藏层参数：</p><ul><li>$B=32,h=256$ ，三个门控参数和一个候选状态的连接参数，即 $\begin{aligned}\mathbf{W}^{(0)}_{ix}\\\mathbf{W}^{(0)}_{fx}\\\mathbf{W}^{(0)}_{ox}\\\mathbf{W}^{(0)}_{hx}\end{aligned}\in \mathbb{R}^{(4\times h)\times V}=\left((4\times 256), 28\right)$</li></ul><p>第 $1$ 隐藏层参数：</p><ul><li>$B=32,h=256$ ，三个门控参数和一个候选状态的连接参数，即 $\begin{aligned}\mathbf{W}^{(1)}_{ix}\\\mathbf{W}^{(1)}_{fx}\\\mathbf{W}^{(1)}_{ox}\\\mathbf{W}^{(1)}_{hx}\end{aligned}\in \mathbb{R}^{(4\times h)\times h}=\left((4\times 256), 256\right)$</li></ul><h3 id="9-1-4-双向深度循环神经网络"><a href="#9-1-4-双向深度循环神经网络" class="headerlink" title="9.1.4 双向深度循环神经网络"></a>9.1.4 双向深度循环神经网络</h3><p>对于时序预测模型，单向RNN面向的任务是给定观测的情况下，对下一个输出进行建模</p><p>而NLP中还存在一种填空任务，给定上下文，对中间缺失状态的进行填充</p><blockquote><p>如：</p><ul><li>我<code>___</code>。</li><li>我<code>___</code>饿了。</li><li>我<code>___</code>饿了，我可以吃半头猪。</li></ul><p>在下文未知的情况下，可能会填入与 “饿” 没有关系的内容，但有下文的情况下，内容会限制在 “饿” 相关的内容</p></blockquote><p>单向RNN只看过去信息，即上文。在下文已知的情况下，当前的时刻的信息量会增加许多，很大程度上会提高当前时刻填充内容的质量</p><p><strong>不同长度的上下文范围重要性都是相同的</strong> —— 通过隐马尔科夫模型的DP说明</p><h4 id="隐马尔科夫模型中的动态规划"><a href="#隐马尔科夫模型中的动态规划" class="headerlink" title="隐马尔科夫模型中的动态规划"></a>隐马尔科夫模型中的动态规划</h4><blockquote><p>为什么使用 <strong>双向</strong> 深度循环网络，为什么选择深度网络架构</p></blockquote><p><img src="/posts/2699312118/image-20240408145702650.png" alt="image-20240408145702650"></p><p>在任意时间步 $t$ ，会有一个隐变量 $\mathbf{h}_t$ ，每个隐状态都有一定概率 $p(\mathbf{x}_t\vert \mathbf{h}_t)$ 得到观测 $\mathbf{x}_t$。另外，隐状态之间存在状态转移概率 $p(\mathbf{h}_{t+1}\vert \mathbf{h}_t)$ ，因此获取一个序列 $\{\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_T\}$ 的概率服从一个联合概率</p><script type="math/tex;mode=display">p(\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_T,\mathbf{h}_1,\mathbf{h}_2,\cdots,\mathbf{h}_T)=\prod\limits_{t=1}^Tp(\mathbf{h}_{t}\vert \mathbf{h}_{t-1})p(\mathbf{x}_t\vert \mathbf{h}_t)</script><p>假设序列缺失 $\mathbf{x}_j$ ，变为一个填空任务，则目标函数变为 $\max p\left(\mathbf{x}_j\vert \{\mathbf{x}_{-j}\}\right)=\max p(\mathbf{x}_{j}\vert \mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{j-1},\mathbf{x}_{j+1},\cdots,\mathbf{x}_T)$</p><p>隐状态 $\mathbf{h}_t$ 是一个与时间有关的变量，若所有隐状态的状态值是有限的，即每个 $\mathbf{h}_t$ 都可以有 $k$ 个不同的值，则相邻两个时刻间的状态转移 $p(\mathbf{h}_{t+1}\vert \mathbf{h}_t)\in \mathbb{R}^{k\times k}$ 是一个状态转移矩阵 ，要得到 $p(\mathbf{x}_1,\cdots,\mathbf{x}_T)$ 是 $k^T$ 个路径的求和</p><script type="math/tex;mode=display">P(\mathbf{x}_1, \ldots, \mathbf{x}_T)=\sum_{\mathbf{h}_1, \ldots, \mathbf{h}_T} P(\mathbf{x}_1, \ldots, \mathbf{x}_T, \mathbf{h}_1, \ldots, \mathbf{h}_T)</script><p>基于动态规划，可以得到两种递归形式</p><ul><li><p><em>前向递归</em></p><script type="math/tex;mode=display">\pi_{t}(\mathbf{h}_{t}) = \sum_{\mathbf{h}_{t-1}} \pi_{t-1}(\mathbf{h}_{t-1}) P(\mathbf{x}_{t-1} \vert \mathbf{h}_{t-1}) P(\mathbf{h}_{t} \vert \mathbf{h}_{t-1})\\
\pi_1(\mathbf{h}_1)=p(\mathbf{h}_1)</script></li><li><p><em>后向递归</em></p><script type="math/tex;mode=display">\rho_{t}(\mathbf{h}_{t})= \sum_{\mathbf{h}_{t+1}} P(\mathbf{h}_{t+1} \vert \mathbf{h}_{t})  P(\mathbf{x}_{t+1} \vert \mathbf{h}_{t+1})\rho_{t+1}(\mathbf{h}_{t+1})\\
\rho_T(\mathbf{h}_T) = 1</script></li></ul><p>两种递归方式，都允许我们在 $\mathcal{O}(kT)$ 的时间内对 $T$ 个隐变量序列 $(h_1,\cdots,h_T)$ 的所有值求和，避免指数级循环嵌套即指数级运算</p><p><strong>为预测当前状态，前向需要结合历史状态，后向需要结合未来状态</strong> ，将二者结合</p><p>可以计算</p><script type="math/tex;mode=display">P(x_j \vert x_{-j}) \propto \sum_{h_j} \pi_j(h_j) \rho_j(h_j) P(x_j \vert h_j)</script><blockquote><p>前向递归推导</p><script type="math/tex;mode=display">\begin{aligned}
    P(x_1, \ldots, x_T)=& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\
    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^T P(h_t \vert h_{t-1}) P(x_t \vert h_t) \\
    =& \sum_{h_2, \ldots, h_T} \underbrace{\left[\sum_{h_1} P(h_1) P(x_1 \vert h_1) P(h_2 \vert h_1)\right]}_{\pi_2(h_2) \triangleq}
    P(x_2 \vert h_2) \prod_{t=3}^T P(h_t \vert h_{t-1}) P(x_t \vert h_t) \\
    =& \sum_{h_3, \ldots, h_T} \underbrace{\left[\sum_{h_2} \pi_2(h_2) P(x_2 \vert h_2) P(h_3 \vert h_2)\right]}_{\pi_3(h_3)\triangleq}
    P(x_3 \vert h_3) \prod_{t=4}^T P(h_t \vert h_{t-1}) P(x_t \vert h_t)\\
    =& \dots \\
    =& \sum_{h_T} \pi_T(h_T) P(x_T \vert h_T).
\end{aligned}</script><p>后向递归推导</p><script type="math/tex;mode=display">\begin{aligned}
P(x_1, \ldots, x_T)=& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\
    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^{T-1} P(h_t \vert h_{t-1}) P(x_t \vert h_t) \cdot P(h_T \vert h_{T-1}) P(x_T \vert h_T) \\
    =& \sum_{h_1, \ldots, h_{T-1}} \prod_{t=1}^{T-1} P(h_t \vert h_{t-1}) P(x_t \vert h_t) \cdot
    \underbrace{\left[\sum_{h_T} P(h_T \vert h_{T-1}) P(x_T \vert h_T)\right]}_{\rho_{T-1}(h_{T-1})\triangleq} \\
    =& \sum_{h_1, \ldots, h_{T-2}} \prod_{t=1}^{T-2} P(h_t \vert h_{t-1}) P(x_t \vert h_t) \cdot
    \underbrace{\left[\sum_{h_{T-1}} P(h_{T-1} \vert h_{T-2}) P(x_{T-1} \vert h_{T-1}) \rho_{T-1}(h_{T-1}) \right]}_{\rho_{T-2}(h_{T-2})\triangleq} \\
    =& \ldots \\
    =& \sum_{h_1} P(h_1) P(x_1 \vert h_1)\rho_{1}(h_{1}).
\end{aligned}</script></blockquote><h4 id="双向循环网络"><a href="#双向循环网络" class="headerlink" title="双向循环网络"></a>双向循环网络</h4><p>知道未来观测对隐马尔科夫模型中是有益的。</p><p>为了更好的完成 <strong>填空</strong> 任务，我们希望在RNN中加入这种前瞻能力（结合下文推导上文的能力），使得我们可以 <strong>使用来自序列两端的信息来估计中间的输出</strong></p><p>双向循环网络可以实现需求，实际上是对隐马尔可夫统计模型的一种实现，使用统计模型的函数依赖类型，将其参数化为通用形式</p><h5 id="1-双向递归"><a href="#1-双向递归" class="headerlink" title="1.双向递归"></a>1.双向递归</h5><p><img src="/posts/2699312118/image-20240408163046075.png" alt="image-20240408163046075"></p><p>在任意时间步 $t$ ，有小批量数据 $\mathbf{X}_{t}\in \mathbb{R}^{B\times V}$ 。该时间步的前向隐状态 $\overrightarrow{\mathbf{H}}_t\in \mathbb{R}^{B\times h}$ 和反向隐状态 $\overleftarrow{\mathbf{H}}_t\in \mathbb{R}^{B\times h}$</p><script type="math/tex;mode=display">\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi\left(\mathbf{X}_t \cdot\mathbf{W}_{hx}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \cdot\mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}\right),\\
\overleftarrow{\mathbf{H}}_t &= \phi\left(\mathbf{X}_t \cdot\mathbf{W}_{hx}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \cdot\mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}\right),
\end{aligned}</script><p>其中，权重$\mathbf{W}_{hx}^{(f)} \in \mathbb{R}^{h \times V}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{hx}^{(b)} \in \mathbb{R}^{h \times V}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}$ 和偏置 $\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$ 都是模型的可学习参数</p><ul><li><p><strong>正向递归</strong>，结合 $\mathbf{X}_t$ 与 $\mathbf{H}_{t-1}$ 计算 $\mathbf{H}_t$</p><p><img src="/posts/2699312118/image-20240408173857167.png" alt="image-20240408173857167"></p></li><li><p><strong>反向递归</strong> ，将输入序列 $\{\mathbf{X}_1,\mathbf{X}_2,\cdots,\mathbf{X}_T\}$ 翻转，变为 $\{\mathbf{X}_T,\mathbf{X}_{T-1},\cdots,\mathbf{X}_1\}$ ，将 $\{\overleftarrow{\mathbf{H}}_1,\overleftarrow{\mathbf{H}}_2,\cdots,\overleftarrow{\mathbf{H}}_T\}$ 序列翻转为 $\{\overleftarrow{\mathbf{H}}_T,\overleftarrow{\mathbf{H}}_{T-1},\cdots,\overleftarrow{\mathbf{H}}_1\}$</p><p>利用前向递归函数计算 $\overleftarrow{\mathbf{H}}_{t-1}=\phi\left(\mathbf{X}_{t-1}\cdot \mathbf{W}_{hx}^{(b)}+\overleftarrow{\mathbf{H}}_t\cdot \mathbf{W}_{hh}^{(b)}+\mathbf{b}_h^{(b)}\right)$ ，得到 $\{\overleftarrow{\mathbf{H}}_T,\overleftarrow{\mathbf{H}}_{T-1},\cdots,\overleftarrow{\mathbf{H}}_1\}$ ，在将其翻转得到 $\{\overleftarrow{\mathbf{H}}_1,\overleftarrow{\mathbf{H}}_2,\cdots,\overleftarrow{\mathbf{H}}_T\}$</p><p><img src="/posts/2699312118/image-20240408173400912.png" alt="image-20240408173400912"></p></li></ul><h5 id="2-输出"><a href="#2-输出" class="headerlink" title="2. 输出"></a>2. 输出</h5><p>再将前向隐状态 $\overrightarrow{\mathbf{H}}_t$ 和反向隐状态 $\overleftarrow{\mathbf{H}}_t$ 连接起来，获取需要送入输出层的隐状态 $\mathbf{H}_t\in \mathbb{R}^{B\times 2h}$ 得到输出 $\mathbf{O}_t\in \mathbb{R}^{B\times q}$</p><ul><li>若是深度双向循环网络，将 $\mathbf{H}_t$ 作为下一层的输入</li></ul><script type="math/tex;mode=display">\mathbf{O}_t = \mathbf{H}_t \cdot\mathbf{W}_{qh} + \mathbf{b}_q</script><p>这里，权重矩阵$\mathbf{W}_{qh} \in \mathbb{R}^{q \times 2h}$ 和偏置 $\mathbf{b}_q \in \mathbb{R}^{1 \times q}$ 是输出层的模型参数</p><ul><li>$\mathbf{W}_{qh}$ 也可以分别拆分为两个不同方向上的输出</li></ul><h4 id="双向循环神经网络的代价"><a href="#双向循环神经网络的代价" class="headerlink" title="双向循环神经网络的代价"></a>双向循环神经网络的代价</h4><ul><li><p>双向RNN不适合做预测（推理）</p><p>在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度</p><p>在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差</p></li><li><p>双向循环神经网络的计算速度非常慢</p><p>主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链</p></li></ul><p>双向层的使用在实践中非常少，并且仅仅应用于部分场合</p><p>双向RNN主要用于对序列的 <strong>时序特征进行提取（编码器）</strong> 、填空。例如，填充缺失的单词、词元注释、以及作为序列处理流水线中的一个步骤对序列进行编码</p><h4 id="双向深度LSTM实现"><a href="#双向深度LSTM实现" class="headerlink" title="双向深度LSTM实现"></a>双向深度LSTM实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, d2l.try_gpu()</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line"><span class="comment"># 通过设置“bidirective=True”来定义双向LSTM模型</span></span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240408174035997.png" alt="image-20240408174035997"></p><p>可见，模型容量加大，在训练数据上很快就过拟合</p><p>但用作预测并没有任何意义</p><h5 id="参数与输出形状"><a href="#参数与输出形状" class="headerlink" title="参数与输出形状"></a>参数与输出形状</h5><p><img src="/posts/2699312118/image-20240413184724630.png" alt="image-20240413184724630"></p><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, d2l.try_gpu()</span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="number">28</span>, <span class="number">256</span>, <span class="number">3</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X = torch.zeros((num_steps,batch_size,num_inputs))</span><br><span class="line">h0 = torch.zeros((num_layers * <span class="number">2</span>, batch_size, num_hiddens))</span><br><span class="line">c0 = torch.zeros((num_layers * <span class="number">2</span>, batch_size, num_hiddens))</span><br><span class="line">out, state = lstm_layer(X,(h0,c0))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out.shape,  out.reshape((num_steps, batch_size,<span class="number">2</span>, num_hiddens)).shape</span><br><span class="line"><span class="comment"># (torch.Size([35, 32, 512]), torch.Size([35, 32, 2, 256]))</span></span><br></pre></td></tr></table></figure><p>双向RNN的输出分为前向输出和后向输出，即 $\overrightarrow{\mathbf{O}}\in \mathbb{R}^{T\times B\times h}=(35\times 32\times 256)$ ，$\overleftarrow{\mathbf{O}}\in \mathbb{R}^{T\times B\times h}=(35, 32, 256)$ ，所以 $\mathbf{O}=\begin{bmatrix}\overrightarrow{\mathbf{O}}\\\overleftarrow{\mathbf{O}}\end{bmatrix}\in \mathbb{R}^{T\times B\times 2h}=(32,32,512)$</p><p><strong>隐状态</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">state[<span class="number">0</span>].shape,state[<span class="number">1</span>].shape</span><br><span class="line"><span class="comment">#	(torch.Size([6, 32, 256]), torch.Size([6, 32, 256]))</span></span><br></pre></td></tr></table></figure><p>$\overrightarrow{\mathbf{H}}_T\in \mathbb{R}^{L\times B\times h }=(3,32,256),\overleftarrow{\mathbf{H}}_T\in \mathbb{R}^{L\times B\times h }=(3,32,256)$</p><p>$\mathbf{H}_T=\begin{bmatrix}\overrightarrow{\mathbf{H}}_T\\\overleftarrow{\mathbf{H}}_T\end{bmatrix}\in \mathbb{R}^{2L\times B\times h }$</p><p>同理，$\mathbf{C}_T=\begin{bmatrix}\overrightarrow{\mathbf{C}}_T\\\overleftarrow{\mathbf{C}}_T\end{bmatrix}\in \mathbb{R}^{2L\times B\times h }$</p><p><strong>参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> lstm_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数名:<span class="subst">&#123;name&#125;</span>,参数形状:<span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240413194025317.png" alt="image-20240413194025317"></p><p>$B=32,h=256$</p><p>第 $0$ 隐藏层参数：</p><ul><li>正向，三个门控参数和一个候选状态的连接参数，即 $\begin{aligned}\overrightarrow{\mathbf{W}}^{(0)}_{ix}\\\overrightarrow{\mathbf{W}}^{(0)}_{fx}\\\overrightarrow{\mathbf{W}}^{(0)}_{ox}\\\overrightarrow{\mathbf{W}}^{(0)}_{hx}\end{aligned}\in \mathbb{R}^{(4\times h)\times V }=\left((4\times 256), 28\right)$</li><li>反向，三个门控参数和一个候选状态的连接参数，即 $\begin{aligned}\overleftarrow{\mathbf{W}}^{(0)}_{ix}\\\overleftarrow{\mathbf{W}}^{(0)}_{fx}\\\overleftarrow{\mathbf{W}}^{(0)}_{ox}\\\overleftarrow{\mathbf{W}}^{(0)}_{hx}\end{aligned}\in \mathbb{R}^{(4\times h)\times V }=\left((4\times 256), 28\right)$</li></ul><p>第 $1$ 隐藏层参数：</p><ul><li>正向，三个门控参数和一个候选状态的连接参数，即 $\begin{aligned}\overrightarrow{\mathbf{W}}^{(0)}_{ix}\\\overrightarrow{\mathbf{W}}^{(0)}_{fx}\\\overrightarrow{\mathbf{W}}^{(0)}_{ox}\\\overrightarrow{\mathbf{W}}^{(0)}_{hx}\end{aligned}\in \mathbb{R}^{(4\times h)\times h }=\left((4\times 256), 256\right)$</li><li>反向，三个门控参数和一个候选状态的连接参数，即 $\begin{aligned}\overleftarrow{\mathbf{W}}^{(0)}_{ix}\\\overleftarrow{\mathbf{W}}^{(0)}_{fx}\\\overleftarrow{\mathbf{W}}^{(0)}_{ox}\\\overleftarrow{\mathbf{W}}^{(0)}_{hx}\end{aligned}\in \mathbb{R}^{(4\times h)\times h }=\left((4\times 256), 256\right)$</li></ul><h2 id="9-2-机器翻译"><a href="#9-2-机器翻译" class="headerlink" title="9.2 机器翻译"></a>9.2 机器翻译</h2><p>机器翻译是一种 <strong>序列转换模型</strong>，也是语言模型的基准测试。机器翻译解决的问题是将 <strong>输入序列转换为输出序列</strong>。可以追溯到计算机破解语言编码</p><p>机器翻译是指将句子序列从一种语言翻译成另一种语言</p><ul><li>统计机器翻译：涉及翻译模型和语言模型等组成部分的统计分析</li><li>神经网络翻译</li></ul><h3 id="9-2-1-数据集"><a href="#9-2-1-数据集" class="headerlink" title="9.2.1 数据集"></a>9.2.1 数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h4 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h4><p>下载双语句子，“英-法”数据集。</p><p>按字符类型读入内存</p><p>数据集的每一行由制表符分割两种语言，前半部分为英语句子后半部分为法语句子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_nmt</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br><span class="line"><span class="comment">#	Go.		Va !</span></span><br><span class="line"><span class="comment">#	Hi.		Salut !</span></span><br><span class="line"><span class="comment">#	Run!	Cours !</span></span><br><span class="line"><span class="comment">#	Run!	Courez !</span></span><br><span class="line"><span class="comment">#	Who?	Qui ?</span></span><br><span class="line"><span class="comment">#	Wow!	Ça alors !</span></span><br></pre></td></tr></table></figure><h4 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_nmt</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 判断两个字符标点符号前是否有空格，切词时将标点符号单独作为一个token</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">no_space</span>(<span class="params">char, prev_char</span>):</span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将utf-8的半角和全角空格转为空格</span></span><br><span class="line">    <span class="comment"># 使用小写字母替换大写字母</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># text是逐字符索引的，1个字符对应text中的一个元素</span></span><br><span class="line"><span class="comment"># 如 text[79:80]=!</span></span><br><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br><span class="line"><span class="comment"># go .		va !</span></span><br><span class="line"><span class="comment"># hi .		salut !</span></span><br><span class="line"><span class="comment"># run !		cours !</span></span><br><span class="line"><span class="comment"># run !		courez !</span></span><br><span class="line"><span class="comment"># who ?		qui ?</span></span><br><span class="line"><span class="comment"># wow !		ça alors !</span></span><br></pre></td></tr></table></figure><h4 id="词元化与词表"><a href="#词元化与词表" class="headerlink" title="词元化与词表"></a>词元化与词表</h4><h5 id="词元化"><a href="#词元化" class="headerlink" title="词元化"></a>词元化</h5><p>对文本进行 <strong>单词级</strong>的词元化，</p><p>实现对前 <code>num_examples</code> 个文本的进行词元化，一个词元要么是一个英文单词 / 法文单词，要么是一个标点符号</p><p>返回值：词元化的英文句子序列集 <code>source</code> 和法文句子序列集 <code>target</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 将每一行通过 &#x27;\t&#x27; 切分为两部分</span></span><br><span class="line">        <span class="comment"># 前半部分为源序列，后半部分为目标序列</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 将每个序列通过 &#x27; &#x27; 切分为单个token，</span></span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br><span class="line">source[:<span class="number">6</span>], target[:<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#([[&#x27;go&#x27;, &#x27;.&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;hi&#x27;, &#x27;.&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;run&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;run&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;who&#x27;, &#x27;?&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;wow&#x27;, &#x27;!&#x27;]],</span></span><br><span class="line"><span class="comment">#  [[&#x27;va&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;salut&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;cours&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;courez&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;qui&#x27;, &#x27;?&#x27;],</span></span><br><span class="line"><span class="comment">#  [&#x27;ça&#x27;, &#x27;alors&#x27;, &#x27;!&#x27;]])</span></span><br></pre></td></tr></table></figure><p><strong>查看句子序列集中的各种长度(token数)的句子数</strong></p><p><img src="/posts/2699312118/image-20240408221457210.png" alt="image-20240408221457210"></p><p>绘制每个文本序列所包含的词元数量的直方图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    <span class="comment"># 以每个句子的长度(词元数量)作为因变量</span></span><br><span class="line">    _, _, patches = d2l.plt.hist(</span><br><span class="line">        [[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist], [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    d2l.plt.xlabel(xlabel)</span><br><span class="line">    d2l.plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>,</span><br><span class="line">                        <span class="string">&#x27;count&#x27;</span>, source, target);</span><br></pre></td></tr></table></figure><h5 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h5><p>对句子序列进行单词集的词元化时，词表大小明显大于字符词元。</p><p>将出现频率少于2的低频单词进行忽略，视为 <code>&lt;unk&gt;</code> ，指定保留字 <code>&lt;pad&gt;，&lt;bos&gt;，&lt;eos&gt;</code> 用于填充句子和标识句子开始和解决</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># d2l.Vocab：获取源句子语料集的词表</span></span><br><span class="line"><span class="comment">#    在保留字符映射完成后，按照词频降序与数字索引一一对应</span></span><br><span class="line"><span class="comment">#    返回词表，通过词表可完成数字索引与token的互相转换</span></span><br><span class="line">src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                      reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line"><span class="comment"># pad：填充；bos：begin of sequence；eos:end of sequence</span></span><br><span class="line"><span class="comment"># 若某个token出现次数小于等于2，则丢弃该token</span></span><br><span class="line"><span class="built_in">len</span>(src_vocab)</span><br><span class="line"><span class="comment"># 10012</span></span><br></pre></td></tr></table></figure><h4 id="数据集迭代器"><a href="#数据集迭代器" class="headerlink" title="数据集迭代器"></a>数据集迭代器</h4><h5 id="根据时序步长调整句子"><a href="#根据时序步长调整句子" class="headerlink" title="根据时序步长调整句子"></a>根据时序步长调整句子</h5><p>在机器翻译中，每个样本都是由源和目标组成的文本序列对，为提高计算效率，在封装为批量前，需要对每行句子进行调整，使每个小批量的形状相同。首先，需要使所有的句子子序列都等于时序步长</p><p>时序步长（词元数量）由 <code>num_steps</code> 指定，</p><ul><li>截断 <code>truncation</code></li><li>填充 <code>padding</code></li></ul><p>如果文本序列的词元数目少于<code>num_steps</code>时， 我们将继续在其末尾添加特定的 <code>&#39;&lt;pad&gt;&#39;</code> 词元， 直到其长度达到<code>num_steps</code>； 反之，我们将截断文本序列时，只取其前<code>num_steps</code> 个词元， 并且丢弃剩余的词元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="comment"># 对数字索引的列表格式化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在给定一个时序窗口时，若句子序列够长，则截断</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="comment"># 若句子序列长度不够时序窗口，则添加填充字符 pad</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line"><span class="comment"># [47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span></span><br></pre></td></tr></table></figure><p>当模型通过一个词元接一个词元地生成序列进行预测时， 生成的 <code>&lt;eos&gt;</code> 词元说明完成了序列输出工作。所以对于每一个句子子序列，需要将特定的 <code>&lt;eos&gt;</code> 词元添加到所有序列的末尾</p><p>其次，为后续参数更新，记录每个句子子序列的有效长度（排除 <code>&lt;pad&gt;</code> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将句子序列切分为等长的数字索引子序列，长度为时序窗口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将输入数据集中的每一句子序列从token转化为数字序列</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="comment"># 在每个句子末尾添加 &#x27;&lt;eos&gt;&#x27; 标识句子结束</span></span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="comment"># 对完整句子逐行按时序步长切分，长度不够的用pad填充</span></span><br><span class="line">    array = torch.tensor([truncate_pad(l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    <span class="comment"># 每个子序列的有效长度：除去填充字符的token数，即为句子的有效长度</span></span><br><span class="line">    valid_len = (array ! = vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure><h5 id="返回数据迭代器"><a href="#返回数据迭代器" class="headerlink" title="返回数据迭代器"></a>返回数据迭代器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    <span class="comment"># 将文本数据集词元化</span></span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    <span class="comment"># 将传入的data_arrays元组的各项按批量大小划分后，组合为一个迭代项</span></span><br><span class="line">    <span class="comment">#   一个迭代项是包含多个元素的一个元组，其中每个元素都是各项的一个批量</span></span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><p>查看输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> X:</span><br><span class="line">        <span class="built_in">print</span>([src_vocab.idx_to_token[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> it],<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> Y:</span><br><span class="line">        <span class="built_in">print</span>([tgt_vocab.idx_to_token[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> it],<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#	X: tensor([[ 6, 97,  4,  3,  1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#	        [ 6, 18, 60,  4,  3,  1,  1,  1]], dtype=torch.int32)</span></span><br><span class="line"><span class="comment">#	[&#x27;i&#x27;, &#x27;tried&#x27;, &#x27;.&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;] </span></span><br><span class="line"><span class="comment">#	</span></span><br><span class="line"><span class="comment">#	[&#x27;i&#x27;, &#x27;am&#x27;, &#x27;here&#x27;, &#x27;.&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;] </span></span><br><span class="line"><span class="comment">#	</span></span><br><span class="line"><span class="comment">#	X的有效长度: tensor([4, 5])</span></span><br><span class="line"><span class="comment">#	Y: tensor([[10,  0,  4,  3,  1,  1,  1,  1],</span></span><br><span class="line"><span class="comment">#	        [ 6,  7, 53,  4,  3,  1,  1,  1]], dtype=torch.int32)</span></span><br><span class="line"><span class="comment">#	[&quot;j&#x27;ai&quot;, &#x27;&lt;unk&gt;&#x27;, &#x27;.&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;] </span></span><br><span class="line"><span class="comment">#	</span></span><br><span class="line"><span class="comment">#	[&#x27;je&#x27;, &#x27;suis&#x27;, &#x27;ici&#x27;, &#x27;.&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;] </span></span><br><span class="line"><span class="comment">#	</span></span><br><span class="line"><span class="comment">#	Y的有效长度: tensor([4, 5])</span></span><br></pre></td></tr></table></figure><h3 id="9-2-2-编码器-解码器"><a href="#9-2-2-编码器-解码器" class="headerlink" title="9.2.2 编码器-解码器"></a>9.2.2 编码器-解码器</h3><blockquote><p>可以理解为一种设计模式</p></blockquote><p>编码器：将输入数据转换为中间表达形式（特征），且这个中间表达形式对机器学习友好</p><p>解码器：将中间表达形式转换为输出数据（标号）</p><p><strong>CNN中的编码器-解码器</strong></p><p><img src="/posts/2699312118/image-20240409103816847.png" alt="image-20240409103816847"></p><p><strong>RNN中的编码器-解码器</strong></p><p>编码器：将文本转换为隐状态向量</p><p>解码器：将隐状态向量转换为输出</p><p><img src="/posts/2699312118/image-20240409103914543.png" alt="image-20240409103914543"></p><h4 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h4><p><img src="/posts/2699312118/image-20240409104505573.png" alt="image-20240409104505573"></p><p>编码器：处理输入，输出为解码器的初始隐状态特征张量</p><p>解码器：生成输出，基于中间特征张量生成输出</p><hr><p>以 “英语-法语” 机器翻译这样的 <em>序列-序列</em> 为例，给定词元化的英文句子序列 <code>&#39;They&#39; &#39;are&#39; &#39;watching&#39; &#39;.&#39;</code> ，将长度可变的输入序列编码成一个隐状态。然后对该状态解码，一个 <code>token</code> 接一个 <code>token</code> 生成翻译后的序列 <code>&#39;Ils&#39; &#39;regordent&#39; &#39;.&#39;</code></p><h4 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h4><h5 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h5><p>在编码器中，只需要输入源序列 $X$ ，与神经网络的前向传递类似</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h5 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h5><p>在神经网络前向传递的基础上，需要接收编码器的输出状态 <code>init_state()</code> ，并将其转为解码器的初始状态</p><p>若对于解码器的输出序列有额外的信息，（如：输出序列的长度），解码器还可接受输入 <code>enc_outputs</code></p><p>编码器会逐个地生成长度可变的词元序列，在每个时间步都会将 <em>输入</em> 和 <em>编码后的状态</em> 映射成当前时间步的输出词元</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h5 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        <span class="comment">#整个架构前向传递时，先编码，用编码器的输出和解码器的输入初始化解码器的状态</span></span><br><span class="line">        <span class="comment"># 返回编码器的输出序列</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><h3 id="9-2-3-Seq2Seq"><a href="#9-2-3-Seq2Seq" class="headerlink" title="9.2.3 Seq2Seq"></a>9.2.3 Seq2Seq</h3><p>Seq2Seq可以解决 <strong>变长序列-变长序列</strong> 的学习任务</p><h4 id="编码器解码器架构"><a href="#编码器解码器架构" class="headerlink" title="编码器解码器架构"></a>编码器解码器架构</h4><p><img src="/posts/2699312118/image-20240410001959811.png" alt="image-20240410001959811"></p><p>编码器是一个RNN，读取整个句子</p><ul><li>输入序列的信息被 <strong>编码</strong> 到RNN的隐状态中</li><li>可以双向RNN，因为不需要用编码器做预测</li></ul><p>解码器用另一个RNN输出</p><ul><li>解码器基于 <strong>输入序列的编码信息</strong> 的 <strong>输出序列已经生成的词元</strong> 来预测下一个词元</li><li><code>&lt;eos&gt;</code> 是序列结束的标识，一旦输出序列生成此词元，模型停止预测</li><li><code>&lt;bos&gt;</code> 是序列开始的标识，是解码器输入的第一个词元</li></ul><h5 id="编码器获取序列的上下文"><a href="#编码器获取序列的上下文" class="headerlink" title="编码器获取序列的上下文"></a>编码器获取序列的上下文</h5><p>假设编码器的输入序列是 $\{x_1,x_2,\cdots,x_T\}$ ，在第 $t$ 个时间步，RNN的循环层将词元 $x_t$ 的输入词元向量 $\mathbf{x}_t$ 和上一时刻隐状态 $\mathbf{h}_{t-1}$ 转换为 $\mathbf{h}_t$</p><script type="math/tex;mode=display">\mathbf{h}_t=f(\mathbf{x}_t,\mathbf{h}_{t-1})</script><p>可以将编码器理解为 $q()$ ，将输入序列与隐状态转换为序列的上下文变量，在序列的最后一个时间步 $T$ ，可以获得输入序列完整的上下文信息</p><script type="math/tex;mode=display">\mathbf{c}=q(\mathbf{h}_1,\cdots,\mathbf{h}_T)</script><h5 id="解码器-1"><a href="#解码器-1" class="headerlink" title="解码器"></a>解码器</h5><p>来自训练数据集的输出序列 $\{y_1,\cdots,y_{T’}\}$ ，对于时间步 $t’$ ，解码器输出 $y_{t’}$ 的概率取决于输出子序列的上文与输入序列的上下文变量 $\mathbf{c}$ ，即 $P(y_{t’}\vert y_1,\cdots,y_{t’-1},\mathbf{c})$</p><ul><li>上下文变量需要在所有时间步与解码器的输入拼接</li></ul><p>所以在解码器中，对于任一时间步 $t’$ ，RNN将来自上一时间步的输出词元向量 $\mathbf{y}_{t’-1}$ 、输入序列的上下文变量 $\mathbf{c}$ 、输出序列的上文 $\mathbf{s}_{t’-1}$ 转换为本层隐状态 $\mathbf{s}_{t’}$ 和本层输出词元变量 $\mathbf{y}_{t’}$</p><script type="math/tex;mode=display">\mathbf{s}_{t'}=f(\mathbf{y}_{t'-1},\mathbf{c},\mathbf{s}_{t'-1})</script><h5 id="编码器与解码器间的状态传递"><a href="#编码器与解码器间的状态传递" class="headerlink" title="编码器与解码器间的状态传递"></a>编码器与解码器间的状态传递</h5><p>将编码器最后一个时间步的输出隐状态 $\mathbf{h}_T$ 用作解码器的初始隐状态 $\mathbf{s}_0$</p><p>即基于RNN实现的编码器与解码器需要有相同的循环层数及每个隐状态数的神经元数相等</p><p><img src="/posts/2699312118/image-20240410012002938.png" alt="image-20240410012002938"></p><h5 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h5><p>在得到解码器的隐状态后，用输出层和 <em>softmax</em> 计算 $t’$ 时的输出 $y_{t’}$ 的条件概率 $P(y_{t’}\vert y_1,\cdots,y_{t’-1},\mathbf{c})$</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>在每个时间步，解码器都预测了输出词元的概率分布，使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化</p><p>在生成批量子序列时，不同长度的子序列通过填充或截取 <code>truncate_pad()</code> 组成长度相同的批量子序列，但填充词元的预测不应该算入损失函数，需要 <strong>通过零值化屏蔽不相关的项</strong></p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p><img src="/posts/2699312118/image-20240410102345041.png" alt="image-20240410102345041"></p><p>强制教学：特定的序列开始词元 <code>&lt;bos&gt;</code> 和 真实的标签序列 拼接在一起作为解码器的输入</p><h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p><img src="/posts/2699312118/image-20240410102423598.png" alt="image-20240410102423598"></p><p>一个词元接一个词元的方式预测输出序列，每个解码器当前时间步的输入都来自于上一时间步的预测词元。</p><p>在预测开始，序列开始词元 <code>&lt;bos&gt;</code> 需要被送入解码器</p><h4 id="预测序列的评估"><a href="#预测序列的评估" class="headerlink" title="预测序列的评估"></a>预测序列的评估</h4><p>预测序列 $\hat{\mathbf{Y}}$ 通过与真实的标签序列 $\mathbf{Y}$ 比较来评估预测序列的质量。BLEU是最早用于评估机器翻译结果的指标。</p><p>对于预测序列 $\hat{\mathbf{Y}}$ 中的 <em>n-元语法</em> ，BLEU评估的是这个 <em>n-元语法</em> 是否出现在真实的标签序列 $\mathbf{Y}$ 中</p><p><img src="/posts/2699312118/image-20240410102154589.png" alt="image-20240410102154589"></p><p>$\text{len}_{label},\text{len}_{pred}$ 分别是标签序列中的词元数和预测序列中的词元数，$k$ 是检测的最大语法单元，即最多检测预测序列中的 $k-gram$ 属于真实序列 $\mathbf{Y}$ 的概率</p><p>具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$ 和预测序列$A$、$B$、$B$、$C$、$D$，</p><ul><li><em>1-gram</em> ：$\{A,B,B,C,D\}$ 所以1元语法有5个，$A,B,C,D$ 在 $\mathbf{Y}$ 中出现一次，$B$ 只出现一次，所以 $p_1=\frac{4}{5}$</li><li><em>2-gram</em> ：$\{AB,BB,BC,CD\}$ ，2元语法有4个，其中，$BB$ 未在 $\mathbf{Y}$ 中出现，所以 $p_2=\frac{3}{4}$</li><li><em>3-gram</em> ：$\{ABB,BBC,BCD\}$ ，3元语法有3个，只有 $BCD$ 在 $\mathbf{Y}$ 中出现，所以 $\mathbf{p}_3=\frac{1}{3}$</li></ul><p>当预测序列与标签序列完全相同时，<em>BLEU</em> 是1。分别从预测序列的长度可语法匹配度衡量 Encoder-Decoder 的预测质量</p><ul><li><p>$n-gram$ 越长，则匹配难度越大，因此为更长的 $n$ 元语法分配更大的权重，即对于 $0\le p_n\le 1$ 固定时，$p_n^{\frac{1}{2^n}}$ 会随着 $n$ 的增大而增大。同时，对于同样的 $n$ ，$p_n$ 越大，$p_n^{\frac{1}{2^n}}$ 也越大。</p></li><li><p>同时，由于生成的预测序列越短，与真实标签序列匹配的可能性 $p_n$ 越大，为避免过短的预测序列，采用惩罚项系数</p><p>当 $\mathrm{len}_{\text{label}}&gt;\mathrm{len}_{\text{pred}}$ 时，惩罚项系数小于1</p><p>当 $\mathrm{len}_{\text{label}}\le \mathrm{len}_{\text{pred}}$ 时，惩罚项系数等于1</p></li></ul><h4 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h5 id="编码器-1"><a href="#编码器-1" class="headerlink" title="编码器"></a>编码器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># word2vec / 嵌入层： 将输入的整数索引转换成固定维度的稠密向量</span></span><br><span class="line">        <span class="comment">#  用一个one-hot向量表示一个token，</span></span><br><span class="line">        <span class="comment">#    首先会造成批量子序列张量的稀疏</span></span><br><span class="line">        <span class="comment">#    其次，在空间上两个one-hot正交，表示token之间没有关系，因此并不能表示token之间的时序关系</span></span><br><span class="line">        <span class="comment">#  为弥补上述两个缺陷，需要词嵌入层，将词表表示为稠密向量, embed间的内积可表示二者的时序关系</span></span><br><span class="line">        <span class="comment">#  num_embeddings：token数量</span></span><br><span class="line">        <span class="comment">#  embedding_dim：嵌入层每个embed的维度</span></span><br><span class="line">        <span class="comment"># 返回(1,embed_size)的词向量</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># nn.GRU使用默认的init_state()，将初始状态变为0</span></span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="comment"># 输入X的形状 (batch_size,num_steps)</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># 功能：将输入的批量子序列的词元用embed_size维的张量embed表示</span></span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步，即 (T,B,E) (7,4,8)</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 如果未初始化状态，则默认为0</span></span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps,batch_size,num_hiddens) (7,4,16)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens) (2,4,16)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>嵌入层是一个矩阵，行数等于输入词表的大小（<code>vocab_size</code>），列数等于特征向量的维度（<code>embed_size</code>）。对于任意输入词元的索引 $i$ ， 嵌入层获取权重矩阵的第 $i$ 行返回其特征向量</p><p><strong>encoder测试</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># dropout不生效</span></span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 输入是批量的数字索引的词元子序列，</span></span><br><span class="line"><span class="comment">#    即一个实数表示一个词元，用vocab可将数字映射为token </span></span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br></pre></td></tr></table></figure><h5 id="解码器-2"><a href="#解码器-2" class="headerlink" title="解码器"></a>解码器</h5><p>李沐的示例代码实现的实下图，”lls” 的上下文变量为编码器生成的原始上下文变量，而 “regardent” 的上下文变量不再是编码器得到原始上下文变量。变为 $s_{t’-1}$ ，是对 $\mathbf{y}_1$ 与 $\mathbf{c}$ 的组合提取出的时序特征</p><p><img src="/posts/2699312118/d3fb1230446f4f08e0313ce89f703b75a59a3e58.png" alt="image"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        <span class="comment"># 解码器有输出层，需要将时序隐状态转换为输出</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="comment"># 从encoder()返回值中取最后时间步的时序隐状态state作为decoder的初始隐状态</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># 输入X的形状：(batch_size,num_steps)</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(num_steps,batch_size,embed_size)-(7,4,8)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># state[-1]:返回state的最后两维 (批量,embed)-(4,16)</span></span><br><span class="line">        <span class="comment">#  encoder输出隐状态的最后一层，累积了encoder最大可提取的批量子序列的时序信息</span></span><br><span class="line">        <span class="comment">#     即批量中每个子序列的上下文信息</span></span><br><span class="line">        <span class="comment"># context-(7,4,16):沿时序窗口广播最后一个时刻隐状态的最后一层，使其具有与X相同的num_steps </span></span><br><span class="line">        <span class="comment">#   repeat(a,b,c) 沿着第一个维度重复a次，第二个维度重复b次，第三个维度重复c次</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># X_and_context-(7, 4, 24):相当于为目标序列每个时间步的输入都提供源序列的上下文时序信息</span></span><br><span class="line">        <span class="comment"># [X context] 解码器输入的信息越充分，效果越好。</span></span><br><span class="line">        <span class="comment">#   所以将源序列的上下文与目标序列的输入一起送入解码器进行解码</span></span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240410001959811.png" alt="image-20240410001959811"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        <span class="comment"># 解码器有输出层，需要将时序隐状态转换为输出</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="comment"># 保存encoder对输入序列生成的context变量</span></span><br><span class="line">        self.ori_context = enc_outputs[<span class="number">1</span>][-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 从encoder()返回值中取最后时间步的时序隐状态state作为decoder的初始隐状态</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># 输入X的形状：(batch_size,num_steps)</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(num_steps,batch_size,embed_size)-(7,4,8)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># state[-1]:返回state的最后两维 (批量,embed)-(4,16)</span></span><br><span class="line">        <span class="comment">#  encoder输出隐状态的最后一层，累积了encoder最大可提取的批量子序列的时序信息</span></span><br><span class="line">        <span class="comment">#     即批量中每个子序列的上下文信息</span></span><br><span class="line">        <span class="comment"># context-(7,4,16):沿时序窗口广播最后一个时刻隐状态的最后一层，使其具有与X相同的num_steps </span></span><br><span class="line">        <span class="comment">#   repeat(a,b,c) 沿着第一个维度重复a次，第二个维度重复b次，第三个维度重复c次</span></span><br><span class="line">        context = self.ori_context.repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># X_and_context-(7, 4, 24):相当于为目标序列每个时间步的输入都提供源序列的上下文时序信息</span></span><br><span class="line">        <span class="comment"># [X context] 解码器输入的信息越充分，效果越好。</span></span><br><span class="line">        <span class="comment">#   所以将源序列的上下文与目标序列的输入一起送入解码器进行解码</span></span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p><strong>解码器测试</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># X(4,7) </span></span><br><span class="line"><span class="comment">#  encoder(X)[0]:encoder输出的output(7,4,16);</span></span><br><span class="line"><span class="comment">#  encoder(X)[1]:encoder输出的state(2,4,16)</span></span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br></pre></td></tr></table></figure><h5 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h5><p>所有预测词元的掩码设置为1，对于有效长度外的序列，其掩码设置为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在序列中屏蔽不相关的项，过滤&lt;pad&gt;&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 因为源序列与目标序列都是变长的，所以需要成批量的返回每个输入子序列的有效长度</span></span><br><span class="line">    <span class="comment">#  批量子序列为(句子数,词元数,词向量)，子序列长度为词元数，所以子序列的长度为第一维长度</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># maxlen为批量系列X中的最大长度。</span></span><br><span class="line">    <span class="comment">#  对于一个序列，若某个位置的索引小于序列的有效长度，则说明在有效序列中，mask[][i]=True</span></span><br><span class="line">    <span class="comment">#   若某个位置的索引大于等于序列的有效长度，mask[][i]=False</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># 将mask为False的元素用value填充</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"><span class="comment">#	tensor([[1, 0, 0],</span></span><br><span class="line"><span class="comment">#	        [4, 5, 0]])</span></span><br></pre></td></tr></table></figure><p>通过带屏蔽的softmax交叉熵损失遮蔽不相关损失：将预测序列中所有词元的损失乘掩码，过滤掉填充词元生成的不相关预测产生的损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred的形状：(batch_size,num_steps,vocab_size)</span></span><br><span class="line">    <span class="comment"># label的形状：(batch_size,num_steps) label是数字索引的词元</span></span><br><span class="line">    <span class="comment"># valid_len的形状：(1,batch_size)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span><br><span class="line">        <span class="comment"># 掩码矩阵初始化为1，形状与label相同</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        <span class="comment"># 将有效部分设1，无效部分设0</span></span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># unweighted_loss指含无效词元的预测输出与真实输出的损失</span></span><br><span class="line">        <span class="comment">#   由于torch计算损失时，是在第1维上进行，所以需要将词向量转置到第1维</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>().forward(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 对批量中每个子序列有效部分的预测输出与真实输出损失取平均，作为批量的损失</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><h5 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">xavier_init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    <span class="comment"># 进入训练模式</span></span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                     xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失总和，词元数量</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            <span class="comment"># Y-(B,T),为每个句子子序列开头添加&#x27;&lt;bos&gt;&#x27; 并后移其他词元</span></span><br><span class="line">            <span class="comment">#   bos-(B,1)，即B个&lt;bos&gt;的列向量</span></span><br><span class="line">            bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>],</span><br><span class="line">                          device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 为输入Y在第1维拼接bos列，在数据迭代器中，为每个句子子序列的末尾添加了&lt;eos&gt;</span></span><br><span class="line">            <span class="comment">#   拿掉最后一个词元，因为decoder的输入不需要 &lt;eos&gt; </span></span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()	<span class="comment"># 损失函数的标量进行“反向传播”</span></span><br><span class="line">            d2l.grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">        <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        <span class="comment">#整个架构前向传递时，先编码，用编码器的输出和解码器的输入初始化解码器的状态</span></span><br><span class="line">        <span class="comment"># 返回编码器的输出序列</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/2699312118/image-20240410104005425.png" alt="image-20240410104005425"></p><p>出现过拟合</p><h5 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span><br><span class="line"><span class="params">                    device, save_attention_weights=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在预测时将net设置为评估模式</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 对src_sentence的预处理，末尾添加&lt;eos&gt;并转化为数字索引词元并添加 </span></span><br><span class="line">    <span class="comment">#  按时间步长截取子序列</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># encoder的输入是当前时刻词元的数字索引  形状是 (batch_size,num_steps) 添加批量轴</span></span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># encoder输出为output,state</span></span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    <span class="comment"># 解码器的输入为 &#x27;&lt;bos&gt;&#x27; ，由于解码器输入形状为 (batch_size,num_steps) 添加批量轴 </span></span><br><span class="line">    dec_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), </span><br><span class="line">        dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        <span class="comment"># Y的形状:(batch_size,num_steps,vocab_size)-(1,T,q)</span></span><br><span class="line">        <span class="comment"># dec_state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        <span class="comment">#  .argmax()，返回2轴最大值的索引</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重（稍后讨论）</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure><h5 id="质量评估"><a href="#质量评估" class="headerlink" title="质量评估"></a>质量评估</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="comment">#长为 n 的n-gram放到字典 label_subs 中</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 111</span></span><br><span class="line"><span class="comment"># go . =&gt; va le chercher au feu ., bleu 0.000</span></span><br><span class="line"><span class="comment"># 111</span></span><br><span class="line"><span class="comment"># i lost . =&gt; j&#x27;ai perdu ., bleu 1.000</span></span><br><span class="line"><span class="comment"># 111</span></span><br><span class="line"><span class="comment"># he&#x27;s calm . =&gt; il est &lt;unk&gt; ., bleu 0.658</span></span><br><span class="line"><span class="comment"># 111</span></span><br><span class="line"><span class="comment"># i&#x27;m home . =&gt; je suis sûr ., bleu 0.512</span></span><br></pre></td></tr></table></figure><p>修改为原始context后</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">111</span></span><br><span class="line">go . =&gt; va !, bleu <span class="number">1.000</span></span><br><span class="line"><span class="number">111</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu ., bleu 1.000</span></span><br><span class="line"><span class="string">111</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; il est riche ., bleu <span class="number">0.658</span></span><br><span class="line"><span class="number">111</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi ., bleu 1.000</span></span><br></pre></td></tr></table></figure><h3 id="9-2-4-束搜索"><a href="#9-2-4-束搜索" class="headerlink" title="9.2.4 束搜索"></a>9.2.4 束搜索</h3><p><strong>解码器逐个词元地生成输出序列</strong>，在任意时间步 $t’$ ，解码器输出 $\mathbf{y}_{t’}$ 的概率取决于 $\{\mathbf{y}_{t’-1},\cdots,\mathbf{y}_{1}\}$ 和源序列上下文变量 $\mathbf{c}$ 。若输出词表 $\mathcal{Y}$ ，输出序列的最大词元数为 $T’$ ，则解码器生成序列时相当于从所有 $\mathcal{O}\left(\vert \mathcal{Y}\vert^{T’} \right)$ 个可能序列中找出最理想的组合。</p><h4 id="其他搜索方法"><a href="#其他搜索方法" class="headerlink" title="其他搜索方法"></a>其他搜索方法</h4><h5 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h5><p><code>argmax</code> 是使用贪心算法选择当前看到的可能性最大的词元，对于输出序列的每一时间步 $t’$ ，基于贪心搜索从 $\mathcal{Y}$ 中找出最高条件概率的词元</p><script type="math/tex;mode=display">y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \vert y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p><img src="/posts/2699312118/image-20240410165521289.png" alt="image-20240410165521289"></p><p>如：序列 $\{A,B,C,<eos>\}$ 的条件概率为 $0.5\times 0.4\times 0.4\times 0.6=0.048$</eos></p><p>实际上，最优序列应该是最大化 $\prod\limits_{t’=1}^{T’}p\left(y_{t’}\vert y_1,\cdots,y_{t’-1},\mathbf{c}\right)$ 的输出序列，<strong>贪心搜索并不能保证条件概率的连乘最大</strong></p><p><img src="/posts/2699312118/image-20240410170543002.png" alt="image-20240410170543002"></p><p>时间步 $2$ 中，选择条件概率第二高的词元 $C$ ，由于时间步 $3$ 基于时间步 $1,2$ 处的输出子序列生成词元，$1,2$ 步的词元从 $A,B$ 变为 $A,C$ ，时间步 $3$ 处每个词元的条件概率也会相应变化。同理，时间步 $4$ 每个词元的条件概率也发生变化。此时，变化后的序列条件概率为 $0.5\times 0.3\times 0.6\times 0.6=0.054$</p><h5 id="穷举搜索"><a href="#穷举搜索" class="headerlink" title="穷举搜索"></a>穷举搜索</h5><p>若目标是获得全局最优序列，不考虑解码器的前提下，使用 <strong>穷举搜索</strong> 可以获得全局最优的条件概率</p><p>但是，计算每个序列的条件概率时间复杂度为 $\mathcal{O}\left(\vert \mathcal{Y}\vert^{T’} \right)$ ，指数级复杂度现有的计算机不可能计算</p><h4 id="束搜索"><a href="#束搜索" class="headerlink" title="束搜索"></a>束搜索</h4><p>为了在精度与计算成本之间做权衡，有束搜索</p><p>超参数：束宽 $k$</p><h5 id="输出序列生成"><a href="#输出序列生成" class="headerlink" title="输出序列生成"></a>输出序列生成</h5><p>在每个时间步 $t’$ 选择条件概率最高的前 $k$ 个词元，成为 $k$ 个候选输出序列的第一个词元 。</p><p>在下一个时间步 $t’+1$ ，从 $k\vert \mathcal{Y}\vert$ 个可能的词元中选择条件概率最高的 $k$ 个候选词元</p><p><img src="/posts/2699312118/image-20240410173453704.png" alt="image-20240410173453704"></p><p>假设输出的词表只包含五个元素$\mathcal{Y} = \{A, B, C, D, E\}$，其中有一个是“&lt;eos&gt;”。设置 $k=2$，输出序列的最大长度为 $T’=3$</p><p>在时间步$1$，假设具有最高条件概率$ P(y_1 \vert \mathbf{c})$ 的词元是 $A$ 和 $C$</p><p>在时间步$2$，我们计算所有 $y_2 \in \mathcal{Y}$ 为：</p><script type="math/tex;mode=display">\begin{aligned}
P(A, y_2 \vert \mathbf{c}) \triangleq \begin{cases}
P(A \vert \mathbf{c})P(A \vert A, \mathbf{c})\\
P(A \vert \mathbf{c})P(B \vert A, \mathbf{c})\\
P(A \vert \mathbf{c})P(C \vert A, \mathbf{c})\\
P(A \vert \mathbf{c})P(D \vert A, \mathbf{c})\\
P(A \vert \mathbf{c})P(E \vert A, \mathbf{c})\\
\end{cases}\\ \\
P(C, y_2 \vert \mathbf{c}) \triangleq \begin{cases}
P(C \vert \mathbf{c})P(A \vert C, \mathbf{c})\\
P(C \vert \mathbf{c})P(B \vert C, \mathbf{c})\\
P(C \vert \mathbf{c})P(C \vert C, \mathbf{c})\\
P(C \vert \mathbf{c})P(D \vert C, \mathbf{c})\\
P(C \vert \mathbf{c})P(E \vert C, \mathbf{c})\\
\end{cases}\\ 
\end{aligned}</script><p>从这十个值中选择最大的两个，比如$P(A, B \vert \mathbf{c})$和$P(C, E \vert \mathbf{c})$。</p><p>然后在时间步$3$，计算所有 $y_3 \in \mathcal{Y}$ 为：</p><script type="math/tex;mode=display">\begin{aligned}
P(A, B, y_3 \vert \mathbf{c}) = P(A, B \vert \mathbf{c})P(y_3 \vert A, B, \mathbf{c})\\
P(C, E, y_3 \vert \mathbf{c}) = P(C, E \vert \mathbf{c})P(y_3 \vert C, E, \mathbf{c})
\end{aligned}</script><p>从这十个值中选择最大的两个，即$P(A, B, D \vert \mathbf{c})$和$P(C, E, D \vert \mathbf{c})$</p><p>我们会得到六个候选输出序列：</p><p>（1）$A$；（2）$C$；（3）$A,B$；（4）$C,E$；（5）$A,B,D$；（6）$C,E,D$。</p><p>最后，基于这六个序列（例如，丢弃包括“&lt;eos&gt;”和之后的部分），我们获得最终候选输出序列集合。</p><h5 id="考虑输出序列长度"><a href="#考虑输出序列长度" class="headerlink" title="考虑输出序列长度"></a>考虑输出序列长度</h5><p>计算候选序列的分数</p><script type="math/tex;mode=display">\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\vert \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \vert y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p>其中$L$是候选序列的长度，$\alpha$通常设置为$0.75$。</p><p>然后我们选择其中候选序列的分数最高的即条件概率乘积最高的序列作为输出序列</p><p>因为长序列的条件概率都很小，且求和的对数项更多，所以用 $L^\alpha$ 奖励长序列</p><ul><li>序列越长，条件概率越小 $\Rightarrow \log P&lt;0$ 越小，直接比较对数条件概率的和会造成长序列分数永远小于短序列分数。</li><li>长度越长 $\frac{1}{L^{\alpha}}$ 越小，$\frac{1}{L^{\alpha}}\sum \log P&lt;0$ 越大</li></ul><p>束搜索的计算量为$\mathcal{O}(k\times \vert \mathcal{Y}\vert \times T’)$</p><h2 id="9-3-应用"><a href="#9-3-应用" class="headerlink" title="9.3 应用"></a>9.3 应用</h2><h3 id="9-3-1-应用到机器学习"><a href="#9-3-1-应用到机器学习" class="headerlink" title="9.3.1 应用到机器学习"></a>9.3.1 应用到机器学习</h3><p>循环神经网络可以应用到很多不同类型的机器学习任务，分为三种模式：</p><ul><li>序列到类别模式</li><li>同步的序列到序列模式</li><li>异步的序列到序列模式</li></ul><h4 id="序列到类别模式"><a href="#序列到类别模式" class="headerlink" title="序列到类别模式"></a>序列到类别模式</h4><p>主要用于序列数据的分类问题：输入为序列，输出为类别</p><p>如：语言的情感分类</p><p><img src="/posts/2699312118/image-20231013103557927.png" alt="image-20231013103557927"></p><p>序列样本 $x_{1:T}=(x_1,\cdots,x_T)$ 为一个长度为 $T$ 的序列，输出为一个类别 $y\in \{1,\cdots,C\}$ ，将样本按不同时刻输入到RNN中，得到不同时刻的隐状态 $h_1,\cdots,h_T$ ，将 $h_T$ 作为整个序列的最终表示，输入给分类器 $g(\cdot)$ 进行分类</p><script type="math/tex;mode=display">\hat{y}=g(h_T)</script><ul><li>其中 $g(\cdot)$ 可以是简单的线性分类器（Logistic）或者复杂的分类器（多层前馈神经网络）</li></ul><p><img src="/posts/2699312118/image-20231013104135009.png" alt="image-20231013104135009"></p><p>除了将最后时刻的状态作为整个序列的表示外（正常模式），还可以对整个序列的平均状态作为整个序列的表示（平均采样模式）</p><script type="math/tex;mode=display">\hat{y}=g\left(\frac{1}{T}\sum\limits_{t=1}^T\right)</script><h4 id="同步的序列到序列模式"><a href="#同步的序列到序列模式" class="headerlink" title="同步的序列到序列模式"></a>同步的序列到序列模式</h4><blockquote><p>主要用于 <strong>序列标注</strong> 任务，即每一时刻的输入和输出一一对应，输入序列和输出序列的长度相同</p></blockquote><p>输入为序列样本 $x_{1:T}=(x_1,\cdots,x_T)$ ，输出为序列 $y_{1:T}=(y_1,\cdots,y_T)$ 。样本按不同时刻输入到RNN中，得到不同时刻的隐状态 $h_1,\cdots,h_T$ ，每个时刻的隐状态 $h_t$ 代表了当前时刻和历史信息，并输入给分类器 $g(\cdot)$ 得到当前时刻的标签 $\hat{y}_t$</p><script type="math/tex;mode=display">\hat{y}_t=g(h_t),\forall t\in [1,T]</script><p><img src="/posts/2699312118/image-20231013104732726.png" alt="image-20231013104732726"></p><p><strong>应用</strong></p><ul><li><p>词性标注问题：中文分词</p><p><img src="/posts/2699312118/image-20231013105018503.png" alt="image-20231013105018503"></p></li><li><p>信息抽取：从无结构的文本中抽取结构化的信息，形成知识</p><p><img src="/posts/2699312118/image-20231013105058398.png" alt="image-20231013105058398"></p></li></ul><h5 id="异步的序列到序列模式"><a href="#异步的序列到序列模式" class="headerlink" title="异步的序列到序列模式"></a>异步的序列到序列模式</h5><blockquote><p>编码器-解码器模型：输入序列和输出序列不需要有严格的对应关系，也不需要保持长度相同</p></blockquote><p>输入为长度为 $T$ 的序列样本 $x_{1:T}=(x_1,\cdots,x_T)$ ，输出为长度为 $M$ 的序列 $y_{1:M}=(y_1,\cdots,x_M)$ 。</p><p>一般通过 <strong>先编码后半解码</strong> 的方式实现</p><p>先将样本 $x$ 按不同时刻输入到一个循环神经网络（编码器）中，并得到其编码 $h_T$ 。然后再使用另一个循环神经网络（解码器）得到输出序列 $\hat{y}_{1:M}$</p><p><img src="/posts/2699312118/image-20231013124904362.png" alt="image-20231013124904362"></p><ul><li>EOS表示输入序列结束</li></ul><script type="math/tex;mode=display">\begin{aligned}
&h_t=f_1(h_{t-1},x_t)&\quad \forall t\in [1,T]\\
&h_{T+1}=f_2(h_{T+t-1},\hat{y}_{t-1})&\quad \forall t\in [1,M]\\
&\hat{y}_t=g(h_{T+t})&\quad \forall t\in [1,M]
\end{aligned}</script><ul><li>$g(\cdot)$ 为分类器</li><li>$\hat{y}_t$ 为预测输出</li><li>解码器通常使用非线性的自回归模型，每一时刻输入为上一时刻的预测结果 $\hat{y}_{t-1}$</li></ul><p><strong>应用</strong></p><p>如：机器翻译，输入为源语言的单词序列，输出为目标语言的单词序列</p><ul><li><img src="/posts/2699312118/image-20231013130523936.png" alt="image-20231013130523936"></li></ul><h3 id="9-3-2-相关领域"><a href="#9-3-2-相关领域" class="headerlink" title="9.3.2 相关领域"></a>9.3.2 相关领域</h3><h4 id="生成语言模型"><a href="#生成语言模型" class="headerlink" title="生成语言模型"></a>生成语言模型</h4><p>自然语言理解：一个句子的可能性，合理性</p><p><img src="/posts/2699312118/image-20231015121736584.png" alt="image-20231015121736584"></p><h4 id="作词机"><a href="#作词机" class="headerlink" title="作词机"></a>作词机</h4><h4 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h4><p><strong>传统统计学习机器翻译</strong></p><p><img src="/posts/2699312118/image-20231015121856862.png" alt="image-20231015121856862"></p><p><strong>基于序列到序列的机器翻译</strong></p><p><img src="/posts/2699312118/image-20231015121943840.png" alt="image-20231015121943840"></p><ul><li>一个RNN用来编码</li><li>一个RNN用来解码</li></ul><h4 id="看图说话"><a href="#看图说话" class="headerlink" title="看图说话"></a>看图说话</h4><p><img src="/posts/2699312118/image-20231015122048994.png" alt="image-20231015122048994"></p><h4 id="写字"><a href="#写字" class="headerlink" title="写字"></a>写字</h4><p><img src="/posts/2699312118/image-20231015122131583.png" alt="image-20231015122131583"></p><p><img src="/posts/2699312118/image-20231015122142678.png" alt="image-20231015122142678"></p><h4 id="对话系统"><a href="#对话系统" class="headerlink" title="对话系统"></a>对话系统</h4></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/2699312118/" title="9.动手学深度学习-经典循环神经网络">https://amostian.github.io/posts/2699312118/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/1556323108/" rel="prev" title="8.动手学深度学习-循环神经网络"><i class="fa fa-chevron-left"></i> 8.动手学深度学习-循环神经网络</a></div><div class="post-nav-item"><a href="/posts/3125950576/" rel="next" title="5.Ceph操作及管理">5.Ceph操作及管理 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-RNN%E6%94%B9%E8%BF%9B"><span class="nav-text">9.1 RNN改进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-1-%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CGRU"><span class="nav-text">9.1.1 基于门控机制的循环神经网络GRU</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%97%E5%BF%98%E9%97%A8%E4%B8%8E%E6%9B%B4%E6%96%B0%E9%97%A8"><span class="nav-text">遗忘门与更新门</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-text">门控的实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%80%99%E9%80%89%E9%9A%90%E7%8A%B6%E6%80%81"><span class="nav-text">候选隐状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81"><span class="nav-text">隐状态</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%A8%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-text">门的作用</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-text">实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-text">简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-2-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9CLSTM"><span class="nav-text">9.1.2 长短期记忆网络LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%A8%E6%9C%BA%E5%88%B6"><span class="nav-text">门机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E5%85%83"><span class="nav-text">候选记忆元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E5%85%83"><span class="nav-text">记忆元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81-1"><span class="nav-text">隐状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-1"><span class="nav-text">实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0-1"><span class="nav-text">简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-3-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">9.1.3 深度循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-2"><span class="nav-text">实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0"><span class="nav-text">参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-4-%E5%8F%8C%E5%90%91%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">9.1.4 双向深度循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-text">隐马尔科夫模型中的动态规划</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C"><span class="nav-text">双向循环网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%8F%8C%E5%90%91%E9%80%92%E5%BD%92"><span class="nav-text">1.双向递归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E8%BE%93%E5%87%BA"><span class="nav-text">2. 输出</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7"><span class="nav-text">双向循环神经网络的代价</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E5%90%91%E6%B7%B1%E5%BA%A6LSTM%E5%AE%9E%E7%8E%B0"><span class="nav-text">双向深度LSTM实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%B8%8E%E8%BE%93%E5%87%BA%E5%BD%A2%E7%8A%B6"><span class="nav-text">参数与输出形状</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="nav-text">9.2 机器翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-1-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">9.2.1 数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">加载数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96%E4%B8%8E%E8%AF%8D%E8%A1%A8"><span class="nav-text">词元化与词表</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96"><span class="nav-text">词元化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8"><span class="nav-text">词表</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="nav-text">数据集迭代器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E6%97%B6%E5%BA%8F%E6%AD%A5%E9%95%BF%E8%B0%83%E6%95%B4%E5%8F%A5%E5%AD%90"><span class="nav-text">根据时序步长调整句子</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%94%E5%9B%9E%E6%95%B0%E6%8D%AE%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="nav-text">返回数据迭代器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-2-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">9.2.2 编码器-解码器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84"><span class="nav-text">编码器-解码器架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-3"><span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%88%E5%B9%B6"><span class="nav-text">合并</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-3-Seq2Seq"><span class="nav-text">9.2.3 Seq2Seq</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84"><span class="nav-text">编码器解码器架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E8%8E%B7%E5%8F%96%E5%BA%8F%E5%88%97%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="nav-text">编码器获取序列的上下文</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-1"><span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8%E9%97%B4%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%A0%E9%80%92"><span class="nav-text">编码器与解码器间的状态传递</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%87%BA"><span class="nav-text">输出</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B"><span class="nav-text">预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%BA%8F%E5%88%97%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="nav-text">预测序列的评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-4"><span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-1"><span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-2"><span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B-1"><span class="nav-text">预测</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0"><span class="nav-text">质量评估</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-4-%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="nav-text">9.2.4 束搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95"><span class="nav-text">其他搜索方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83%E6%90%9C%E7%B4%A2"><span class="nav-text">贪心搜索</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2"><span class="nav-text">穷举搜索</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="nav-text">束搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90"><span class="nav-text">输出序列生成</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%80%83%E8%99%91%E8%BE%93%E5%87%BA%E5%BA%8F%E5%88%97%E9%95%BF%E5%BA%A6"><span class="nav-text">考虑输出序列长度</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-3-%E5%BA%94%E7%94%A8"><span class="nav-text">9.3 应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-1-%E5%BA%94%E7%94%A8%E5%88%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-text">9.3.1 应用到机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E7%B1%BB%E5%88%AB%E6%A8%A1%E5%BC%8F"><span class="nav-text">序列到类别模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E7%9A%84%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%BC%8F"><span class="nav-text">同步的序列到序列模式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E7%9A%84%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%BC%8F"><span class="nav-text">异步的序列到序列模式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-2-%E7%9B%B8%E5%85%B3%E9%A2%86%E5%9F%9F"><span class="nav-text">9.3.2 相关领域</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-text">生成语言模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%9C%E8%AF%8D%E6%9C%BA"><span class="nav-text">作词机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="nav-text">机器翻译</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9C%8B%E5%9B%BE%E8%AF%B4%E8%AF%9D"><span class="nav-text">看图说话</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%99%E5%AD%97"><span class="nav-text">写字</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F"><span class="nav-text">对话系统</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">218</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">82</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1186.8k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">48:09</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>