<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="策略最简单的表示方法是查找表，即表格型策略，使用表格型策略的强化学习方法称为表格型方法  Q-learning Sarsa"><meta property="og:type" content="article"><meta property="og:title" content="强化学习——蘑菇书-表格型方法"><meta property="og:url" content="https://amostian.github.io/posts/1625593630/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="策略最简单的表示方法是查找表，即表格型策略，使用表格型策略的强化学习方法称为表格型方法  Q-learning Sarsa"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240127213921527.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128132917336.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128133526413.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128194801709.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128193119651.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128193132667.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128201557938.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128201858712.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128202126009.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128202256932.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240128212931067.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240129104505010.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240130094737368.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240130114131007.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240130114117555.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/3.19.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240126105101874-17062570388546.png"><meta property="og:image" content="https://amostian.github.io/posts/1625593630/image-20240130220344858.png"><meta property="article:published_time" content="2024-01-26T08:18:19.000Z"><meta property="article:modified_time" content="2024-02-04T14:41:54.569Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="强化学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/1625593630/image-20240127213921527.png"><link rel="canonical" href="https://amostian.github.io/posts/1625593630/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>强化学习——蘑菇书-表格型方法 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">58</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">353</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/1625593630/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">强化学习——蘑菇书-表格型方法</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-01-26 16:18:19" itemprop="dateCreated datePublished" datetime="2024-01-26T16:18:19+08:00">2024-01-26</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-02-04 22:41:54" itemprop="dateModified" datetime="2024-02-04T22:41:54+08:00">2024-02-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">6.3k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>策略最简单的表示方法是查找表，即表格型策略，使用表格型策略的强化学习方法称为表格型方法</p><ul><li>Q-learning</li><li>Sarsa</li></ul><span id="more"></span><h1 id="Q表"><a href="#Q表" class="headerlink" title="Q表"></a>Q表</h1><p>Q表：状态-价值表</p><p>Q函数的意义是计算在某个状态下选择某个动作，未来能够获得多少总奖励</p><p>如果可以预估未来的总奖励的大小，我们就知道在当前的状态下选择哪个动作未来获取的累积价值更高。</p><h3 id="为何用未来奖励评估当前动作好坏"><a href="#为何用未来奖励评估当前动作好坏" class="headerlink" title="为何用未来奖励评估当前动作好坏"></a>为何用未来奖励评估当前动作好坏</h3><p>在现实世界中奖励往往是延迟的，所以强化学习需要学习长期的奖励。对于MDP，我们一般会从当前状态开始，把后续有可能收到的奖励求期望作为当前动作的 $Q$ 值，代表当前状态下动作的真正价值</p><p>但多长时间的奖励算“长期”，如果任务很快就结束，考虑到最后一步的奖励无可厚非。但如果任务是一个持续的没有尽头的任务，我们做不到将未来全部的奖励相加作为当前状态的价值。所以我们引入折扣因子 $\gamma$ 来计算未来总奖励，$\gamma\in [0,1]$ ，$\gamma^n$ 越往后越小，即越后面的奖励对当前状态价值的影响就越小</p><p><img src="/posts/1625593630/image-20240127213921527.png" alt="image-20240127213921527"></p><p>若 $\gamma=0$ ，则只考虑单步的奖励，认为是一种目光短浅的算法</p><p>若 $\gamma=1$ ，则相当于将后续所有奖励都加起来，认为是一种目光过于长远的算法</p><h2 id="基于DP方法的有模型预测与控制"><a href="#基于DP方法的有模型预测与控制" class="headerlink" title="基于DP方法的有模型预测与控制"></a>基于DP方法的有模型预测与控制</h2><h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><blockquote><p>计算一个MDP的最优策略与最佳价值函数</p></blockquote><p>策略评估(预测)：迭代贝尔曼期望方程</p><script type="math/tex;mode=display">V^{(k)}(s)=\sum\limits_{a\in \mathcal{A}}\pi(a\vert s)\left(R(s,a)+\gamma\sum\limits_{s'\in \mathcal{S}}P(s'\vert s,a)V^{(k-1)}(s')\right)</script><p>策略改进：贪心 $\pi(a\vert s)=greedy(Q(s,a))$</p><script type="math/tex;mode=display">Q_{\pi^{(k)}}(s,a)=R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^{(k-1)}}(s')\\
\pi^{(k+1)}(a\vert s)=\mathop{\mathrm{argmax}}_aQ_{\pi^{(k)}}(s,a)</script><h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><blockquote><p>计算一个MDP的最优价值函数</p></blockquote><p>当获得最优价值函数时，满足贝尔曼最优方程</p><script type="math/tex;mode=display">\begin{align}
V^*(s)&=\max\limits_{a}Q^*(s,a)\\
&\xlongequal{Q=f(V)}\max\limits_{a}\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_{\pi^*}^*(s')\right)
\end{align}</script><p>迭代贝尔曼最优方程，直至收敛</p><script type="math/tex;mode=display">V_{\pi^{(k+1)}}(s)\leftarrow\max\limits_{a\in\mathcal{A}}\left(R(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'\vert s,a)V_{\pi^{(k)}}(s')\right)</script><p>当获取到最优价值函数时，贪心获取最佳策略</p><script type="math/tex;mode=display">\pi^*(a\vert s)=\mathop{\mathrm{argmax}}_a \left(R(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'\vert s,a)V_{\pi^{(H)}}(s')\right)</script><h1 id="免模型预测"><a href="#免模型预测" class="headerlink" title="免模型预测"></a>免模型预测</h1><p>在无法获取MDP模型的情况下，可以通过蒙特卡洛方法和时序差分方法来估计某个给定策略的价值</p><h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><h3 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h3><p>蒙特卡洛是基于采样的方法，给定金购策略 $\pi$ ，让智能体与环境交互，可以得到很多轨迹。在回合结束后，可计算每条轨迹的回报</p><script type="math/tex;mode=display">G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}\cdots</script><p>求出这些轨迹的回报的期望，就可以知道某一策略下的某个状态的价值</p><script type="math/tex;mode=display">V_{\pi}(s)=E_{\tau\sim \pi}[G_t\vert s_t=s]</script><p>蒙特卡洛方法使用 <strong>经验评估回报</strong> 的方法估计奖励，这样就不需要MDP中的状态转移函数与奖励函数。但蒙特卡洛方法的局限性是必须用在有终止的马尔科夫决策过程中。</p><h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol><li>在每个回合中，若 $t$ 时刻状态 $s$ 被访问了，则<ul><li>状态 $s_t$ 的访问数 $N(s_t)$ 增加1，$N(s_t)\leftarrow N(s_t)+1$</li><li>状态 $s_t$ 的总回报 $S(s_t)$ 增加 $G_t$ ，$S(s_t)\leftarrow S(s_t)+G_t$</li></ul></li><li>状态 $s_t$ 的价值通过回报的平均来估计，即 $V(s_t)=\frac{V(s_t)}{N(s_t)}$</li></ol><p>根据大数定律，只要有足够多的轨迹，就可以趋近这个策略对应的价值函数。当 $N(s_t)\rightarrow \infty$ 时，$V(s_t)\rightarrow V_{\pi}(s_t)$</p><h3 id="增量式蒙特卡洛"><a href="#增量式蒙特卡洛" class="headerlink" title="增量式蒙特卡洛"></a>增量式蒙特卡洛</h3><h4 id="增量式均值"><a href="#增量式均值" class="headerlink" title="增量式均值"></a>增量式均值</h4><p>假设有一批样本 $x_1,x_2,\cdots,x_t$ ，可以将经验均值转换为增量均值的形式</p><script type="math/tex;mode=display">\begin{aligned}
\mu_t&=\frac{1}{t}\sum\limits_{j=1}^tx_j\\
&=\frac{1}{t}\left(x_t+\sum\limits_{j=1}^{t-1}x_j\right)\\
&=\frac{1}{t}[x_t+(t-1)\mu_{t-1}]\\
&=\frac{1}{t}(x_t+t\mu_{t-1}-\mu_{t-1})\\
&=\mu_{t-1}+\frac{1}{t}(x_t-\mu_{t-1})
\end{aligned}</script><p>通过增量式均值，可以将当前时刻的均值与上一时刻的均值关联起来。当我们得到 $x_t$ 时，就可以用上一时刻的值来更新现在的值</p><ul><li>$x_t-\mu_{t-1}$ ：残差</li><li>$\frac{1}{t}$ ：学习率</li></ul><h4 id="增量式蒙特卡洛方法"><a href="#增量式蒙特卡洛方法" class="headerlink" title="增量式蒙特卡洛方法"></a>增量式蒙特卡洛方法</h4><p>可以将蒙特卡洛方法更新的方式改为增量式蒙特卡洛(incremental MC)</p><p>若现在有一个轨迹 $(s_1,a_1,r_1,s_2,\cdots,s_t)$ ，可以用增量的方法更新状态价值</p><script type="math/tex;mode=display">N(s_t)\leftarrow N(s_t)+1\\
V(s_t)\leftarrow V(s_t)+\frac{1}{N(s_t)}(G_t-V(s_t))</script><p>若将 $\frac{1}{N(s_t)}$ 变为 $\alpha$ 学习率，即</p><script type="math/tex;mode=display">V(s_t)\leftarrow V(s_t)+\alpha(G_t-V(s_t))</script><p>其中，$\alpha$ 代表更新的速率，可以将其作为超参数设置</p><h3 id="DP与MC方法的总结与对比"><a href="#DP与MC方法的总结与对比" class="headerlink" title="DP与MC方法的总结与对比"></a>DP与MC方法的总结与对比</h3><p><strong>DP</strong></p><ul><li><p>使用了自举的思想，基于之前估计的量来估计一个量</p></li><li><p>使用贝尔曼期望备份，通过上一时刻的 $V^{(k-1)}(s)$ 来更新当前时刻的 $V^{(k)}(s)$</p><script type="math/tex;mode=display">V^{(k)}(s)\leftarrow \sum\limits_{a\in \mathcal{A}}\pi(a\vert s)\left(R(s,a)+\gamma\sum\limits_{s'\in \mathcal{S}}P(s'\vert s,a)V^{(k-1)}\right)</script><p>将其不断迭代，最后可以收敛</p></li></ul><p>贝尔曼期望备份有两层加和，即内部加和与外部加和，计算两次期望，更新一次</p><p><img src="/posts/1625593630/image-20240128132917336.png" alt="image-20240128132917336"></p><p><strong>MC</strong></p><p>通过一个回合的经验平均回报(实际得到的奖励)来进行更新，即</p><script type="math/tex;mode=display">V^{(k)}(s_t)\leftarrow V^{(k-1)}(s_t)+\alpha\left(G_t^{(k)}-V^{(k-1)}(s_t)\right)</script><p>MC方法得到的轨迹对应树上蓝色的轨迹，轨迹上的状态已经是确定的，采取的动作也是确定的，现在只更新这条轨迹上的所有状态，与这条轨迹无关的状态不更新</p><p><img src="/posts/1625593630/image-20240128133526413.png" alt="image-20240128133526413"></p><h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><div class="table-container"><table><thead><tr><th></th><th>DP</th><th>MC</th></tr></thead><tbody><tr><td>环境是否已知</td><td>只适用于环境已知的MDP</td><td>都适用</td></tr><tr><td>更新部分</td><td>全部更新</td><td>局部更新</td></tr><tr><td>迭代速度</td><td>更新速度慢</td><td>更新速度快</td></tr></tbody></table></div><h2 id="时序差分方法"><a href="#时序差分方法" class="headerlink" title="时序差分方法"></a>时序差分方法</h2><p>时序差分方法的目的是对于某个给定的策略 $\pi$ ，在线的计算出它的价值函数 $V_{\pi}$ ，即一步一步地算。</p><p>时序差分方法是一种介于蒙特卡洛方法和动态规划方法中间的免模型方法，不需要MDP中的状态转移矩阵与奖励函数。时序差分方法可以从不完整的回合中学习，并且结合了自举的思想。</p><h3 id="一步时序差分"><a href="#一步时序差分" class="headerlink" title="一步时序差分"></a>一步时序差分</h3><p>每走一步更新一次Q表(做一步自举)，用得到的估计回报 $r_{t+1}+\gamma V(s_{t+1})$ 来更新上一时刻的值 $V(s_t)$ ，这种单步更新的方法称为一步时序差分，即 $TD(0)$</p><script type="math/tex;mode=display">V(s_t)\leftarrow V(s_t)+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_t))</script><ul><li><p>估计回报 $r_{t+1}+\gamma V(s_{t+1})$ 称为时序差分目标，带衰减的下一状态价值的和</p><ul><li>时序差分目标相当于对回报的采样</li><li>使用当前估计的 $V$ 而不是真实的 $V_{\pi}$</li></ul></li><li><p>时序差分误差 $\delta=r_{t+1}+\gamma V(s_{t+1})-V(s_t)$</p><p>类比蒙特卡洛方法，给定一个回合 $k$ ，可以更新 $V(s_t)$ 来逼近真实的回报 $G_t$</p><script type="math/tex;mode=display">V(s_t)\leftarrow V(s_t)+\alpha(G^{(k)}_t-V(s_t))</script><p>在MC方法中，$G_{t}^{(k)}$ 是可计算的值，因为一个回合已经完成了，可以算出每个状态实际的回报</p><p>在TD方法中，不等回合结束，只执行一步，状态的价值就会更新</p></li></ul><h3 id="n步时序差分"><a href="#n步时序差分" class="headerlink" title="n步时序差分"></a>n步时序差分</h3><p>利用两步得到的回报，使用自举来更新状态的价值</p><script type="math/tex;mode=display">\begin{array}{ll}
n=1(TD)&G_{1,t}=r_{t+1}+\gamma V(s_{t+1})\\
n=2&G_{2,t}=r_{t+1}+\gamma r_{t+2}+\gamma^2 V(s_{t+2})\\
&\vdots\\
n=n&G_{n,t}=r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots+\gamma^{n-1}r_{t+n}+\gamma^n V(s_{t+n})\\
&\vdots\\
n=\infty(MC)&G_{\infty,t}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-t-1}r_T
\end{array}</script><p><img src="/posts/1625593630/image-20240128194801709.png" alt="image-20240128194801709"></p><p>在得到时序差分目标之后，可以用增量式学习的方法来更新状态的价值</p><script type="math/tex;mode=display">V(s_t)\leftarrow V(s_t)+\alpha\left(G_{n,t}-V(s_t)\right)</script><h2 id="TD与MC对比"><a href="#TD与MC对比" class="headerlink" title="TD与MC对比"></a>TD与MC对比</h2><div class="table-container"><table><thead><tr><th></th><th>TD</th><th>MC</th></tr></thead><tbody><tr><td>是否支持在线</td><td>支持在线学习，也可离线学习</td><td>只能离线学习</td></tr><tr><td>数据序列是否完整</td><td>可以从不完整系列上学习，以采样的方式得到不完整的状态序列，估计某状态在该状态序列完整后可能得到的状态价值，并通过不断地采样持续更新价值以逼近真实的期望<br><img src="/posts/1625593630/image-20240128193119651.png" alt="image-20240128193119651"></td><td>只能从完整的序列上学习，在经历完整的状态序列后，再来更新状态的真实价值<br><img src="/posts/1625593630/image-20240128193132667.png" alt="image-20240128193132667"></td></tr><tr><td>序列是否有限</td><td>可以在连续的环境下(无终止)进行学习</td><td>只能在有终止的情况下学习</td></tr><tr><td>马尔科夫性质</td><td>利用了马尔科夫性质</td><td>适用于非马尔科夫环境</td></tr></tbody></table></div><h2 id="DP、MC、TD的自举和采样"><a href="#DP、MC、TD的自举和采样" class="headerlink" title="DP、MC、TD的自举和采样"></a>DP、MC、TD的自举和采样</h2><p>自举：更新时用了估计的方法</p><ul><li>MC没有使用</li><li>DP、TD都使用了自举</li></ul><p>采样：通过采样来获取期望</p><ul><li>MC方法是纯采样的方法</li><li>DP完全没用采样，直接用贝尔曼方程更新状态价值</li><li>TD使用了采样逼近真实回报</li></ul><p>DP算法在已知所有状态转换的情况下，可以把所有的状态都进行加和</p><script type="math/tex;mode=display">V(s_t)\leftarrow E_{\pi}[r_{t+1}+\gamma V(s_{t+1})\vert s_t=s]</script><p><img src="/posts/1625593630/image-20240128201557938.png" alt="image-20240128201557938"></p><p>MC在当前的状态下，采取一条支路，在回合完成后，更新这条支路上的所有状态价值</p><script type="math/tex;mode=display">V(s_t)\leftarrow V(s_t)+\alpha(G_t-V(s_t))</script><p><img src="/posts/1625593630/image-20240128201858712.png" alt="image-20240128201858712"></p><p>TD从当前状态开始，往前走了一步，关注的是局部更新</p><script type="math/tex;mode=display">TD(0):V(s_t)\leftarrow V(s_t)+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_t))</script><p><img src="/posts/1625593630/image-20240128202126009.png" alt="image-20240128202126009"></p><h3 id="四种计算价值函数的方法"><a href="#四种计算价值函数的方法" class="headerlink" title="四种计算价值函数的方法"></a>四种计算价值函数的方法</h3><p><img src="/posts/1625593630/image-20240128202256932.png" alt="image-20240128202256932"></p><h1 id="免模型控制"><a href="#免模型控制" class="headerlink" title="免模型控制"></a>免模型控制</h1><p>策略迭代分两个步骤</p><p><img src="/posts/1625593630/image-20240128212931067.png" alt="image-20240128212931067"></p><ol><li><p>根据给定的当前策略 $\pi$ 来估计价值函数</p><script type="math/tex;mode=display">V_{\pi}(s)=\sum\limits_{s'\in S}\pi(a\vert s)\cdot Q_{\pi}(s,a)\\
Q_{\pi}(s,a)=R(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'\vert s,a)V_{\pi}(s')</script></li><li><p>得到估计的价值函数后，通过贪心的方法来改进策略</p><script type="math/tex;mode=display">\pi'=greedy(V_{\pi})</script></li></ol><p>但由于在很多实际任务中，MDP的环境并不是可知的，即状态转移函数 $P(s’\vert s,a)$ 及奖励函数 $R(s,a)$ 是未知的</p><p>所以将策略迭代进行广义推广，得到 <strong>广义策略迭代</strong> (generalized policy iteration,GPI)，将蒙特卡洛方法或时序差分方法引入策略评估来计算当前策略的价值函数 $Q$ ，可以在不知道MDP环境的情况下，完成价值函数的优化并得到最佳策略，实现免模型控制</p><h2 id="基于MC方法的免模型控制"><a href="#基于MC方法的免模型控制" class="headerlink" title="基于MC方法的免模型控制"></a>基于MC方法的免模型控制</h2><h3 id="探索性开始"><a href="#探索性开始" class="headerlink" title="探索性开始"></a>探索性开始</h3><p>一个保证策略迭代收敛的假设是回合有探索性开始。假设每个回合都有一个探索性开始，能保证所有的状态和动作在无限步的执行后能被采样到，即一定会出现包含该起始状态的轨迹</p><h3 id="MC方法的策略评估"><a href="#MC方法的策略评估" class="headerlink" title="MC方法的策略评估"></a>MC方法的策略评估</h3><p>蒙特卡洛思想是通过很多轨迹的价值均值取估计Q函数，当轨迹数量足够多时，这个均值趋于收敛，则可以生成Q表。进而使用策略改进算法选取更好的策略</p><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化\\
&\quad \pi(a\vert s)\in A(s)(随机初始化)，对于所有的 s\in \mathcal{S}\\
&\quad Q(s,a)\in R(s,a)(随机初始化)，对于所有的 s\in \mathcal{S},a\in \mathcal{A}\\
&\quad R(s,a)\leftarrow 空值，对于所有的 s\in \mathcal{S},a\in \mathcal{A}\\
&遍历每个回合:\\
&\quad 随机选择 s_0\in \mathcal{S},a_0\in A(s_0)，并且保证所有的数据对的概率大于0(探索性开始)\\
&\quad 从s_0,a_0生成回合，产生历史H_\pi^{(k)}:s_0,a_0,r_1,s_1,\cdots,s_{T-1},a_{T-1},r_{T}\\
&\quad G\leftarrow 0\\
&\quad 对于一个回合的每一步进行循环 t=T-1,T-2,\cdots,0:\\
&\qquad G\leftarrow \gamma G+t_{t+1}\\
&\qquad 如果(s_t,a_t)出现在历史H_\pi^{(k)}中:\\
&\qquad\quad 将G追加到R(s_t,a_t)\\
&\qquad\quad Q(s_t,a_t)\leftarrow\overline{R(s_t,a_t)}\\
&\qquad\quad \pi(a\vert s_t)\leftarrow\mathop{\mathrm{argmax}}\limits_{a}Q(s_t)\\
\hline\\
&基于探索性开始与贪心的蒙特卡洛方法
\end{array}</script><h3 id="MC方法的策略改进"><a href="#MC方法的策略改进" class="headerlink" title="MC方法的策略改进"></a>MC方法的策略改进</h3><p>为确保MC方法有足够的探索，使用 $\varepsilon-贪心(\varepsilon-greedy)$ 探索：有 $1-\varepsilon$ 的概率按照 $Q$ 函数决定动作，$\varepsilon$ 的概率采取随机动作。</p><p>体现了 <strong>探索-利用</strong> 的权衡：开始时，$\varepsilon$ 比较大，可以对不同动作进行充分探索；$\varepsilon$ 随着时间递减，确保可以获取足够多的奖励</p><script type="math/tex;mode=display">\begin{array}{ll}
\hline
& 初始化Q(s,a)=0,N(s,a)=0,\varepsilon=1,k=1\\
&\pi^{(k)}=\varepsilon-贪心(Q)\\
&进行循环:\\
&\quad进行第k个回合的采样(H_{\pi^{(k)}}^{(k)}=\{s_0,a_0,r_1,s_1,\cdots,s_{T-1},a_{T-1},r_{T}\})\\
&\quad 对于每个数据对 (s_t,a_t): \\
&\qquad 计算当前回报G_t(s_t,a_t)=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-t-1}r_{T}\\
&\qquad N(s_t,a_t)\leftarrow N(s_t,a_t)+1;\\
&\qquad Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\frac{1}{N(s_t,a_t)}(G_t-Q(s_t,a_t))\\
&\quad k\leftarrow k+1,\varepsilon\leftarrow \frac{1}{k}\\
&\pi^{(k)}=\varepsilon-贪心(Q)\\
&停止循环\\
\hline\\
&基于\varepsilon-贪心的增量式蒙特卡洛方法
\end{array}</script><h4 id="策略改进定理"><a href="#策略改进定理" class="headerlink" title="策略改进定理"></a>策略改进定理</h4><blockquote><p>对于任何 $\varepsilon-贪心$ 策略 $\pi$ ，关于 $Q_{\pi}$ 的 $\varepsilon-贪心$ 策略 $\pi’$ 都是一个正向改进，即 $V_{\pi}(s)\le V_{\pi’}(s)$</p></blockquote><p><img src="/posts/1625593630/image-20240129104505010.png" alt="image-20240129104505010"></p><h2 id="基于TD方法的免模型控制"><a href="#基于TD方法的免模型控制" class="headerlink" title="基于TD方法的免模型控制"></a>基于TD方法的免模型控制</h2><p>与蒙特卡洛方法相比，时序差分方法有几个优势：</p><ul><li>低方差：受数据集影响小</li><li>能在线学习</li><li>能从不完整的序列中学习</li></ul><p>所以采取时序差分方法更新Q表，再用 $\varepsilon-贪心$ 探索改进</p><ul><li>Sarsa</li><li>Q-learning</li></ul><h3 id="Sarsa：同策略TD控制"><a href="#Sarsa：同策略TD控制" class="headerlink" title="Sarsa：同策略TD控制"></a>Sarsa：同策略TD控制</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>将原本利用TD方法更新 $V$ 的过程变为更新 $Q$</p><script type="math/tex;mode=display">Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]</script><p>Sarsa直接估计 $Q$ 表格，得到 $Q$ 表之后，就可以更新策略</p><p><img src="/posts/1625593630/image-20240130094737368.png" alt="image-20240130094737368"></p><p>我们想要计算的是 $Q(s_t,a_t)$ 值，用以代替当前状态可得的未来奖励 $G_t$ ，所以将其作为想要逼近的目标值</p><p>$Q(s_{t+1},a_{t+1})$ 用以代替 $G_{t+1}$ ，故 $r_{t+1}+\gamma Q(s_{t+1},a_{t+1})$ 是时序差分目标</p><p>$r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)$ 为时序差分误差</p><p>用软更新(每次更新一点)的方式逼近真实值</p><p>由于该算法需要知道 当前状态 $s_t$ ，当前动作 $a_{t}$ ，当前动作奖励 $r_{t}$ ，下一步的状态 $s_{t+1}$ ，下一步的动作 $a_{t+1}$ ，即需要知道 $(s_t,a_{t},r_{t},s_{t+1},a_{t+1})$ 这一决策历史，因此命名为 Sarsa 算法</p><h4 id="Sarsa算法"><a href="#Sarsa算法" class="headerlink" title="Sarsa算法"></a>Sarsa算法</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&超参数:步长大小 \alpha\in(0,1],很小的\varepsilon>0\\
&初始化:\\
&\quad Q(s,a)随机初始化,其中 s\in \mathcal{S},a\in \mathcal{A}\\
&\quad Q(终点,\cdot)=0\\
&遍历每个回合:\\
&\quad 初始化 s\\
&\quad 使用从Q中衍生出的策略(如\varepsilon-贪心)根据状态s选择a\\
&\quad 遍历一个回合中的每一步:\\
&\qquad 执行动作a,获取观测 r,s'\\
&\qquad 使用从Q中衍生出的策略(如\varepsilon-贪心)根据状态s'选择a'\\
&\qquad Q(s,a)\leftarrow Q(s,a)+\alpha[\underbrace{\underbrace{r+\gamma Q(s',a')}_{目标值}-\underbrace{Q(s,a)}_{当前值}}_{软更新}]\\
&\qquad s\leftarrow s',a\leftarrow a'\\
&\qquad 直至s到达终点\\
\hline\\
&Sarsa算法
\end{array}</script><h4 id="n步Sarsa"><a href="#n步Sarsa" class="headerlink" title="n步Sarsa"></a>n步Sarsa</h4><script type="math/tex;mode=display">\begin{array}{ll}
n=1(Sarsa)&Q_{1,t}=r_{t+1}+\gamma Q(s_{t+1},a_{t+1})\\
n=2&Q_{2,t}=r_{t+1}+\gamma r_{t+2}+\gamma^2 Q(s_{t+2},a_{t+1})\\
&\vdots\\
n=n&Q_{n,t}=r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots+\gamma^{n-1}r_{t+n}+\gamma^n Q(s_{t+n},a_{t+1})\\
&\vdots\\
n=\infty(MC)&Q_{\infty,t}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-t-1}r_T
\end{array}</script><p>如果给 $Q_{n,t}$ 加上资格衰减参数 $\lambda$ 并进行求和，即可得到 $Sarsa(\lambda)$ 的Q回报</p><script type="math/tex;mode=display">Q^{\lambda}_{t}=(1-\lambda)\sum\limits_{n=1}^{\infty}\lambda^{n-1}Q_{n,t}</script><p>因此 $n$ 步 $Sarsa(\lambda)$ 的更新策略为</p><script type="math/tex;mode=display">Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha(Q_t^{\lambda}-Q(s_t,a_t))</script><h3 id="Q学习：以策略TD控制"><a href="#Q学习：以策略TD控制" class="headerlink" title="Q学习：以策略TD控制"></a>Q学习：以策略TD控制</h3><h4 id="同策略与异策略概念"><a href="#同策略与异策略概念" class="headerlink" title="同策略与异策略概念"></a>同策略与异策略概念</h4><p><strong>Sarsa</strong> 是一种同策略(on-policy)算法，它优化的是它实际执行的策略，直接用下一步要执行的动作去优化Q表。在整个学习过程中，只存在一种策略，进行动作选取与策略改进。 所以整体上，Sarsa是一种偏保守的策略，从动作选择到策略改进都会尽量避免回合失败且尽可能远离失败状态</p><p><img src="/posts/1625593630/image-20240130114131007.png" alt="image-20240130114131007"></p><p><strong>Q学习</strong> 是一种异策略(off-policy)算法，异策略算法在学习过程中，有两种不同策略：目标策略(target policy)和行为策略(behavior policy)。</p><ul><li>目标策略：需要学习的策略，用 $\pi$ 表示，根据经验学习最优的策略，不需要与环境交互</li><li>行为策略：探索环境的策略，用 $\mu$ 表示，可以大胆地探索所有可能的轨迹，只会选取价值最大的动作执行，不管是否会造成回合失败</li></ul><p><img src="/posts/1625593630/image-20240130114117555.png" alt="image-20240130114117555"></p><h5 id="异策略好处"><a href="#异策略好处" class="headerlink" title="异策略好处"></a>异策略好处</h5><ul><li>可以 <strong>利用</strong> 探索策略学到的最佳策略，学习效率高</li><li>可以学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹</li><li>可以重用就的策略产生的轨迹，因为 <strong>探索</strong> 需要很多计算资源，这样可以节省资源</li></ul><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><p>目标策略 $\pi$ 直接在Q表上使用贪心策略，即</p><script type="math/tex;mode=display">\pi(a\vert s)=\mathop{\mathrm{argmax}}_{a\in \mathcal{A}}Q(s,a)</script><p>行为策略 $\mu$ 可以是一个随机策略，但一般采取 $\varepsilon-贪心$ 策略</p><p>Q学习的下一个动作都是通过 $argmax$ 操作选出来的，所以对于时序差分目标</p><script type="math/tex;mode=display">\begin{align}
r_{t+1}+\gamma Q(s',a')&=r_{t+1}+\gamma Q(s',\mathop{\mathrm{argmax}}_{a'} Q(s',a'))\\
&=r_{t+1}+\gamma \max_{a'} Q(s',a')
\end{align}</script><p>故，Q学习的Q表更新式为</p><script type="math/tex;mode=display">Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\left[r_{t+1}+\gamma \max_{a'} Q(s',a')-Q(s_t,a_t)\right]</script><p>Sarsa算法更新 $Q$ 表的 $a’$ 一定是下一步骤实际要执行的动作，这个动作可以是随机动作、可以是最大化 Q 表选取出来的动作、也可以是 $\varepsilon-贪心$ 选取的策略</p><p>Q学习更新 $Q$ 表的 $a’$ 不一定是下一个步骤会实际执行的动作。因为默认动作不是根据行为策略选取出来的。</p><blockquote><p>Q 学习算法被提出的时间更早，Sarsa 算法是Q 学习算法的改进</p></blockquote><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&超参数:步长大小 \alpha\in(0,1],很小的\varepsilon>0\\
&初始化:\\
&\quad Q(s,a)随机初始化,其中 s\in \mathcal{S},a\in \mathcal{A}\\
&\quad Q(终点,\cdot)=0\\
&遍历每个回合:\\
&\quad 初始化 s\\
&\quad 遍历一个回合中的每一步:\\
&\qquad 使用从Q中衍生出的策略(如\varepsilon-贪心)根据状态s选择a\\
&\qquad 执行动作a,获取观测 r,s'\\
&\qquad Q(s,a)\leftarrow Q(s,a)+\alpha[\underbrace{\underbrace{r+\gamma \max\limits_{a'}Q(s',a')}_{目标值}-\underbrace{Q(s,a)}_{当前值}}_{软更新}]\\
&\qquad s\leftarrow s'\\
&\qquad 直至s到达终点\\
\hline\\
&Q学习
\end{array}</script><h3 id="Q学习与Sarsa算法对比"><a href="#Q学习与Sarsa算法对比" class="headerlink" title="Q学习与Sarsa算法对比"></a>Q学习与Sarsa算法对比</h3><p>Sarsa算法与Q学习唯一区别就是 Q学习不需要知道 $a’$ 就能更新 $Q$ 表，只需要知道 $(s_t,a_t,r_t,s_{t+1})$</p><p><img src="/posts/1625593630/3.19.png" alt="img"></p><h4 id="同策略与异策略对比"><a href="#同策略与异策略对比" class="headerlink" title="同策略与异策略对比"></a>同策略与异策略对比</h4><p>Sarsa算法是典型的同策略算法，整个训练过程只有一个策略 $\pi$ ，不仅使用策略 $\pi$ 学习，还使用策略 $\pi$ 与环境交互产生经验。若采用 $\varepsilon-贪心$ 进行策略改进，为了兼顾探索与利用，在训练时的探索能一直保持在安全区域内。由于 $\varepsilon$ 会不断变小，在训练过程中策略不稳定</p><p>Q学习是典型异策略算法，行为策略可以采用 $\varepsilon-贪心$ 算法；目标策略采用贪心算法，直接根据行为策略采集到的数据采用最佳策略。可以大胆地利用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。</p><p>Q学习是非常激进的方法，它希望每一步都获得最大收益；Sarsa算法较为保守，会选择一条相对安全的迭代路线</p><p><img src="/posts/1625593630/image-20240126105101874-17062570388546.png" alt="image-20240126105101874"></p><h3 id="悬崖寻路实验"><a href="#悬崖寻路实验" class="headerlink" title="悬崖寻路实验"></a>悬崖寻路实验</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>智能体由48个状态，动作空间有4个动作(上下左右)，每移动一步得到-1的奖励。起点和终点之间是一段悬崖，编号为37~46。有以下四点限制：</p><ol><li>智能体不能移出网格，如果智能体选择的下一动作将移出网格，则不执行动作，但仍会得到 -1 的奖励</li><li>若智能体“掉入悬崖”，会立即回到起点位置，并得到-100的奖励</li><li>当智能体移动到终点，该回合结束，该回合总奖励为各步奖励之和</li></ol><p><img src="/posts/1625593630/image-20240130220344858.png" alt="image-20240130220344858"></p><p>目标：以最小的移动步数到终点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym <span class="comment"># 导入gym模块</span></span><br><span class="line"><span class="keyword">from</span> envs.gridworld_env <span class="keyword">import</span> CliffWalkingWapper <span class="comment"># 导入自定义装饰器</span></span><br><span class="line">env = gym.make(<span class="string">&#x27;CliffWalking-v0&#x27;</span>) <span class="comment"># 定义环境</span></span><br><span class="line">env = CliffWalkingWapper(env) <span class="comment"># 装饰环境</span></span><br><span class="line"></span><br><span class="line">n_states = env.observation_space.n <span class="comment"># 状态数</span></span><br><span class="line">n_actions = env.action_space.n <span class="comment"># 动作数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;状态数：<span class="subst">&#123;n_states&#125;</span>，动作数：<span class="subst">&#123;n_actions&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">state = env.reset() <span class="comment"># 随机初始化初始状态</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;初始状态：<span class="subst">&#123;state&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h4><ol><li>初始化环境和智能体</li><li>对于每个回合<ol><li>智能体选择动作</li><li>环境接收动作并反馈下一状态和奖励</li><li>智能体进行策略更新(学习)</li></ol></li><li>多个回合收敛后，保存模型并进行后续的分析、画图等</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">&#x27;CliffWalking-v0&#x27;</span>) <span class="comment"># 定义环境</span></span><br><span class="line">env = CliffWalkingWapper(env) <span class="comment"># 装饰环境</span></span><br><span class="line">env.seed(<span class="number">1</span>) <span class="comment"># 设置随机种子</span></span><br><span class="line"></span><br><span class="line">n_states = env.observation_space.n <span class="comment"># 状态数</span></span><br><span class="line">n_actions = env.action_space.n <span class="comment"># 动作数</span></span><br><span class="line">agent = QLearning(n_states,n_actions,cfg) <span class="comment"># cfg存储超参数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_ep <span class="keyword">in</span> <span class="built_in">range</span>(cfg.train_eps): <span class="comment"># cfg.train_eps表示最大的训练回合数</span></span><br><span class="line">	ep_reward = <span class="number">0</span> <span class="comment"># 记录每个回合的奖励</span></span><br><span class="line">	state = env.reset() <span class="comment"># 重置环境</span></span><br><span class="line">	</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">		action = agent.choose_action(state) <span class="comment"># 算法选择一个动作</span></span><br><span class="line">		next_state, reward, done, _ = env.step(action) <span class="comment"># 环境根据动作反馈奖励和下一个状态</span></span><br><span class="line">		agent.update(state, action, reward, next_state, done) <span class="comment"># 算法更新</span></span><br><span class="line">		state = next_state <span class="comment"># 更新状态</span></span><br><span class="line">		ep_reward += reward</span><br><span class="line">        </span><br><span class="line">		<span class="keyword">if</span> done: <span class="comment"># 终止状态</span></span><br><span class="line">			<span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>通常需要记录与分析奖励变化情况，所以会在框架基础上增加一些变量以记录每回合奖励。此外，由于强化学习训练过程中得到的奖励可能产生振荡，所以使用一个滑动平均的量来反映奖励变化的趋势</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">&#x27;CliffWalking-v0&#x27;</span>) <span class="comment"># 定义环境</span></span><br><span class="line">env = CliffWalkingWapper(env) <span class="comment"># 装饰环境</span></span><br><span class="line">env.seed(<span class="number">1</span>) <span class="comment"># 设置随机种子</span></span><br><span class="line"></span><br><span class="line">n_states = env.observation_space.n <span class="comment"># 状态数</span></span><br><span class="line">n_actions = env.action_space.n <span class="comment"># 动作数</span></span><br><span class="line">agent = QLearning(n_states,n_actions,cfg) <span class="comment"># cfg存储超参数</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># learning_rate=cfg.policy_lr,</span></span><br><span class="line"><span class="comment"># gamma=cfg.gamma,</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line">rewards = []</span><br><span class="line">ma_rewards = [] <span class="comment"># 滑动平均奖励</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_ep <span class="keyword">in</span> <span class="built_in">range</span>(cfg.train_eps):</span><br><span class="line">	ep_reward = <span class="number">0</span> <span class="comment"># 记录每个回合的奖励</span></span><br><span class="line">	state = env.reset() <span class="comment"># 重置环境, 重新开始（开始一个新的回合）</span></span><br><span class="line">	</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">		action = agent.choose_action(state) <span class="comment"># 根据算法选择一个动作</span></span><br><span class="line">		next_state, reward, done, _ = env.step(action) <span class="comment"># 与环境进行一次动作交互</span></span><br><span class="line">		agent.update(state, action, reward, next_state, done) <span class="comment"># Q学习算法更新</span></span><br><span class="line">		state = next_state <span class="comment"># 存储上一个观察值</span></span><br><span class="line">		ep_reward += reward</span><br><span class="line">		</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">	rewards.append(ep_reward)</span><br><span class="line">	<span class="keyword">if</span> ma_rewards:</span><br><span class="line">		ma_rewards.append(ma_rewards[-<span class="number">1</span>]*<span class="number">0.9</span>+ep_reward*<span class="number">0.1</span>)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		ma_rewards.append(ep_reward)</span><br></pre></td></tr></table></figure><h4 id="Q学习"><a href="#Q学习" class="headerlink" title="Q学习"></a>Q学习</h4><p>智能体在整个训练中只做两件事，一是动作选择，二是更新策略</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动作更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, state</span>):</span><br><span class="line">	self.sample_count += <span class="number">1</span></span><br><span class="line">	self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-<span class="number">1.</span> * self.sample_count / self.epsilon_decay) <span class="comment"># epsilon是会递减的，这里选择指数递减</span></span><br><span class="line">	<span class="comment"># 带有探索的贪心策略</span></span><br><span class="line">	<span class="keyword">if</span> np.random.uniform(<span class="number">0</span>, <span class="number">1</span>) &gt; self.epsilon:</span><br><span class="line">		action = np.argmax(self.Q_table[<span class="built_in">str</span>(state)]) <span class="comment"># 选择Q(s,a)最大值对应的动作</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		action = np.random.choice(self.action_dim) <span class="comment"># 随机选择动作</span></span><br><span class="line">	<span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 策略改进</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, state, action, reward, next_state, done</span>):</span><br><span class="line">	Q_predict = self.Q_table[<span class="built_in">str</span>(state)][action]</span><br><span class="line">	<span class="keyword">if</span> done: <span class="comment"># 终止状态</span></span><br><span class="line">		Q_target = reward</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		Q_target = reward + self.gamma * np.<span class="built_in">max</span>(self.Q_table[<span class="built_in">str</span>(next_state)])</span><br><span class="line">	self.Q_table[<span class="built_in">str</span>(state)][action] += self.lr * (Q_target - Q_predict)</span><br></pre></td></tr></table></figure><h1 id="状态近似"><a href="#状态近似" class="headerlink" title="状态近似"></a>状态近似</h1></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/1625593630/" title="强化学习——蘑菇书-表格型方法">https://amostian.github.io/posts/1625593630/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 强化学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/2520454134/" rel="prev" title="强化学习——蘑菇书"><i class="fa fa-chevron-left"></i> 强化学习——蘑菇书</a></div><div class="post-nav-item"><a href="/posts/1573065823/" rel="next" title="3.代数系统">3.代数系统 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Q%E8%A1%A8"><span class="nav-text">Q表</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BD%95%E7%94%A8%E6%9C%AA%E6%9D%A5%E5%A5%96%E5%8A%B1%E8%AF%84%E4%BC%B0%E5%BD%93%E5%89%8D%E5%8A%A8%E4%BD%9C%E5%A5%BD%E5%9D%8F"><span class="nav-text">为何用未来奖励评估当前动作好坏</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EDP%E6%96%B9%E6%B3%95%E7%9A%84%E6%9C%89%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%8E%A7%E5%88%B6"><span class="nav-text">基于DP方法的有模型预测与控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-text">策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">价值迭代</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="nav-text">免模型预测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="nav-text">蒙特卡洛方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF"><span class="nav-text">整体思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="nav-text">具体步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B"><span class="nav-text">增量式蒙特卡洛</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E5%9D%87%E5%80%BC"><span class="nav-text">增量式均值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="nav-text">增量式蒙特卡洛方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DP%E4%B8%8EMC%E6%96%B9%E6%B3%95%E7%9A%84%E6%80%BB%E7%BB%93%E4%B8%8E%E5%AF%B9%E6%AF%94"><span class="nav-text">DP与MC方法的总结与对比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94"><span class="nav-text">对比</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95"><span class="nav-text">时序差分方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E6%AD%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="nav-text">一步时序差分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#n%E6%AD%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="nav-text">n步时序差分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TD%E4%B8%8EMC%E5%AF%B9%E6%AF%94"><span class="nav-text">TD与MC对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DP%E3%80%81MC%E3%80%81TD%E7%9A%84%E8%87%AA%E4%B8%BE%E5%92%8C%E9%87%87%E6%A0%B7"><span class="nav-text">DP、MC、TD的自举和采样</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E7%A7%8D%E8%AE%A1%E7%AE%97%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">四种计算价值函数的方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6"><span class="nav-text">免模型控制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EMC%E6%96%B9%E6%B3%95%E7%9A%84%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6"><span class="nav-text">基于MC方法的免模型控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B"><span class="nav-text">探索性开始</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MC%E6%96%B9%E6%B3%95%E7%9A%84%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-text">MC方法的策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MC%E6%96%B9%E6%B3%95%E7%9A%84%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B"><span class="nav-text">MC方法的策略改进</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B%E5%AE%9A%E7%90%86"><span class="nav-text">策略改进定理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8ETD%E6%96%B9%E6%B3%95%E7%9A%84%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6"><span class="nav-text">基于TD方法的免模型控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa%EF%BC%9A%E5%90%8C%E7%AD%96%E7%95%A5TD%E6%8E%A7%E5%88%B6"><span class="nav-text">Sarsa：同策略TD控制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sarsa%E7%AE%97%E6%B3%95"><span class="nav-text">Sarsa算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#n%E6%AD%A5Sarsa"><span class="nav-text">n步Sarsa</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BB%A5%E7%AD%96%E7%95%A5TD%E6%8E%A7%E5%88%B6"><span class="nav-text">Q学习：以策略TD控制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E7%AD%96%E7%95%A5%E4%B8%8E%E5%BC%82%E7%AD%96%E7%95%A5%E6%A6%82%E5%BF%B5"><span class="nav-text">同策略与异策略概念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%82%E7%AD%96%E7%95%A5%E5%A5%BD%E5%A4%84"><span class="nav-text">异策略好处</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86-1"><span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q%E5%AD%A6%E4%B9%A0%E4%B8%8ESarsa%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="nav-text">Q学习与Sarsa算法对比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E7%AD%96%E7%95%A5%E4%B8%8E%E5%BC%82%E7%AD%96%E7%95%A5%E5%AF%B9%E6%AF%94"><span class="nav-text">同策略与异策略对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%82%AC%E5%B4%96%E5%AF%BB%E8%B7%AF%E5%AE%9E%E9%AA%8C"><span class="nav-text">悬崖寻路实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="nav-text">问题描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6"><span class="nav-text">框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Q%E5%AD%A6%E4%B9%A0"><span class="nav-text">Q学习</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E8%BF%91%E4%BC%BC"><span class="nav-text">状态近似</span></a></li></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">353</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">58</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">74</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">760.6k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">31:50</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>