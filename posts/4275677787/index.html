<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="6-深度强化学习"><meta property="og:url" content="https://amostian.github.io/posts/4275677787/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240511161738085.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240511162004879.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240304100543544.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240305170620192.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240305170633924.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240512000320865.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240512000949548.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240512001305477.png"><meta property="og:image" content="https://amostian.github.io/posts/4275677787/image-20240512001449151.png"><meta property="article:published_time" content="2024-05-10T11:19:07.000Z"><meta property="article:modified_time" content="2024-10-04T11:29:41.199Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="强化学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/4275677787/image-20240511161738085.png"><link rel="canonical" href="https://amostian.github.io/posts/4275677787/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>6-深度强化学习 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">65</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">82</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">220</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/4275677787/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">6-深度强化学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-05-10 19:19:07" itemprop="dateCreated datePublished" datetime="2024-05-10T19:19:07+08:00">2024-05-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-10-04 19:29:41" itemprop="dateModified" datetime="2024-10-04T19:29:41+08:00">2024-10-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">3.7k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><p>参数化的价值函数 $\hat{V}(s,\phi),\hat{Q}(s,\phi)$ 和策略函数 $\pi_{\theta}(a\vert s)$，使得环境的连续状态和连续决策得以表示，是强化学习落地的第一步</p><ul><li><p>价值函数通过最小化价值函数与TD目标之间的平方损失学习</p><script type="math/tex;mode=display">J(w)=E\left[\frac{1}{2}\left(V_{\pi}(s)-\hat{V}_{\pi}(S,w)\right)^2\right]\\
w_{t+1}=w_t+\alpha_t\left(\overbrace{\underbrace{r_{t+1}+\gamma \hat{V}(s_{t+1},w_t)}_{\mbox{TD target}}-\hat{V}_{\pi}(s_t,w_t)}^{\mbox{TD error}}\right)\bigtriangledown_w\hat{V}_{\pi}(s_t,w)</script></li><li><p>策略函数通过最大化策略度量指标的期望得以学习</p><script type="math/tex;mode=display">\theta_{t+1}\xlongequal{随机梯度上升法}\theta_{t}+\alpha\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot Q_{\pi}(s_t,a_t)</script></li></ul><h2 id="深度学习与强化学习的结合"><a href="#深度学习与强化学习的结合" class="headerlink" title="深度学习与强化学习的结合"></a>深度学习与强化学习的结合</h2><h3 id="传统计算机视觉与深度计算机视觉"><a href="#传统计算机视觉与深度计算机视觉" class="headerlink" title="传统计算机视觉与深度计算机视觉"></a>传统计算机视觉与深度计算机视觉</h3><p><img src="/posts/4275677787/image-20240511161738085.png" alt="image-20240511161738085"></p><ol><li>给定一张图片，先要提取它的特征，使用一些设计好的特征，如：方向梯度直方图(histogram of oriented gradient, HOG)、可变形的组件模型(deformable part model, DPM)</li><li>提取这些特征，再单独训练一个分类器，这个分类器可以是支持向量机或Boosting，就可以对图像进行分类</li></ol><p>2012年，AlexNet将特征提取与分类过程合并。通过训练一个神经网络实现端到端的训练，这个神经网络既可以做特征提取，又可以做分类。</p><h3 id="传统强化学习与深度强化学习"><a href="#传统强化学习与深度强化学习" class="headerlink" title="传统强化学习与深度强化学习"></a>传统强化学习与深度强化学习</h3><p><strong>标准强化学习</strong></p><ol><li>设计很多特征，这些特征可以描述环境当前的整个状态</li><li>得到特征后，通过训练一个分类网络或分别训练一个价值估计函数来采取动作</li><li>价值函数和策略函数的基函数选择也是特征抽取的一部分</li></ol><p><strong>深度强化学习</strong></p><p>智能体引入神经网络，可以用神经网络拟合价值函数或策略网络，省去特征工程的过程，训练的过程变为一个端到端的训练。</p><p><img src="/posts/4275677787/image-20240511162004879.png" alt="image-20240511162004879"></p><p>采用策略网络每个动作进行采样，则其输入为环境当前状态，输出为所有动作的可能性，直到当前动作序列结束，依次对每个动作进行惩罚。</p><ul><li><p>近年来深度强化学习被广泛应用</p><p>我们有了更多的算力资源，有了更多的GPU，可以更快地做更多的试错尝试。通过不同的尝试，智能体在环境中获得了很多信息，可以在环境中取得很大的奖励</p><p>有了深度强化学习这种端到端的训练方式，可以把特征提取、价值估计以及决策部分一起优化，得到一个更强的决策网络</p></li></ul><p>深度强化学习的关键变化</p><ul><li>相当高维的参数空间，容易过拟合</li><li>难以稳定地训练</li><li>需要大量的数据</li><li>需要高性能计算</li><li>CPU（收集数据）和GPU（训练神经网络）间的平衡</li></ul><p><strong>两方面研究</strong></p><ul><li>在深度学习模型基础上，强化学习如何更加有效的工作，并避免深度学习模型带来的一系列问题，如：Q值估计过高问题</li><li>在强化学习场景下，深度学习如何有效地学习到有用的模式，如设计Dueling DQN的价值网络与动作优势网络</li></ul><h3 id="DRL分类"><a href="#DRL分类" class="headerlink" title="DRL分类"></a>DRL分类</h3><p>基于价值的方法：DQN及其扩展</p><p>基于策略的方法：使用神经网络的策略梯度，如：自然策略梯度，信任区域策略优化（TPRO），近端策略优化（PPO），A3C</p><p>基于确定性策略的方法：确定性策略DDPG</p><h2 id="基于价值网络的方法"><a href="#基于价值网络的方法" class="headerlink" title="基于价值网络的方法"></a>基于价值网络的方法</h2><p>不直接更新策略，学习价值函数后，更新策略</p><h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><blockquote><p>最早和最成功地将神经网络引入RL，且引入的技巧被后续更多的DRL采用</p><p>通常只能处理动作离散的情况，因为存在 $\max Q$</p></blockquote><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>对于最优的动作满足贝尔曼最优方程</p><script type="math/tex;mode=display">\overline{Q}(s,a)=E\left[R_{t+1}+\gamma \max\limits_{a\in \mathcal{A}(S_{t+1})}Q(S_{t+1},a)\bigg\vert S_t=s,A_t=a\right],\forall s,a</script><p>我们希望最优动作价值 $\overline{Q}(s,a)$ 与其价值近似函数 $\hat{Q}(S,A,w)$ 在 $(S_t,A_t)=(s,a)$ 时在期望上误差为0</p><p>DQN目标是最小化目标函数（损失函数）——贝尔曼最优误差</p><script type="math/tex;mode=display">J(w)=E\left[\left(R+\gamma \max\limits_{a\in \mathcal{A}(S')}\hat{Q}\left(S',a,w\right)-\hat{Q}\left(S,A,w\right)\right)^2\right]</script><p>使用梯度下降法求解最小化损失函数，但由于涉及两处对 $w$ 求梯度，可以先固定一处 $w$ ，设 $y$ 中的 $w$ 为一个常数</p><script type="math/tex;mode=display">y=R+\gamma \max\limits_{a\in \mathcal{A}(S')}\hat{Q}\left(S',a,w\right)</script><p>对 $J(w)$ 求解关于 $w$ 的梯度 $\bigtriangledown_w J(w)$ ，只是关于 $\hat{Q}\left(S,A,w\right)$ 中 $w$ 的函数</p><p>为此，引入两个网络</p><ul><li>一个是 main network $\hat{Q}\left(S,A,w\right)$</li><li>另一个是 target network 为 $y$ 中的 $\hat{Q}\left(S,A,w_T\right)$</li></ul><p>main network中的 $w$ 会随着回合的每一步进行更新，target network 中的参数 $w_T$ 会先当做常数，每隔一段时间将 main network 中的参数值复制 $w_T=w$ 再基于更新后的 $w_T$ 作为常数，更新 $w$ ，最后二者都会收敛到最优值 。即目标函数变为</p><script type="math/tex;mode=display">J(w)=E\left[\left(R+\gamma \max\limits_{a\in \mathcal{A}(S')}\hat{Q}\left(S',a,w_T\right)-\hat{Q}\left(S,A,w\right)\right)^2\right]</script><p>$J(w)$ 的梯度下降变为</p><script type="math/tex;mode=display">\bigtriangledown_wJ(w)=E\left[\left(R+\gamma \max\limits_{a\in\mathcal{A}(S')}\hat{Q}\left(S',a,w_T\right)-\hat{Q}\left(S,A,w\right)\right)\bigtriangledown_w\hat{Q}\left(S,A,w\right)\right]</script><h4 id="DQN的两个技巧"><a href="#DQN的两个技巧" class="headerlink" title="DQN的两个技巧"></a>DQN的两个技巧</h4><p><strong>经验回放有助于防止过拟合，限制目标 Q 网络的更新速率已被证明可以增加训练过程的稳定性和效率</strong></p><p>带有价值近似函数的 Q-learning 算法是带有梯度的，连续采样到的经验 $<s,a,s',r'>$ 不满足独立分布且 $\hat{Q}\left(S’,a,w\right)$ 更新频繁，不适合用于训练</p><p>神经网络会批量训练一些黑盒模型，然后更高效地训练神经网络</p><h5 id="引入两个网络"><a href="#引入两个网络" class="headerlink" title="引入两个网络"></a>引入两个网络</h5><p>main network 与 target network</p><p>因为对目标函数中的两个位置求解参数在数学上比较复杂，所以采取异步更新</p><p>初始：$w=w_T$</p><p>收集数据：使用 $\epsilon-贪心$ 策略进行探索，将得到的状态-动作经验 $<s,a,s',r'>$ 放入经验池</p><p>每轮迭代，从经验回放池 $\mathcal{B}=\{(s,a,r’,s’)\}$ 中取出使用 $k$ 条经验进行训练</p><p>训练：</p><ol><li><p>输入：$s,a$ ；target network输出：$y(w_T)=r’+\gamma \max\limits_{a\in \mathcal{A}(s’)}\hat{Q}(s’,a,w_T)$</p></li><li><p>在少批量经验集上 $\{(s,a,y(w_T))\}$ 上最小化TD误差 / 损失函数 $\left(y(w_T)-\hat{Q}(s,a,w)\right)^2$ ，更新 $w$ 相当于训练 main network</p><script type="math/tex;mode=display">\bigtriangledown_wJ(w)=E\left[\left(y(w_T)-\hat{Q}\left(S,A,w\right)\right)\bigtriangledown_w\hat{Q}\left(S,A,w\right)\right]</script></li><li><p>$w$ 的每 $C$ 次迭代（更新Q网络），更新一次目标网络 $w_T=w$ ，重复执行 $1,2$</p></li></ol><h5 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h5><p>当收集到经验数据后，并不是按照采集顺序使用这些经验，将这些经验存储在成为经验缓存池的集合中 $\mathcal{B}=\{(s,a,r’,s’)\}$ ，每次训练神经网络时，从经验缓存池中取出少批量随机经验</p><h6 id="经验回放中的分布"><a href="#经验回放中的分布" class="headerlink" title="经验回放中的分布"></a>经验回放中的分布</h6><p>经验回放的必要性与均匀地经验回放</p><p>$(s,a)$ 给定之后，$(r’,s’)$ 要服从系统模型的动态特性 $P(r’,s’\vert s,a)$ ，若知道哪些访问会接近目标状态这个先验知识前提下，这些访问对应该被多采样。若没有这个先验知识，则对于所有访问都要一视同仁，保证探索的充分性，即这些经验应该 <strong>按均匀分布组成少批量训练经验集</strong></p><p>深度Q函数容易过拟合到局部特征，所以若数据是按照时间顺序抽取，则一定会过拟合到时序的最后的经验，为打破经验之间的时序关系，采用经验回放技巧，从经验缓存池中均匀地取出经验</p><p><strong>表格型Q-learning中无经验回放</strong></p><p>表格型Q-learning中并未涉及到 $(S,A)$ 的分布</p><p>因为DQN是一类价值函数近似方法，数学上解决优化问题，需要有一个标量的目标函数去优化，价值函数近似中的标量目标函数是一个期望，会涉及到 $(S,A)$ 的分布。</p><p>在基于表格的方法中，如 <strong>Q-learning</strong> 中求解贝尔曼最优公式，对于每个 $(S,A)$ 都有一组等式去求解，进而得出最优的动作价值</p><p><strong>经验回放技术也可应用与表格型方法</strong></p><p>对于确定性策略，每个访问对只需要被访问一次就可以，所以并不需要过多的访问</p><p>但先前介绍的算法按照时序使用经验，之前所有的经验都不会被使用，在一定程度上是一种浪费</p><h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&目标：从基于探索策略\mu生成的经验集中学习最优的 \mbox{target network}去近似最优动作价值\\
&初始化：用随机的网络参数 w 初始化网络\hat{Q}(s,a,w)和目标网络\hat{Q}(s,a,w^T),初始化经验回放池\\
&对于每个回合e=1\rightarrow E:\\
&\quad 获取环境初始状态s_0\\
&\quad 对于每个时间步t=0\rightarrow T:\\
&\qquad 根据当前网络 \hat{Q}(s,a,w),以\epsilon-贪心策略选择动作a_t\\
&\qquad 执行动作a_t，获得回报r_{t+1}，将生成的经验放入经验回放池\mathcal{B}=\{(s,a,r',s')\}\\
&\qquad \mathcal{B}中的数据足够，从\mathcal{B}中采样 N个数据\{<s_i,a_i,r_{i+1},s_{i+1}>\}_{i=1,\cdots,N}\\
&\qquad 对每个数据:\\
&\quad\qquad 用目标网络\hat{Q}(s,a,w_T)计算目标价值y_i(w_T)=r_{i+1}+\gamma \max\limits_{a\in \mathcal{A}(s_{i+1})}\hat{Q}(s_{i+1},a,w_T)\\
&\qquad 最小化损失函数 L=\frac{1}{N}\sum\limits_{i}\left(y_i(w_T)-\hat{Q}(s_i,a_i,w)\right)^2，更新 \mbox{main network}\\
&\qquad 在C轮迭代后，令w_T=w\\
\hline
\end{array}</script><p>异策略的 <strong>Q-learning</strong> 不需要在每轮迭代后更新策略，因为更新后的策略不会用于生成经验数据</p><p>Q-learning解决的是控制问题，在价值更新收敛后，可以直接求解出最优策略</p><h4 id="与DQN论文的不同"><a href="#与DQN论文的不同" class="headerlink" title="与DQN论文的不同"></a>与DQN论文的不同</h4><p>在于对目标价值的计算，DQN的计算效率更高，输入状态 $s$ 即可得出 $\mathbf{y}(w_T)$ ，在此处，为求解最大的目标动作价值，需要代入 $\vert \mathcal{A}(s)\vert$ 次，再求出最大值</p><p><img src="/posts/4275677787/image-20240304100543544.png" alt="image-20240304100543544"></p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>为每个访问对 $(s,a)$ 学习最优的动作价值，只要获取到最优动作价值，贪心策略就能求出最优策略</p><p>基于探索策略生成的一个回合训练网络，这个回合有1000步，而表格型 Q-learning需要100000步</p><p>非线性价值近似网络 $\hat{Q}(s,a,w)$ 结构：三层网络，输入层-隐藏层(100神经元)-输出层</p><p><img src="/posts/4275677787/image-20240305170620192.png" alt="image-20240305170620192"></p><p>探索不充分，虽然TD误差为0，但状态误差并未收敛到0</p><p><img src="/posts/4275677787/image-20240305170633924.png" alt="image-20240305170633924"></p><h2 id="基于策略函数的方法"><a href="#基于策略函数的方法" class="headerlink" title="基于策略函数的方法"></a>基于策略函数的方法</h2><h3 id="与基于值函数学习的对比"><a href="#与基于值函数学习的对比" class="headerlink" title="与基于值函数学习的对比"></a>与基于值函数学习的对比</h3><p><strong>基于价值的学习</strong>由一个 $w$ 为参数的价值函数 $Q(s,a,w)$</p><p>优化目标为最小化TD误差：</p><script type="math/tex;mode=display">J(w)=E_{\pi}\left[\frac{1}{2}\left(r_{t+1}+\gamma \max\limits_{a'\in \mathcal{A}(s')}\hat{Q}(s',a',w)-\hat{Q}(s,a,w)\right)^2\right]</script><p>更新方式</p><script type="math/tex;mode=display">w_{t+1}=w_{t}+\alpha_t\left[r_{t+1}+\gamma \max\limits_{a\in \mathcal{A}(s_{t+1})} \hat{Q}_{\pi}\left(s_{t+1},a,w_t\right)-\hat{Q}\left(s_t,a_t,w_t\right)\right]\bigtriangledown_w\hat{Q}\left(s_t,a_t,w_t\right)</script><p><strong>基于策略梯度的学习</strong> 由一个 $\theta$ 为参数的策略函数 $\pi(a\vert s,\theta)$</p><p>优化目标为最大化策略度量指标</p><script type="math/tex;mode=display">\max\limits_{\theta}J(\theta)=E_{\pi_\theta}\left[\pi(a\vert s,\theta)\hat{\delta}_{\pi}(s,a)\right]</script><p>更新方式</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[ \bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \hat{\delta}_{t}^{\pi}(s_t,a_t)\right]</script><h3 id="随机策略与确定性策略"><a href="#随机策略与确定性策略" class="headerlink" title="随机策略与确定性策略"></a>随机策略与确定性策略</h3><p><img src="/posts/4275677787/image-20240512000320865.png" alt="image-20240512000320865"></p><h4 id="随机策略梯度"><a href="#随机策略梯度" class="headerlink" title="随机策略梯度"></a>随机策略梯度</h4><p>softmax策略的梯度</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta\ln\pi(a\vert s,\theta)&=\bigtriangledown_\theta h(s,a,\theta)-\frac{1}{\sum\limits_{a\in \mathcal{A}(s)}e^{h(s,a,\theta)}}\sum\limits_{a\in \mathcal{A}(s)}e^{h(s,a,\theta)}\bigtriangledown_\theta h(s,a,\theta)\\
&=\bigtriangledown_\theta h(s,a,\theta)-E_{a\sim \pi(a\vert s,\theta)}\left[\bigtriangledown_\theta h(s,a,\theta)\right]
\end{aligned}</script><p>因此，策略网络的梯度为</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta J(\theta)&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot Q_{\pi}(S,A)\right]\\
&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\left(\bigtriangledown_\theta h(S,A,\theta)-E_{a\sim \pi(A\vert S,\theta)}\left[\bigtriangledown_\theta h(S,A,\theta)\right]\right)Q_{\pi}(S,A)\right]
\end{aligned}</script><h4 id="确定性策略梯度"><a href="#确定性策略梯度" class="headerlink" title="确定性策略梯度"></a>确定性策略梯度</h4><p>基于策略梯度，使用梯度上升法最大化策略度量函数</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha_{\theta}E_{S\sim \eta_{\pi}}\left[\bigtriangledown_\theta \pi(s,\theta)(\bigtriangledown_a Q_{\pi}(s,a))\vert_{a=\pi(s)}\right]</script><p>相应的使用随机梯度上升法去近似</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha_{\theta}\bigtriangledown_\theta \pi(s_t,\theta_{t})(\bigtriangledown_a Q_{\pi}(s_t,a))\vert_{a=\pi(s_t)}</script><p><img src="/posts/4275677787/image-20240512000949548.png" alt="image-20240512000949548"></p><h3 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h3><p>在实际应用中，带有神经网络的AC方法是不稳定的，</p><p>DDPG给出了DPG基础上的改进方法</p><ul><li><p>经验回放（离线策略）：提高样本利用效率</p></li><li><p>目标网络：DDPG参考DQN，为每个网络引入了目标网络</p><ul><li><p>Actor网络（策略网络）也需要目标网络。因为也会被用于计算Q值</p></li><li><p>与DQN中不同，DQN每隔一个固定时间更新目标价值网络的参数，DDPG中目标价值网络采用一种软更新方法，逐渐接近价值网络</p><script type="math/tex;mode=display">w_T=\tau w+(1-\tau)w_T</script><p>$\tau=1$ ，则和DQN更新方式一致</p></li><li><p>采用Double DQN的做法更新目标网络中 $s’$ 的Q值：用训练网络的Q值选择目标网络在 $s’$ 的动作</p><script type="math/tex;mode=display">Q\left(s',\arg\max\limits_{a'}Q(s',a',w)\right)\iff r'+\gamma \max\limits_{a'}Q(s',a',w)</script></li></ul></li><li><p>在动作输入前批标准化Q网络</p></li><li><p>添加连续噪声 $\mathcal{N}_t$ ：由于DDPG是一种确定性策略，本身探索能力有限</p><p>作为一种离线策略学习，类比DQN采用 $\epsilon-贪心$ 进行动作探索，DDPG为动作添加连续噪音用于探索</p></li></ul><p><img src="/posts/4275677787/image-20240512001305477.png" alt="image-20240512001305477"></p><p><img src="/posts/4275677787/image-20240512001449151.png" alt="image-20240512001449151"></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/4275677787/" title="6-深度强化学习">https://amostian.github.io/posts/4275677787/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 强化学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3839502760/" rel="prev" title="值函数近似"><i class="fa fa-chevron-left"></i> 值函数近似</a></div><div class="post-nav-item"><a href="/posts/1565497506/" rel="next" title="策略函数">策略函数 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%93%E5%90%88"><span class="nav-text">深度学习与强化学习的结合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%8E%E6%B7%B1%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="nav-text">传统计算机视觉与深度计算机视觉</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">传统强化学习与深度强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DRL%E5%88%86%E7%B1%BB"><span class="nav-text">DRL分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%BD%91%E7%BB%9C%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">基于价值网络的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DQN"><span class="nav-text">DQN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DQN%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%8A%80%E5%B7%A7"><span class="nav-text">DQN的两个技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%95%E5%85%A5%E4%B8%A4%E4%B8%AA%E7%BD%91%E7%BB%9C"><span class="nav-text">引入两个网络</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="nav-text">经验回放</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83"><span class="nav-text">经验回放中的分布</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-text">伪代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8EDQN%E8%AE%BA%E6%96%87%E7%9A%84%E4%B8%8D%E5%90%8C"><span class="nav-text">与DQN论文的不同</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">基于策略函数的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-text">与基于值函数学习的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E7%AD%96%E7%95%A5%E4%B8%8E%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5"><span class="nav-text">随机策略与确定性策略</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-text">随机策略梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-text">确定性策略梯度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDPG"><span class="nav-text">DDPG</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">220</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">82</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1192.1k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">48:20</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>