<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="8.动手学深度学习-循环神经网络"><meta property="og:url" content="https://amostian.github.io/posts/1556323108/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240401220741404.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240401222458665.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240401222604260.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240401223259308.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240401223846263.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402101312574.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402103709677.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402110149020.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402115919798.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402122733890.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402215634070.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402215141031.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402215730521.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402215849625.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402215901792.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402195012277.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402200800162.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240402192339346.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240403103644552.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240403105050187.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240403132319888.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240404202223945.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240405154839479.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240406012341824.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240406144846478.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240406150835082.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240405013034969.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240405013044736.png"><meta property="og:image" content="https://amostian.github.io/posts/1556323108/image-20240405014237128.png"><meta property="article:published_time" content="2024-04-01T14:40:10.000Z"><meta property="article:modified_time" content="2024-04-06T07:35:03.000Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/1556323108/image-20240401220741404.png"><link rel="canonical" href="https://amostian.github.io/posts/1556323108/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>8.动手学深度学习-循环神经网络 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">58</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">363</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/1556323108/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">8.动手学深度学习-循环神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-04-01 22:40:10" itemprop="dateCreated datePublished" datetime="2024-04-01T22:40:10+08:00">2024-04-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-04-06 15:35:03" itemprop="dateModified" datetime="2024-04-06T15:35:03+08:00">2024-04-06</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">16.6k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>44 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><p>卷积神经网络可以有效地处理空间信息，循环神经网络可以更好地处理序列信息。通过引入状态变量存储过去的信息和当前的输入，从而确定当前的输出</p><h2 id="8-1-序列模型"><a href="#8-1-序列模型" class="headerlink" title="8.1 序列模型"></a>8.1 序列模型</h2><h3 id="8-1-1-序列数据"><a href="#8-1-1-序列数据" class="headerlink" title="8.1.1 序列数据"></a>8.1.1 序列数据</h3><p>数据是有时序结构的，</p><p>对当前结果的预测不仅受当前数据影响，历史数据也会对当下、未来数据产生影响，但历史数据的重要性随时间会衰减</p><h4 id="序列数据例子"><a href="#序列数据例子" class="headerlink" title="序列数据例子"></a>序列数据例子</h4><p><img src="/posts/1556323108/image-20240401220741404.png" alt="image-20240401220741404"></p><h3 id="8-1-2-序列模型"><a href="#8-1-2-序列模型" class="headerlink" title="8.1.2 序列模型"></a>8.1.2 序列模型</h3><h4 id="归纳偏置"><a href="#归纳偏置" class="headerlink" title="归纳偏置"></a>归纳偏置</h4><p>虽然特定的随机变量 $x_t$ 会改变，但序列本身的动力学不会改变。在统计学上称不变的动力学为 静止的。</p><p>在时刻 $t$ 可以观察到数据 $x_t$ ，得到 $T$ 个与时间相关的不独立随机变量 $x_1,x_2,\cdots,x_T$</p><p>因此整个序列的估计值可以通过全概率公式得到</p><script type="math/tex;mode=display">\begin{aligned}
p(x_1,x_2,\cdots,x_T)\xlongequal{p(a,b)=p(a)p(b\vert a)}&p(x_T\vert x_{1},\cdots,x_{T-1})p(x_{1},\cdots,x_{T-1})\\
=&p(x_T\vert x_{1},\cdots,x_{T-1})p(x_{T-1}\vert x_{1},\cdots,x_{T-2})\cdots p(x_2\vert x_1)p(x_1)\\
=&\prod\limits_{t=1}^Tp(x_t\vert x_{t-1},\cdots,x_1)
\end{aligned}</script><p>即计算 $T$ 时刻的数据 $x_T$ ，需要结合历史数据</p><p><img src="/posts/1556323108/image-20240401222458665.png" alt="image-20240401222458665"></p><p>也可以反向计算：根据未来数据倒退过去数据</p><p><img src="/posts/1556323108/image-20240401222604260.png" alt="image-20240401222604260"></p><p>因此，我们可以对一个对象的历史数据建模，使其能拟合自身，因为标签为数据本身，所以称为 <strong>自回归模型</strong></p><script type="math/tex;mode=display">p(x_t\vert x_1,\cdots,x_{t-1})=p\left(x_t\vert f(x_1,\cdots,x_{t-1})\right)</script><ul><li>若数据是离散的，则 $f()$ 为分类器</li><li>若数据是连续的，则 $f()$ 为回归模型</li></ul><h4 id="马尔科夫假设"><a href="#马尔科夫假设" class="headerlink" title="马尔科夫假设"></a>马尔科夫假设</h4><p>当前数据只与 $\tau$ 个过去数据点相关</p><p><img src="/posts/1556323108/image-20240401223259308.png" alt="image-20240401223259308"></p><ul><li><p>即自回归模型有 $\tau$ 个输入特征，将变长数据变为固定长度的数据</p><p>直接好处是参数的数量不变</p></li></ul><h4 id="潜变量模型"><a href="#潜变量模型" class="headerlink" title="潜变量模型"></a>潜变量模型</h4><p>引入潜变量 $h_t$ 表示随机变量的历史信息（隐变量：未观测到的变量；潜变量：可能不存在的变量，包含因变量范畴）</p><p><img src="/posts/1556323108/image-20240401223846263.png" alt="image-20240401223846263"></p><p><img src="/posts/1556323108/image-20240402101312574.png" alt="image-20240402101312574"></p><script type="math/tex;mode=display">h_t=f(h_{t-1},x_{t-1})\\
\hat{x}_{t}=p(\hat{x}_{t}\vert h_{t},x_{t-1})</script><p>需要学习两个模型</p><ul><li>第一个模型是潜变量模型，根据上一时刻数据 $x_{t-1}$ 与潜变量 $h_{t-1}$ 计算当前时刻的潜变量 $h_t$</li><li>第二个模型是数据自回归模型，根据上一时刻数据 $x_{t-1}$ 与当前时刻潜变量 $h_t$ 计算当前时刻数据 $x_t$</li></ul><h3 id="8-1-3-基于马尔科夫假设的自回归模型"><a href="#8-1-3-基于马尔科夫假设的自回归模型" class="headerlink" title="8.1.3 基于马尔科夫假设的自回归模型"></a>8.1.3 基于马尔科夫假设的自回归模型</h3><blockquote><p>基于马尔科夫假设结合单隐藏层MLP，利用预测的数据拟合很远的未来数据会失效</p></blockquote><h4 id="序列数据生成"><a href="#序列数据生成" class="headerlink" title="序列数据生成"></a>序列数据生成</h4><p>使用正弦函数和噪声成序列数据，时间步为 1000</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">T = <span class="number">1000</span>  <span class="comment"># 总共产生1000个点</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line"><span class="comment"># 数据服从sin函数，再加随机噪声</span></span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line">d2l.plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402103709677.png" alt="image-20240402103709677"></p><h4 id="序列数据集生成"><a href="#序列数据集生成" class="headerlink" title="序列数据集生成"></a>序列数据集生成</h4><p>假设窗口为 $\tau$ ，我们的目的是用 $\tau$ 个历史数据 $\mathbf{x}_t$ 去拟合当前数据 $x_t$ ，因此，<em>特征-标签</em> 集为 $\mathbf{x}_t=\{x_{t-1},x_{t-2},\cdots,x_{t-\tau}\}$ ，$y_t=x_t$</p><p>可能前面几个数据没有 $\tau$ 个历史数据：</p><ul><li>若数据序列足够长，则丢弃前面几项，即从 $\tau+1$ 时刻开始估计</li><li>填充 $\tau$ 个零</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tau = <span class="number">4</span></span><br><span class="line">features = torch.zeros((T - tau, tau))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: T - tau + i]</span><br><span class="line">labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><ul><li>窗口第1个取值范围为 $x[0,996]$ ；第2个取值范围 $x[1,997]$ ；第3个取值范围 $x[2,998]$ ；第4个取值范围 $x[3,999]$</li><li>即 $feature[0,0]=x[0],x[1],x[2],x[3]$ 表示第一个样本的四个窗口值</li><li>即此处丢弃了前4个数据，从 $\tau+1$ 时刻开始估计</li></ul><p>使用前600个序列样本进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size, n_train = <span class="number">16</span>, <span class="number">600</span></span><br><span class="line"><span class="comment"># 只有前n_train个样本用于训练</span></span><br><span class="line">train_iter = d2l.load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="回归模型"><a href="#回归模型" class="headerlink" title="回归模型"></a>回归模型</h4><p>使用有两个全连接层的MLP</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络权重的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的多层感知机</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Linear(<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, <span class="number">5</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><h5 id="模型单步预测表现"><a href="#模型单步预测表现" class="headerlink" title="模型单步预测表现"></a>模型单步预测表现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = net(features)</span><br><span class="line"><span class="comment"># plt传入第一个列表参数，分别表示两个函数的自变量；第二个列表参数，分别表示两个函数的因变量</span></span><br><span class="line">d2l.plot([time, time[tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>], xlim=[<span class="number">1</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402110149020.png" alt="image-20240402110149020"></p><p>单步预测效果不错，600+4作为测试数据，结果也是可信的。</p><p>但前提是滑动窗口内的4个数据都是真实的，即</p><script type="math/tex;mode=display">\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\
\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, x_{605}), \\
\hat{x}_{607} = f(x_{603}, x_{604}, x_{605}, x_{606}),\\
\hat{x}_{608} = f(x_{604}, x_{605}, x_{606}, x_{607}),\\
\hat{x}_{609} = f(x_{605}, x_{606}, x_{607}, x_{608}),\\
\ldots</script><h5 id="多步预测表现"><a href="#多步预测表现" class="headerlink" title="多步预测表现"></a>多步预测表现</h5><p>若真实观测只到 $t$ 时刻，对于 $t+k$ 时刻的预测 $\hat{x}_{t+k}$ 称为 $k$ 步预测，即必须依据自己的预测值作为预测未来的历史数据。如：真实数据记录到 $x_{604}$ ，$\hat{x}_{605}$ 为 $x_{604}$ 的单步预测，$\hat{x}_{606}$ 为 $x_{604}$ 的2步预测，$\hat{x}_{604+k}$ 为 $x_{604}$ 的k步预测</p><script type="math/tex;mode=display">\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\
\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\
\hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\
\hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\
\hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\
\ldots</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 真实数据直到x[604]</span></span><br><span class="line">multistep_preds = torch.zeros(T)</span><br><span class="line">multistep_preds[: n_train + tau] = x[: n_train + tau]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, T):</span><br><span class="line">    <span class="comment"># 使用多步预测的预测数据填充预测未来的历史数据</span></span><br><span class="line">    multistep_preds[i] = net(multistep_preds[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>结果如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d2l.plot([time, time[tau:], time[n_train + tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy(),</span><br><span class="line">          multistep_preds[n_train + tau:].detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>, <span class="string">&#x27;multistep preds&#x27;</span>],</span><br><span class="line">         xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402115919798.png" alt="image-20240402115919798"></p><p>605开始为预测数据，刚开始还比较正常，后续趋势就不正常了</p><p>因为误差的积累：每次预测未来数据都会有一定误差，预测误差的累积造成 $k$ 步预测的困难</p><ul><li><p>对于单步预测，积累了误差 $\epsilon_1=\overline{\epsilon}$</p></li><li><p>单步预测的结果被用作2步预测的历史数据，因此2步预测的结果被扰动了 $\epsilon_1$ ，误差累积为 $\epsilon_2=\overline{\epsilon}+c\epsilon_1$</p><p>因此，误差的累积会相当快地偏离真实结果</p></li></ul><p>如：天气预报未来24小时相当准确，但超过后精度会迅速下降</p><p>进一步观察 $k$ 步预测的困难</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">max_steps = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">features = torch.zeros((T - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line"><span class="comment"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: i + T - tau - max_steps + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造934个 预测数据-历史数据序列</span></span><br><span class="line"><span class="built_in">print</span>(features.shape)</span><br><span class="line"><span class="comment">#	torch.Size([933, 68])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用预测数据替换预测窗口内的值</span></span><br><span class="line"><span class="comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line">d2l.plot([time[tau + i - <span class="number">1</span>: T - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">         [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step preds&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps], xlim=[<span class="number">5</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402122733890.png" alt="image-20240402122733890"></p><h5 id="多步预测总结"><a href="#多步预测总结" class="headerlink" title="多步预测总结"></a>多步预测总结</h5><p>马尔科夫假设，短期预测可行，长期预测会失效</p><p>多步预测，填充的是预测值，而不是真实观测值</p><p>虽然 $\tau$ 增大会使模型更准确，但同时也会导致训练样本变少。同时也会导致计算量变大</p><h3 id="8-1-4-改进"><a href="#8-1-4-改进" class="headerlink" title="8.1.4 改进"></a>8.1.4 改进</h3><p>时间序列窗口只是学习局部的时序特征，给定一个窗口的输入，可以根据输入的时序特征预测出未来的数据。</p><p>时序窗口可根据数据调整，即自动选择预测数据所需的相关数据——transformer</p><h2 id="8-2-语言模型"><a href="#8-2-语言模型" class="headerlink" title="8.2 语言模型"></a>8.2 语言模型</h2><p>语言模型：将整个文本当做时序序列，样本间有时序信息</p><h3 id="8-2-1-文本预处理"><a href="#8-2-1-文本预处理" class="headerlink" title="8.2.1 文本预处理"></a>8.2.1 文本预处理</h3><blockquote><p>文本预处理，将文本数据变为可训练的样本</p></blockquote><ol><li>将文本作为字符串加载到内存中</li><li>将字符串拆分为词元</li><li>建立词表，将词元映射到数字索引</li><li>将文本转换为数字索引序列</li></ol><h4 id="从文件将数据集读入内存"><a href="#从文件将数据集读入内存" class="headerlink" title="从文件将数据集读入内存"></a>从文件将数据集读入内存</h4><p>数据集在内存中为一维张量，0轴维度为文本行数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集逐行读入，并将内容转换为小写字母与空格&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        <span class="comment"># 将非26个大小写字母替换为空格，去除回车，相当于只有26个小写字母与空格的文本数据</span></span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# 文本总行数: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#	the time machine by h g wells</span></span><br><span class="line"><span class="comment">#	twinkled and his usually pale face was flushed and animated the#	</span></span><br></pre></td></tr></table></figure><h4 id="将字符串拆分为词元"><a href="#将字符串拆分为词元" class="headerlink" title="将字符串拆分为词元"></a>将字符串拆分为词元</h4><p>将文本序列拆分为二维词元张量，0轴维数为文本行数，1轴维度为当前行的字符串所含字符串词元的数量——序列</p><p><strong>词元</strong> （token）：文本的基本单位，模型最小的可识别数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="comment"># 好处是机器模型简单，坏处是单词数量很多</span></span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="comment"># 好处是字母个数少，坏处是需要学习如何用字符组成单词</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br><span class="line"><span class="comment">#	[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span></span><br><span class="line"><span class="comment">#	[]</span></span><br><span class="line"><span class="comment">#	[]</span></span><br><span class="line"><span class="comment">#	[]</span></span><br><span class="line"><span class="comment">#	[]</span></span><br><span class="line"><span class="comment">#	[&#x27;i&#x27;]</span></span><br><span class="line"><span class="comment">#	[]</span></span><br><span class="line"><span class="comment">#	[]</span></span><br><span class="line"><span class="comment">#	[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;, &#x27;for&#x27;, &#x27;so&#x27;, &#x27;it&#x27;, &#x27;will&#x27;, &#x27;be&#x27;, &#x27;convenient&#x27;, &#x27;to&#x27;, &#x27;speak&#x27;, &#x27;of&#x27;, &#x27;him&#x27;]</span></span><br><span class="line"><span class="comment">#	[&#x27;was&#x27;, &#x27;expounding&#x27;, &#x27;a&#x27;, &#x27;recondite&#x27;, &#x27;matter&#x27;, &#x27;to&#x27;, &#x27;us&#x27;, &#x27;his&#x27;, &#x27;grey&#x27;, &#x27;eyes&#x27;, &#x27;shone&#x27;, &#x27;and&#x27;]</span></span><br><span class="line"><span class="comment">#	[&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]</span></span><br></pre></td></tr></table></figure><h4 id="构建词表"><a href="#构建词表" class="headerlink" title="构建词表"></a>构建词表</h4><p>文本数据集的所有词元，是一个有限集，将字符串类型的词元映射为从0开始的数字索引，即词表的作用</p><ol><li><p>将训练文本数据集中的所有词元合并在一起，对词元进行唯一统计，得到的统计结果为 <em>语料</em></p></li><li><p>根据每个唯一词元的出现频率，为其分配一个数字索引。出现次数越多的词元，索引越小，很少出现的词元通常被移除</p></li><li><p>对于不在语料库中的词元（及因出现次数过少而删除的词元）映射到特殊的未知词元 <code>&lt;unk&gt;</code></p><p>对于保留字词元，使用单独的序列保存： <code>&lt;pad&gt;</code> 填充词元；<code>&lt;bos&gt;</code> 序列开始；<code>&lt;eos&gt;</code> 序列结束</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 若一个词元在tokens中出现的次数少于min_freq，则丢弃该词元</span></span><br><span class="line">    <span class="comment"># 标识文本的特殊保留字</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 对每个token出现次数计数2</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="comment"># 按出现频率排序降序排序：频率高的放在前面</span></span><br><span class="line">        <span class="comment"># 待排序列表为 [&lt;token,freqs&gt;]，排序key为 freqs，返回降序的列表</span></span><br><span class="line">        <span class="comment">#     counter.items()：获取字典中的所有键值对</span></span><br><span class="line">        <span class="comment">#     lambda x: x[1]：返回x元组的第二项，即每个token出现的次数</span></span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                   reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 特殊词元列表为 [&#x27;&lt;unk&gt;&#x27;,*[reserved_tokens]]</span></span><br><span class="line">        <span class="comment">#      表示未知词元的保留字索引为0</span></span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        <span class="comment"># 词元列表转索引为字典：&#123;(token,索引)&#125;</span></span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="comment"># 实现token与索引的互转</span></span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="comment"># 参数为一组token，一个token为tuple类型，一组token为list类型</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="comment"># 魔法方法，可以通过索引访问list元素</span></span><br><span class="line">        <span class="comment">#   实现的功能是 token_to_idx[token] / token_to_idx.getitem(token)</span></span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="comment"># 返回索引对应的token</span></span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freqs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 这里的tokens是1D列表或2D列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="comment"># 将词元列表展平成一个列表</span></span><br><span class="line">        <span class="comment"># for line in tokens：将二维tokens的每一行列表拿出来</span></span><br><span class="line">        <span class="comment">#     for token in line：将line中的每个token拿出来</span></span><br><span class="line">        <span class="comment">#       存入tokens的一维列表中</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="comment"># 对tokens构造字典，计算每个token在tokens文本数据中出现的次数</span></span><br><span class="line">    <span class="comment"># 返回key-value字典，即&#123;&lt;token,freqs&gt;&#125;</span></span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="comment"># 输出语料库的token转idx的前10项</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.idx_to_token)[:<span class="number">10</span>])</span><br><span class="line"><span class="comment">#	[(&#x27;&lt;unk&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span></span><br><span class="line"><span class="comment">#	[&#x27;&lt;unk&gt;&#x27;, &#x27;the&#x27;, &#x27;i&#x27;, &#x27;and&#x27;, &#x27;of&#x27;, &#x27;a&#x27;, &#x27;to&#x27;, &#x27;was&#x27;, &#x27;in&#x27;, &#x27;that&#x27;]</span></span><br><span class="line"><span class="built_in">print</span>(vocab.__getitem__((<span class="string">&#x27;of&#x27;</span>,)))</span><br><span class="line"><span class="comment"># [4]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;文本:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;索引:&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本: [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span></span><br><span class="line"><span class="comment"># 索引: [1, 19, 50, 40, 2183, 2184, 400]</span></span><br><span class="line"><span class="comment"># 文本: [&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]</span></span><br><span class="line"><span class="comment"># 索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]    </span></span><br></pre></td></tr></table></figure><h4 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h4><p>将练习用到的所有功能打包到 <code>load_corpus_time_machine()</code> 中，返回词表 <code>vocab</code> 和数字索引的文本数据集 <code>corpus</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span></span><br><span class="line">    <span class="comment"># 所以将所有文本行展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="comment"># vocab为文本数据的语料库，corpus为文本数据的索引表示</span></span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="comment"># 一个unk字符，vocab有26个小写字母，一个空格</span></span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br><span class="line"><span class="comment"># (170580, 28)</span></span><br></pre></td></tr></table></figure><h3 id="8-2-2-语言模型"><a href="#8-2-2-语言模型" class="headerlink" title="8.2.2 语言模型"></a>8.2.2 语言模型</h3><blockquote><p>语言模型是一种时间序列模型，给定序列文本 $x_1,\cdots,x_T$ ，$x_t$ 可被认为文本序列在时间步 $t$ 处的观测或标签。语言模型的目标是估计文本序列出现的联合概率</p></blockquote><p>应用：</p><ul><li>做预训练模型：BERT，Chat</li><li>给定前面几个词，生成文本：不断使用 $x_t\sim p(x_t\vert x_{t-1},x_{t-2},\cdots,x_1)$ 来生成后续文本</li><li>判断多个序列中那个更常见：语音识别到 “to recognize speech” 和 “to wreck a nice beach” ，判断那个文本序列出现的概率高</li></ul><h4 id="计数建模"><a href="#计数建模" class="headerlink" title="计数建模"></a>计数建模</h4><p>使用计数建模——贝叶斯</p><p>假设序列长度为 $2$ ，预测 $p(x,x’)=p(x)p(x’\vert x)=\frac{n(x)}{n}\frac{n(x,x’)}{n(x)}$</p><ul><li>$n$ 为语料数据集的 len ，即数据集中所有词元的个数（含重复词元）</li><li>$n(x)$ 为单个单词出现的次数，$n(x,x’)$ 为连续单词对出现的次数</li></ul><p>若序列长度为3，预测 $p(x,x’,x’’)=p(x)p(x’\vert x)p(x’’\vert x,x’)=\frac{n(x)}{n}\frac{n(x,x’)}{n(x)}\frac{n(x,x’,x’’)}{n(x,x’)}$</p><p>但使用原始的条件概率模型，可能因文本量不够大，导致 $n(x_1,x_2,\cdots,x_T)\le 1$</p><blockquote><p>一种不可行的方法是拉普拉斯平滑</p><script type="math/tex;mode=display">\begin{aligned}
 \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
 \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
 \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}</script><p>无效原因：</p><ul><li>要存储所有的平滑常数 $\epsilon$</li><li>完全忽略了时序含义</li><li>只关注历史出现的词元组合，对于过去未出现的序列表现不佳</li></ul></blockquote><h4 id="马尔科夫建模"><a href="#马尔科夫建模" class="headerlink" title="马尔科夫建模"></a>马尔科夫建模</h4><p>使用马尔科夫假设可以缓解，设定时间步长，减少序列长度</p><ul><li><p>一元语法：独立的，与历史数据无关</p><script type="math/tex;mode=display">\begin{aligned}
p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2)p(x_3)p(x_4)\\
&=\frac{n(x_1)}{n}\frac{n(x_2)}{n}\frac{n(x_3)}{n}\frac{n(x_4)}{n}
\end{aligned}</script></li><li><p>二元语法：仅与前一个时刻的数据有关</p><script type="math/tex;mode=display">\begin{aligned}
p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2\vert x_1)p(x_3\vert x_2)p(x_4\vert x_3)\\
&=\frac{n(x_1)}{n}\frac{n(x_1,x_2)}{n(x_1)}\frac{n(x_2,x_3)}{n(x_2)}\frac{n(x_3,x_4)}{n(x_3)}
\end{aligned}</script></li><li><p>三元语法：与前两个时刻的数据有关</p><script type="math/tex;mode=display">\begin{aligned}
p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2\vert x_1)p(x_3\vert x_1,x_2)p(x_4\vert x_2,x_3)\\
&=\frac{n(x_1)}{n}\frac{n(x_1,x_2)}{n(x_1)}\frac{n(x_1,x_2,x_3)}{n(x_1,x_2)}\frac{n(x_2,x_3,x_4)}{n(x_2,x_3)}
\end{aligned}</script></li></ul><p>假设有1000个词，二元语法，只需要保存两个词的组合概率，共 1000*1000个组合，还需存1000个词单独出现的概率。此时若查任意长度的序列 $\{x_1,x_2,\cdots,x_T\}$ 出现概率 $p(x_1,\ldots,x_T)$ 的时间复杂度为 $O(T)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        <span class="comment"># 将非26个大小写字母替换为空格，去除回车，相当于只有26个小写字母与空格的文本数据</span></span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">tokens = d2l.tokenize(read_time_machine())</span><br><span class="line"><span class="comment"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span></span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line"><span class="comment"># 获取语料数据集的词表</span></span><br><span class="line">vocab = d2l.Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402215634070.png" alt="image-20240402215634070"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">d2l.plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">         xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402215141031.png" alt="image-20240402215141031"></p><p>在消除前几个高频词和后几个低频词后，词频以 parallel log 的方式迅速衰减，意味着单词的频率满足 <em>齐普夫定律</em>（Zipf’s law）</p><script type="math/tex;mode=display">n_i\propto \frac{1}{i^\alpha}</script><p>即 $\log n_i=-\alpha \log i+c$</p><h5 id="多元语法与词频是否满足双对数曲线"><a href="#多元语法与词频是否满足双对数曲线" class="headerlink" title="多元语法与词频是否满足双对数曲线"></a>多元语法与词频是否满足双对数曲线</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二元语法的token，用元组表示 </span></span><br><span class="line"><span class="comment">#    元组的第一个项：取文本数据的第0到倒数第2个；第二个项：取文本数据的第1到最后一个</span></span><br><span class="line"><span class="comment">#    如：bigram_tokens[5]=(corpus[5],corpus[6])</span></span><br><span class="line"><span class="comment"># zip(a,b)：将a列表和b列表逐元素组合打包为一个元组列表：[(a[0],b[0]), (a[1],b[1]),...]</span></span><br><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line"><span class="comment"># 将二元语法转换为词表</span></span><br><span class="line">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class="line"><span class="comment"># 查看出现次数前10的二元语法</span></span><br><span class="line"><span class="comment">#   token_freqs是一个以频数降序排序的键值列表：[二元词元:频数]</span></span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402215730521.png" alt="image-20240402215730521"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二元语法的token，用元组表示</span></span><br><span class="line">trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">    corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402215849625.png" alt="image-20240402215849625"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">trigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line">d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class="string">&#x27;token: x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">&#x27;unigram&#x27;</span>, <span class="string">&#x27;bigram&#x27;</span>, <span class="string">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240402215901792.png" alt="image-20240402215901792"></p><ul><li>多元语法的单词序列也遵循 齐普夫定律</li><li>$n$ 个单词的 $k$ 元语法理论上会有 $n^k$ 种，但实际上单词之间组合是有特定意义的，某些组合并不会出现，因此拉普拉斯平滑并不适用于语言模型。若去掉低频组合后，会使得 $k$ 元组合的个数比单词的个数 $n$ 还少</li><li>这种现象说明，语言中存在相当多的时序结构，这个时序结构也是我们后续需要学习的</li></ul><h3 id="8-2-3-数据集"><a href="#8-2-3-数据集" class="headerlink" title="8.2.3 数据集"></a>8.2.3 数据集</h3><p>文本序列是可以任意长的，所以可以将这个文本序列划分为具有相同长度的子序列</p><p>模型中的网络一次可以处理时间步为 $T$ 的子序列，即窗口为 $T$ ，需要生成子序列的输入特征及相应的标签用作训练。</p><p>文本序列最简单的就是从随机位置取连续的 $T$ 个 <code>token</code> ，代价是某个token会被重复计算，造成浪费</p><p><img src="/posts/1556323108/image-20240402195012277.png" alt="image-20240402195012277"></p><p>所以最好将序列切分为 $T$ 个时间步的子序列，每次随机取一个批量的子序列</p><p><img src="/posts/1556323108/image-20240402200800162.png" alt="image-20240402200800162"></p><p>但若是固定划分子序列，会造成一些如下方红色子序列无法被采样。</p><p>为解决这个问题，每次随机选择初始位置，从随机初始位置开始重新划分子序列，这样可以遍历所有可能的子序列</p><p>例如：若规定小批量序列长度为5个时间步，每个时间步的词元为一个字符，则可以选择任意初始位置划分子序列</p><p><img src="/posts/1556323108/image-20240402192339346.png" alt="image-20240402192339346"></p><ol><li><p>将文本数据 <code>corpus</code> 分为 $T$ 个 <code>token</code> 的若干个子序列（上图中token为一个字符）</p></li><li><p>从 $[0,T-1]$ 中随机选择一个数作为初始位置索引</p></li><li><p>随机采样：随机子序列组成一个批量</p><ul><li><p>取所有子序列的初始位置下标</p></li><li><p>打乱初始位置下标</p></li><li><p>逐批量返回 <code>[&lt;特征,标签&gt;]</code></p><p>批量多大，则从初始位置list中取几个初始索引</p><p>特征：corpus[初始索引:初始索引+时间步长T]</p><p>标签：corpus[初始索引+1:初始索引+时间步长T+1]</p></li></ul><p>顺序分区：顺序子序列组成一个批量</p><ul><li>构成每个batch需要batch_size个 <code>token</code> ， 计算该 <code>corpus</code> 可划分的 batch数，进而计算这些数量的批量所需的 <code>token</code> 数 <code>num_tokens</code></li><li>从随机初始位置开始，从 <code>corpus</code> 中取 <code>num_tokens</code> 个 <code>token</code> 作为特征 Xs ，并取相应的标签Ys</li><li>将 Xs 划分为 batch 个批量，即Xs的每一列有batch_size个 <code>token</code> ，取一列表示一个批量</li></ul></li></ol><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p><strong>随机采样</strong> ：每个批量都是在原始数据语料集中不重复随机捕获的子序列集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># num_steps为时间窗口</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 长度为num_steps的子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        <span class="comment"># 在这里，initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]</span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#	X:  tensor([[ 3,  4,  5,  6,  7],</span></span><br><span class="line"><span class="comment">#	        [13, 14, 15, 16, 17]]) </span></span><br><span class="line"><span class="comment">#	Y: tensor([[ 4,  5,  6,  7,  8],</span></span><br><span class="line"><span class="comment">#	        [14, 15, 16, 17, 18]])</span></span><br><span class="line"><span class="comment">#	X:  tensor([[28, 29, 30, 31, 32],</span></span><br><span class="line"><span class="comment">#	        [23, 24, 25, 26, 27]]) </span></span><br><span class="line"><span class="comment">#	Y: tensor([[29, 30, 31, 32, 33],</span></span><br><span class="line"><span class="comment">#	        [24, 25, 26, 27, 28]])</span></span><br><span class="line"><span class="comment">#	X:  tensor([[ 8,  9, 10, 11, 12],</span></span><br><span class="line"><span class="comment">#	        [18, 19, 20, 21, 22]]) </span></span><br><span class="line"><span class="comment">#	Y: tensor([[ 9, 10, 11, 12, 13],</span></span><br><span class="line"><span class="comment">#	        [19, 20, 21, 22, 23]])    </span></span><br></pre></td></tr></table></figure><p><strong>顺序分区</strong> ：保证两个相邻的小批量中，子序列在原始序列上也是相邻的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    </span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"><span class="comment">#	X:  tensor([[ 5,  6,  7,  8,  9],</span></span><br><span class="line"><span class="comment">#	        [19, 20, 21, 22, 23]]) </span></span><br><span class="line"><span class="comment">#	Y: tensor([[ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="comment">#	        [20, 21, 22, 23, 24]])</span></span><br><span class="line"><span class="comment">#	X:  tensor([[10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="comment">#	        [24, 25, 26, 27, 28]]) </span></span><br><span class="line"><span class="comment">#	Y: tensor([[11, 12, 13, 14, 15],</span></span><br><span class="line"><span class="comment">#	        [25, 26, 27, 28, 29]])</span></span><br></pre></td></tr></table></figure><h4 id="封装返回数据集迭代器"><a href="#封装返回数据集迭代器" class="headerlink" title="封装返回数据集迭代器"></a>封装返回数据集迭代器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class="line">        <span class="comment"># 限制最大词元数，在学习过程中可以控制数据量，避免训练时间过长</span></span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps,  <span class="comment">#@save</span></span></span><br><span class="line"><span class="params">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure><h2 id="8-3-RNN"><a href="#8-3-RNN" class="headerlink" title="8.3 RNN"></a>8.3 RNN</h2><h3 id="8-3-1-基本概念"><a href="#8-3-1-基本概念" class="headerlink" title="8.3.1 基本概念"></a>8.3.1 基本概念</h3><h4 id="隐变量模型"><a href="#隐变量模型" class="headerlink" title="隐变量模型"></a>隐变量模型</h4><p>潜变量自回归模型使用潜变量 $h_t$ 总结历史信息</p><p><img src="/posts/1556323108/image-20240403103644552.png" alt="image-20240403103644552"></p><p>RNN相较潜变量自回归模型更简单一些</p><ul><li>隐变量 $h_t$ 与上一时刻的输入 $x_{t-1}$ 与隐变量 $h_{t-1}$ 有关</li><li>当前时刻的输出标签 $o_t$ 仅与当前时刻的隐变量 $h_t$ 有关</li></ul><p><img src="/posts/1556323108/image-20240403105050187.png" alt="image-20240403105050187"></p><ul><li>隐变量： $\mathbf{h}_t=\phi(\mathbf{W}_{hh}\mathbf{h}_{t-1}+\mathbf{W}_{hx}\mathbf{x}_{t-1}+\mathbf{b}_h)$ ：<code>token</code> 会被表示为 one-hot 向量作为输入 $\mathbf{x}_{t-1}$</li><li>输出标签： $\hat{\mathbf{y}}_t=\phi(\mathbf{W}_{ho}\mathbf{h}_t+\mathbf{b}_y)$</li></ul><p>损失指预测输出 $\hat{\mathbf{y}}_t$ 与真实 $\mathbf{y}_t$ 之间的差异。真正的输出 $\mathbf{y}_t=\mathbf{x}_t$ ，因此隐变量模型是自回归模型</p><ul><li>预测输出 $\hat{\mathbf{y}}_t$ 时不会用到 $\mathbf{x}_t$ ，等价于只用到历史信息 $\mathbf{x}_{t-1}$ 与 $\mathbf{h}_{t-1}$</li></ul><h5 id="隐变量模型的理解"><a href="#隐变量模型的理解" class="headerlink" title="隐变量模型的理解"></a>隐变量模型的理解</h5><p>隐变量 $\mathbf{h}_t$ 中保存的是上一个输入 $\mathbf{x}_{t-1}$ 与前面所有输入的历史信息 $\mathbf{h}_{t-1}$ ，是对两种信息的通用历史特征的提取，相当于卷积神经网络中的局部特征</p><p>因此，隐变量提取出的通用历史特征有隐藏变量的神经元表示， 其提取程度由 <em>隐藏变量的神经元数量</em>（超参数）来控制，若设置不合理，容易造成过拟合。</p><p>而 <em>输入-隐变量</em> 和 <em>隐变量-隐变量</em> 之间的时序关系由参数 $\mathbf{W}_{hh},\mathbf{W}_{hx}$ 保存，输出与隐变量间的关系由参数 $\mathbf{W}_{hq}$ 保存</p><h4 id="RNN的通用近似定理"><a href="#RNN的通用近似定理" class="headerlink" title="RNN的通用近似定理"></a>RNN的通用近似定理</h4><blockquote><p>循环神经网络的通用近似定理：如果一个完全连接的循环神经网络有足够数量的 $sigmod$ 型隐藏神经元，它可以以任意的准确率去近似任何一个非线性动力系统</p><ul><li><p>动力系统：系统状态按一定的规律随时间变化的系统</p><p>指用一个函数来描述一个给定的状态空间中的所有点随时间的变化情况</p></li></ul></blockquote><script type="math/tex;mode=display">s_t=g(s_{t-1},x_t)\\
y_t=o(s_t)</script><p>其中，$s_t$ 为每个时刻的隐状态，$g(\cdot)$ 是可测的状态转换函数，$x_t$ 为某时刻的外部输入，$o(\cdot)$ 为连续输出函数，且对状态空间的紧致性没有限制</p><p>理论上，循环神经网络可以近似任意的非线性动力系统</p><ul><li><p>前馈神经网络可以近似任何函数</p></li><li><p>循环神经网络可以模拟任何程序</p></li></ul><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p><img src="/posts/1556323108/image-20240403132319888.png" alt="image-20240403132319888"></p><h3 id="8-3-2-RNN模型"><a href="#8-3-2-RNN模型" class="headerlink" title="8.3.2 RNN模型"></a>8.3.2 RNN模型</h3><h4 id="词元-状态的表示"><a href="#词元-状态的表示" class="headerlink" title="(词元)状态的表示"></a>(词元)状态的表示</h4><p>$\mathcal{V}$ 为所有(词元)状态 <code>token</code> 的词表 <code>vocab</code> ，若有 $V$ 个 <code>token</code> ，则每个 <code>token</code> 需要长度 $V$ 的 <em>one-hot</em> 向量表示</p><p>每个(词元)状态都对应 $\mathcal{V}$ 中的一个数字索引，但直接使用数字索引进行反向传播，会导致损失计算困难，所以需要进一步转换为 <em>one-hot</em> 向量。</p><p>one-hot 向量：将每个索引映射为互不相同的单位向量</p><ul><li>如果(词元)状态的索引为 $\nu$ ，则创建长度为 $V$ 的全0向量，再将第 $\nu$ 个元素设置为 $1$</li></ul><h4 id="训练数据集"><a href="#训练数据集" class="headerlink" title="训练数据集"></a>训练数据集</h4><p>实际应用中，数据是逐批量被传入模型的</p><p>令 $T\triangleq时间步长,B\triangleq批量大小=\vert \mathcal{B}\vert ,V\triangleq词表大小=\vert \mathcal{V}\vert$ ，$h\triangleq 隐变量神经元数量$</p><p>$\mathbf{X}\in \R^{T\times B\times V}$ 为一个批量的子序列在一个时间窗口的状态</p><ul><li>$\mathbf{X}^{(t)}\in \R^{B\times V},t\in [1,T]$ 表示在 $t$ 时刻一个批量的子序列的(词元)状态</li><li>$\mathbf{x}_i^{(t)}\in \R^{1\times V}$ 表示在第 $i$ 个子序列中，$t$ 时刻的(词元)状态</li></ul><p>$\mathbf{H}\in \R^{T\times B\times h}$ 为一个批量的子序列在一个时间窗口的隐变量</p><ul><li><p>$\mathbf{H}^{(t)}\in \R^{B\times h},t\in [1,T]$ 表示一个批量子序列的(词元)状态在 $t$ 时刻的隐变量</p></li><li><p>$\mathbf{h}_i^{(t)}\in\R^{1\times h}$ ：从 $\mathbf{h}_{i}^{(t-1)}$ 和 $\mathbf{x}_i^{(t-1)}$ 提取的历史信息</p></li></ul><p>参数：</p><ul><li><p>$\mathbf{W}_{hh}\in \R^{h\times h}$ ，$\mathbf{W}_{hx}\in \R^{h\times V}$ ：保存状态间的时序信息</p></li><li><p>$\mathbf{W}_{qh}\in \R^{q\times h}$ ：保存隐状态与输出的关系</p></li><li>$\mathbf{b}_h\in\R^{1\times h}，\mathbf{b}_q\in\R^{1\times q}$</li><li>即使在不同的时间步，循环神经网络也总是使用这些模型参数。 因此，<strong>循环神经网络的参数开销不会随着时间步的增加而增加</strong></li></ul><h4 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h4><p><img src="/posts/1556323108/image-20240404202223945.png" alt="image-20240404202223945"></p><p>按批量计算时，正向传播</p><script type="math/tex;mode=display">\mathbf{H}^{(t)}=\mathbf{H}^{(t-1)}\mathbf{W}_{hh}^T+\mathbf{X}^{(t-1)}\mathbf{W}_{hx}^T+\mathbf{b}_h</script><p>相当于</p><script type="math/tex;mode=display">\begin{bmatrix}
\mathbf{h}_1^{(t)}=\mathbf{h}_1^{(t-1)}\mathbf{W}_{hh}^T+\mathbf{x}^{(t-1)}_1\mathbf{W}_{hx}^T+\mathbf{b}_h\\
\vdots\\
\mathbf{h}_{B}^{(t)}=\mathbf{h}_{B}^{(t-1)}\mathbf{W}_{hh}^T+\mathbf{x}^{(t-1)}_{B}\mathbf{W}_{hx}^T+\mathbf{b}_h
\end{bmatrix}</script><p>在真正的可运行代码中，执行的是矩阵乘法，而不是用一个循环嵌套多个向量依次计算</p><p><strong>一次正向传播，相当于整个批量的时序状态前进了一个时刻</strong></p><p>当前时间步的输出为</p><script type="math/tex;mode=display">\mathbf{o}_i^{(t)}=\phi\left(\mathbf{h}_i^{(t)}\right)\mathbf{W}_{qh}^T+\mathbf{b}_{q}\quad ,\phi(\cdot) 为非线性函数</script><p>一个批量在 $t$ 时刻的预测输出为</p><script type="math/tex;mode=display">\mathbf{O}^{(t)}=\phi\left(\mathbf{H}^{(t)}\right)\mathbf{W}_{qh}^T+\mathbf{b}_{q}\quad,\phi(\cdot) 为非线性函数</script><p>隐变量 $\mathbf{h}^{(t)}_i$ 记录一个子序列在当前时间步对历史信息的总结，相当于神经网络当前的状态，因此也被称为隐状态</p><p>由于当前时间步，隐状态使用的定义与前一个时间步中使用的定义相同，所以在一个时间窗口 $T$ 内，隐状态的更新是循环的。因此，隐状态更新是循环计算的层 $\mathbf{H}\in \R^{T\times B\times V}$ 称为 <strong>循环层</strong></p><blockquote><p>在真正的GPU计算中，$\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}$ 的计算，相当于 $\mathbf{X}_t$和$\mathbf{H}_{t-1}$ 的拼接 与 $\mathbf{W}_{xh}$ 和 $\mathbf{W}_{hh}$ 的拼接的矩阵乘法</p><ul><li>大矩阵计算的多线程优化会更好，同样的计算量，多次小矩阵乘法的开销大于一次大矩阵乘法的开销。Python有开销，GPU核也有开销</li></ul></blockquote><h4 id="预测输出"><a href="#预测输出" class="headerlink" title="预测输出"></a>预测输出</h4><p>若 <em>one-hot</em> 向量 $\mathbf{y}^{(t)}_i=\mathbf{x}_i^{(t)}$ 的第 $\nu_i$ 个为1，即 $t$ 时刻的 <strong>真实输出</strong> 在词表中的数字索引为 $\nu_i$</p><p>时序任务可以理解为一个分类任务，每个时刻的输出 $\hat{\mathbf{y}}^{(t)}_i$ 相当于 $V$ 分类问题</p><script type="math/tex;mode=display">\begin{aligned}
\hat{\mathbf{y}}^{(t)}_i\xlongequal{g(\cdot)为分类器}&g\left(\mathbf{o}^{(t)}_i\right)\\
=&softmax\left(\mathbf{o}^{(t)}_i\right)
\end{aligned}</script><ul><li>可视化输出，即 $\text{vocab.idx_to_token}\left(\arg \max\limits_{v} softmax\left(\mathbf{o}^{(t)}_i\right)^{(v)}\right),v\in [0,V-1]$</li></ul><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>预测正确即分为正确类的概率</p><script type="math/tex;mode=display">p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)=\left[\hat{\mathbf{y}}^{(t)}_i\right]^{(\nu_i)}=\left[g\left(\mathbf{o}^{(t)}_i\right)\right]^{(\nu_i)}\in[0,1]</script><p>对于第 $i$ 个子序列在 $t$ 时刻预测输出的交叉熵损失为</p><script type="math/tex;mode=display">\begin{aligned}
\ell\left(\mathbf{y}_i^{(t)},\hat{\mathbf{y}}^{(t)}_i\right)=-\mathbf{y}_i^{(t)}\cdot\log\hat{\mathbf{y}}^{(t)}_i&=-\sum\limits_{v=1}^V\left[\mathbf{y}_i^{(t)}\right]^{(v)}\log\left[\hat{\mathbf{y}}^{(t)}_i\right]^{(v)}\\
&=-\left[\mathbf{y}_i^{(t)}\right]^{(\nu_i)}\log\left[\hat{\mathbf{y}}^{(t)}_i\right]^{(\nu_i)}=-\log\left[\hat{\mathbf{y}}^{(t)}_i\right]^{(\nu_i)}\\
&=-\log\left[g\left(\mathbf{o}^{(t)}_i\right)\right]^{(\nu_i)},\nu_i=\arg\max\limits_{v}\left[\mathbf{y}_i^{(t)}\right]^{(v)}\\
&=-\log p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)\triangleq \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)
\end{aligned}</script><p>时间序列模型的一个样本 $\mathbf{x}_i$ 为时间步长为 $T$ 的子序列，一个样本需要进行 $T$ 次标签的预测输出，即 $T$ 次V分类，所以整个时序模型的好坏用平均交叉熵衡量</p><script type="math/tex;mode=display">\begin{aligned}
\pi_i&=\frac{1}{T}\sum\limits_{t=1}^T-\log p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)\\
&=\frac{1}{T}\sum\limits_{t=1}^T\ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)
\end{aligned}</script><p>在语言模型中，将指数化平均交叉熵作为衡量指标</p><script type="math/tex;mode=display">\exp(\pi_i)=e^{\pi_i}=e^{\frac{1}{T}\sum\limits_{t=1}^T\ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}=e^{\frac{1}{T}\sum\limits_{t=1}^T-\log p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)}</script><ul><li><p>放大平均交叉熵</p></li><li><p>1表示完美，预测输出确信是一个值，对应 $p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)=1$</p><p>若 $\exp(\pi_i)&gt;1$ ，对应 $\sum-\log p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)&gt;0\Rightarrow \sum\log p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right)&lt;0$ ，即有多次预测 $0&lt;p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}_i^{(t-1)},\ldots,\right),p\left(\mathbf{x}_i^{(t’)}\vert \mathbf{x}_i^{(t’-1)},\ldots,\right)&lt;1$</p></li></ul><p>一个批量 $\vert \mathcal{B}\vert$ 个子序列的损失函数为</p><script type="math/tex;mode=display">\begin{aligned}
L&=\ell\left(\mathbf{Y},\hat{\mathbf{Y}}\right)=\frac{1}{B}\sum\limits_{i=1}^{B}\exp(\pi_i)=
\frac{1}{B}\sum\limits_{i=1}^{B}\exp\left(\frac{1}{T}\sum\limits_{t=1}^T\ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)\right)\\
&=\frac{1}{B}\sum\limits_{i=1}^{B}\exp\left(\frac{1}{T}\sum\limits_{t=1}^T-\log p\left(\mathbf{x}_i^{(t)}\vert \mathbf{x}^{(t-1)}_i,\cdots,\right)\right)
\end{aligned}</script><h4 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h4><p><img src="/posts/1556323108/image-20240405154839479.png" alt="image-20240405154839479"></p><p>使用梯度下降法进行参数更新时</p><script type="math/tex;mode=display">\mathbf{W}\leftarrow \mathbf{W}-\eta\frac{\partial L}{\partial \mathbf{W}}</script><p>因此，关键是要计算损失函数对参数的梯度</p><script type="math/tex;mode=display">\frac{\partial L}{\partial \mathbf{W}_{hh}},\frac{\partial L}{\partial \mathbf{W}_{hx}},\frac{\partial L}{\partial \mathbf{W}_{qh}}</script><script type="math/tex;mode=display">\frac{\partial L}{\partial \mathbf{W}}=
\frac{1}{B}\sum\limits_{i=1}^{B}\exp\left(\frac{1}{T}\sum\limits_{t=1}^T\ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)\right)\frac{1}{T}\sum\limits_{t=1}^T\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}}</script><h5 id="输出层参数学习"><a href="#输出层参数学习" class="headerlink" title="输出层参数学习"></a>输出层参数学习</h5><p>对于 $\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_{qh}}$ ，使用链式法则</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_{qh}}&=\frac{\partial \ell}{\partial \mathbf{o}^{(t)}_i}\frac{\partial \mathbf{o}^{(t)}_i}{\partial \mathbf{W}_{qh}}=\frac{\partial \ell}{\partial \mathbf{o}^{(t)}_i}\frac{\partial \left(\phi\left(\mathbf{h}_{i}^{(t)}\right)\mathbf{W}_{qh}^T+\mathbf{b}_q\right)}{\partial \mathbf{W}_{qh}}\\
&=\frac{\partial \ell}{\partial \mathbf{o}^{(t)}_i}\phi\left(\mathbf{h}_{i}^{(t)}\right)\in \R^{q\times h}
\end{aligned}</script><h5 id="循环层参数学习"><a href="#循环层参数学习" class="headerlink" title="循环层参数学习"></a>循环层参数学习</h5><p>对于 $\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}$ ，使用链式法则</p><script type="math/tex;mode=display">\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}</script><h6 id="计算-frac-partial-mathbf-h-i-t-partial-mathbf-W-h"><a href="#计算-frac-partial-mathbf-h-i-t-partial-mathbf-W-h" class="headerlink" title="计算 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$"></a>计算 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$</h6><ul><li><p>对于 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}\in \R^{h\times h\times V}$ ，由于 $\mathbf{h}_i^{(t-1)}$ 与 $\mathbf{x}_i^{(t-1)}\mathbf{W}_{hx}^T$ 都是关于 $\mathbf{W}_{hx}$ 的复合函数，所以</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}&=\frac{\partial }{\partial \mathbf{W}_{hx}}\left(\mathbf{h}_i^{(t-1)}\mathbf{W}_{hh}^T+\mathbf{x}_i^{(t-1)}\mathbf{W}_{hx}^T+\mathbf{b}_h\right)\\
&=\frac{\partial f(t)}{\partial \mathbf{W}_{hx}}+\frac{\partial f(t)}{\partial \mathbf{h}_{i}^{(t-1)}}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{hx}}\\
&=\mathbf{x}_i^{(t-1)}+\mathbf{W}_{hh}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{hx}}
\end{aligned}</script></li><li><p>对于 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}$ ，由于 $\mathbf{h}_i^{(t)}$ 的正向传播不仅依赖于 $\mathbf{W}_{hh}$ ，而且循环依赖于 $\mathbf{h}_i^{(t-1)}$ ，所以</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}&=\frac{\partial }{\partial \mathbf{W}_{hh}}\left(\mathbf{h}_i^{(t-1)}\mathbf{W}_{hh}^T+\mathbf{x}_i^{(t-1)}\mathbf{W}_{hx}^T+\mathbf{b}_h\right)\\
&=\frac{\partial f(t)}{\partial \mathbf{W}_{hh}}+\frac{\partial f(t)}{\partial \mathbf{h}_i^{(t-1)}}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{hh}}\\
&=\mathbf{h}_i^{(t-1)}+\mathbf{W}_{hh}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{hh}}
\end{aligned}</script></li></ul><p>因此，可将 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}$ 与 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}$ 统一表示为 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}=\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\frac{\partial f(t)}{\partial \mathbf{h}_i^{(t-1)}}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{h}}$</p><ul><li>若序列 $\{a_t\},\{b_t\},\{c_t\}$ 满足 $a_t=b_t+c_ta_{t-1}$ ，则有<script type="math/tex;mode=display">a_{t}=b_{t}+\sum_{u=1}^{t-1}\left[\left(\prod_{v=u+1}^{t}c_{v}\right)b_{u}\right]</script></li></ul><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}&=\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\frac{\partial f(t)}{\partial \mathbf{h}_i^{(t-1)}}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{h}}\\
&=\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\sum_{u=1}^{t-1}\left[\left(\prod_{v=u+1}^{t}\frac{\partial f(v)}{\partial \mathbf{h}_i^{(v-1)}}\right)\frac{\partial f(u)}{\partial \mathbf{W}_{h}}\right]\\
&=\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\sum_{u=1}^{t-1}\left[\left(\prod_{v=u+1}^{t}\mathbf{W}_{hh}\right)\frac{\partial f(u)}{\partial \mathbf{W}_{h}}\right]\\
&=\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\sum_{u=1}^{t-1}\left[\mathbf{W}_{hh}^{t-u}\frac{\partial f(u)}{\partial \mathbf{W}_{h}}\right]\\
&=\sum\limits_{u=1}^{t}\left[\mathbf{W}_{hh}^{t-u}\frac{\partial f(u)}{\partial \mathbf{W}_h}\right]
\end{aligned}</script><p>虽然每个时间步长的 $\mathbf{W}_h$ 都相同，但当 $t$ 变得很长， $\mathbf{W}_{hh}^{t-u}$ 这个连乘链就会变得很长，造成数值不稳定问题</p><p><strong>时间步截断</strong></p><p>在 $\tau$ 步后截断，使得梯度求和终止于 $\frac{\partial f(t-\tau)}{\partial \mathbf{W}_h}$ ，即变为 $\mathbf{W}_{hh}^{t-\tau}$ 的连乘</p><p>在后续实现中对每个序列梯度的 <code>detach()</code> 就是实现这个方法。</p><p>实践中，效果好，虽然会导致该模型主要侧重于短期影响，但会将估计值偏向更简单和稳定的模型</p><p><strong>随机截断</strong></p><p>使用一个随机变量替换 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$ ，这个随机变量为序列 $\{\xi_t\}$ ，预定义 $\alpha\in [0,1]$ ，且 $p(\xi_t=0)=1-\alpha_t$ 且 $p\left(\xi_t=\alpha_t^{(-1)}\right)=\alpha_t$ ，因此 $E[\xi_t]=1$</p><p>则梯度 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$ 被代替为</p><script type="math/tex;mode=display">z_t=\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\xi_t\frac{\partial f(t)}{\partial \mathbf{h}_i^{(t-1)}}\frac{\partial \mathbf{h}_i^{(t-1)}}{\partial \mathbf{W}_{h}}</script><p>有 $E[z_t]=\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$ ，每当 $\xi=0$ 时，循环层梯度终止于 $t$ 这个时间步。随机截断的好处在于长序列很少出现。</p><p>实际中，时间步截断比随机截断效果好：</p><ul><li>在对过去若干个时间步经过反向传播后， 观测结果足以捕获实际的依赖关系</li><li>增加的方差抵消了时间步数越多梯度越精确的事实</li><li>我们真正想要的是只有短范围交互的模型</li></ul><h6 id="计算-frac-partial-ell-partial-mathbf-h-i-t"><a href="#计算-frac-partial-ell-partial-mathbf-h-i-t" class="headerlink" title="计算 $\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}$"></a>计算 $\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}$</h6><p><strong>对于最后一个时间步</strong> 的输出 $\mathbf{o}_i^{(T)}$ ，依赖于相应的隐变量 $\mathbf{h}_i^{(T)}$</p><script type="math/tex;mode=display">\mathbf{o}_i^{(T)}=\phi\left(\mathbf{h}_i^{(T)}\right)\mathbf{W}^{T}_{qh}+\mathbf{b}_q</script><p>则</p><script type="math/tex;mode=display">\frac{\partial \ell}{\partial \mathbf{h}_i^{(T)}}=\frac{\partial \ell}{\partial \mathbf{o}_i^{(T)}}\frac{\partial \mathbf{o}_i^{(T)}}{\partial \mathbf{h}_i^{(T)}}=\frac{\partial \ell}{\partial \mathbf{o}_i^{(T)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(T)}\right)\right)</script><p><strong>对于任意时间步 $t&lt;T$</strong>，由于损失函数 $\ell$ 通过 $\mathbf{h}_i^{(t+1)}$ 和 $\mathbf{o}_i^{(t)}$ 依赖于 $\mathbf{h}_i^{(t)}$ 由链式法则 $\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}\in \R^{1\times h}$</p><p><img src="/posts/1556323108/image-20240406012341824.png" alt="image-20240406012341824"></p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}&=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t+1)}}\frac{\partial \mathbf{h}_i^{(t+1)}}{\partial \mathbf{h}_i^{(t)}}+\frac{\partial \ell}{\partial \mathbf{o}_i^{(t)}}\frac{\partial \mathbf{o}_i^{(t)}}{\partial \mathbf{h}_i^{(t)}}\\
&=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t+1)}}\mathbf{W}_{hh}+\frac{\partial \ell}{\partial \mathbf{o}_i^{(t)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(t)}\right)\right)
\end{aligned}</script><p>对于任意时间步 $1\le t\le T$ 展开递归式</p><script type="math/tex;mode=display">\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}=\sum\limits_{u=t}^T\frac{\partial \ell}{\partial \mathbf{o}_i^{(T+t-u)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(t)}\right)\right)\left(\mathbf{W}_{hh}\right)^{T-u}</script><h6 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h6><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}&=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}\\
&=\left(\sum\limits_{u=t}^T\frac{\partial \ell}{\partial \mathbf{o}_i^{(T+t-u)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(t)}\right)\right)\left(\mathbf{W}_{hh}\right)^{T-u}\right)\left(\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\sum_{u=1}^{t-1}\left[\mathbf{W}_{hh}^{t-u} \frac{\partial f(u)}{\partial \mathbf{W}_{h}}\right]\right)
\end{aligned}</script><p>两处连乘，会使得 $\mathbf{W}_{hh}$ 存在很大的幂，小于1的值会消失，大于1的值将发散。这种数值不稳定问题表现为梯度消失或梯度爆炸。</p><p>当对 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$ 采用0步截断后，</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_{hh}}&=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}\\
&=\left(\sum\limits_{u=t}^T\frac{\partial \ell}{\partial \mathbf{o}_i^{(T+t-u)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(t)}\right)\right)\left(\mathbf{W}_{hh}\right)^{T-u}\right)\mathbf{h}_i^{(t-1)}\\
\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_{hx}}&=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}\\
&=\left(\sum\limits_{u=t}^T\frac{\partial \ell}{\partial \mathbf{o}_i^{(T+t-u)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(t)}\right)\right)\left(\mathbf{W}_{hh}\right)^{T-u}\right)\mathbf{x}_i^{(t-1)}\\
\end{aligned}</script><h5 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h5><p>通过时间反向传播BPTT实际上是误差反向传播在RNN的一个特定应用</p><p>将RNN的计算图一次展开一个时间步，以获得模型变量和参数之间的依赖关系。基于链式法则，应用反向传播来计算和存储梯度。</p><p>由于一个子序列可能相当长，因此参数间的依赖关系也相当长。时序窗口为1000的子序列，其第一个词元可能会对最后位置的词元产生很大影响，且还需要超过1000个矩阵的乘积才能得到梯度 $\mathbf{g}$</p><h6 id="数值稳定性解决"><a href="#数值稳定性解决" class="headerlink" title="数值稳定性解决"></a>数值稳定性解决</h6><p>在长为 $T$ 的时间窗口上，迭代计算 $T$ 个时间步上的梯度，在反向传播中会产生长度为 $O(T)$ 的矩阵乘法链。当 $T$ 较大，会造成数值不稳定问题。</p><p>一般来说，使用梯度下降法从负梯度方向更新参数 $\mathbf{W}$ ，即 $\mathbf{W}\leftarrow \mathbf{W}-\eta \mathbf{g}$ 。若进一步假设函数 $f(x)=x-\eta g$ 有比较好的性质，在常数 $L$ 下是利普希茨连续的，对于任意的</p><script type="math/tex;mode=display">\vert f(\mathbf{W}_1)-f(\mathbf{W}_2)\vert\le L\Vert \mathbf{W}_1-\mathbf{W}_2\Vert</script><p>则通过 $\eta \mathbf{g}$ 更新参数向量时</p><script type="math/tex;mode=display">\vert f(\mathbf{W}_1)-f(\mathbf{W}_1-\eta\mathbf{g})\vert\le L\eta\Vert \mathbf{g}\Vert</script><p>即我们不会观察到梯度有超过 $L\eta\Vert \mathbf{g}\Vert$ 的变化。好的方面是：当优化方向错误是，变坏的程度小；坏的方面是限制了优化速度</p><p>若梯度很大时，优化算法可能无法收敛，可以通过降低学习率 $\eta$ 来解决。但大梯度只是小概率时间，则直接降低学习率会使训练速度变得很慢，也是不可取的。</p><p>一种替代方案是将 <strong>梯度裁剪</strong>，只有梯度的模超过一个阈值才进行投影，也能达到防止梯度爆炸，使目标函数收敛的目的。</p><p>梯度裁剪的方法是限制梯度连乘的模 $\Vert \mathbf{g}\Vert$ 不超过 $\theta$</p><script type="math/tex;mode=display">\mathbf{g}\leftarrow\min\left(1,\frac{\theta}{\Vert \mathbf{g}\Vert}\right)\mathbf{g}</script><ul><li>若计算一个子序列误差反向传播时，产生模为 $\Vert \mathbf{g}\Vert$ 的梯度 $\mathbf{g}$ ，若这些梯度的模大于 $\theta$ ，则导致 $\frac{\theta}{\Vert \mathbf{g}\Vert}&gt;1$ ，从而使得 $\min\left(1,\frac{\theta}{\Vert\mathbf{g}\Vert}\right)\mathbf{g}=\frac{\theta}{\Vert\mathbf{g}\Vert}\mathbf{g}$ ，即使裁剪后的梯度模为 $\theta$</li></ul><h5 id="误差正向传播"><a href="#误差正向传播" class="headerlink" title="误差正向传播"></a>误差正向传播</h5><p>Real-Time Recurrent Learning，RTRL：通过前向传播的方式来计算梯度</p><p>假设RNN第 $i$ 个子序列在 $t$ 时刻的预测输出状态为 $\mathbf{o}_i^{(t)}$</p><p><img src="/posts/1556323108/image-20240406144846478.png" alt="image-20240406144846478"></p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{o}_i^{(t+1)}&=\phi\left(\mathbf{h}_i^{(t+1)}\right)\mathbf{W}_{qh}^T+\mathbf{b}_q\\
&=\phi\left(\mathbf{h}_i^{(t)}\mathbf{W}_{hh}^T+\mathbf{x}_i^{(t)}\mathbf{W}_{hx}^T+\mathbf{b}_h\right)\mathbf{W}_{qh}^T+\mathbf{b}_q\\
&=\phi\left(\left(\mathbf{h}_i^{(t-1)}\mathbf{W}_{hh}^T+\mathbf{x}_i^{(t-1)}\mathbf{W}_{hx}^T+\mathbf{b}_h \right)\mathbf{W}_{hh}^T+\mathbf{x}_i^{(t)}\mathbf{W}_{hx}^T+\mathbf{b}_h\right)\mathbf{W}_{qh}^T+\mathbf{b}_q\\
&=\cdots
\end{aligned}</script><p>故有：</p><script type="math/tex;mode=display">\frac{\partial \mathbf{o}_i^{(t+1)}}{\partial \mathbf{W}_{h}}=\frac{\partial \mathbf{o}_i^{(t+1)}}{\partial \mathbf{h}_{i}^{(t+1)}}\frac{\partial \mathbf{h}_i^{(t+1)}}{\partial \mathbf{W}_{h}}=\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}\right)\right)\frac{\partial \mathbf{h}_i^{(t+1)}}{\partial \mathbf{W}_{h}}</script><ul><li><p>$\mathbf{h}_i^{(t+1)}$ 通过 $\mathbf{h}_i^{(t)}\mathbf{W}_{hh}^T$ 与 $\mathbf{h}_i^{(t)}$ 依赖于 $\mathbf{W}_{hh}$，故</p><script type="math/tex;mode=display">\frac{\partial \mathbf{h}_i^{(t+1)}}{\partial \mathbf{W}_{hh}}=\frac{\partial \left(\mathbf{h}_i^{(t)}\mathbf{W}_{hh}^T\right)}{\partial \mathbf{W}_{hh}}+\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}=\mathbf{h}_{i}^{(t)}+\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}</script><p>实时循环学习算法从第一个时刻开始，除了计算隐状态外，还需要依次向前计算偏导 $\frac{\partial \mathbf{h}_i^{(1)}}{\partial \mathbf{W}_{hh}},\frac{\partial \mathbf{h}_i^{(2)}}{\partial \mathbf{W}_{hh}},\cdots,\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hh}}$</p><p>当 $t+1=T$ 时，损失函数关于参数的偏导可得</p><script type="math/tex;mode=display">\frac{\partial \ell}{\partial \mathbf{W}_{hh}}=\frac{\partial \ell}{\partial \mathbf{o}_{i}^{(T)}}\frac{\partial \mathbf{o}_i^{(T)}}{\partial \mathbf{h}_i^{(T)}}\frac{\partial \mathbf{h}_i^{(T)}}{\partial \mathbf{W}_{hh}}</script></li><li><p>$\mathbf{h}_i^{(t+1)}$ 通过 $\mathbf{x}_i^{(t)}\mathbf{W}_{hx}^T$ 与 $\mathbf{x}_i^{(t)}$ 依赖于 $\mathbf{W}_{hx}$，故</p><script type="math/tex;mode=display">\frac{\partial \mathbf{h}_i^{(t+1)}}{\partial \mathbf{W}_{hx}}=\frac{\partial \left(\mathbf{x}_i^{(t)}\mathbf{W}_{hx}^T\right)}{\partial \mathbf{W}_{hx}}+\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}=\mathbf{x}_{i}^{(t)}+\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}</script><p>实时循环学习算法从第一个时刻开始，除了计算隐状态外，还需要依次向前计算偏导 $\frac{\partial \mathbf{h}_i^{(1)}}{\partial \mathbf{W}_{hx}},\frac{\partial \mathbf{h}_i^{(2)}}{\partial \mathbf{W}_{hx}},\cdots,\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_{hx}}$</p><p>当 $t+1=T$ 时，损失函数关于参数的偏导可得</p><script type="math/tex;mode=display">\frac{\partial \ell}{\partial \mathbf{W}_{hx}}=\frac{\partial \ell}{\partial \mathbf{o}_{i}^{(T)}}\frac{\partial \mathbf{o}_i^{(T)}}{\partial \mathbf{h}_i^{(T)}}\frac{\partial \mathbf{h}_i^{(T)}}{\partial \mathbf{W}_{hx}}</script></li></ul><h4 id="BTPP-amp-RTRL"><a href="#BTPP-amp-RTRL" class="headerlink" title="BTPP&amp;RTRL"></a>BTPP&amp;RTRL</h4><p>二者都是基于梯度下降的算法，分别通过前向模式和反向模式应用链式法则计算梯度</p><p>循环神经网络中，一般网络输出维度低于输入维度，BTPP的计算量会更小，但BPTT需要保存所有时刻的中间梯度，空间复杂度较高</p><p>RTRL算法不需要梯度回传，因此非常适合用于在线学习和无限序列的任务中</p><h4 id="RNN的长程依赖问题"><a href="#RNN的长程依赖问题" class="headerlink" title="RNN的长程依赖问题"></a>RNN的长程依赖问题</h4><p><img src="/posts/1556323108/image-20240406150835082.png" alt="image-20240406150835082"></p><script type="math/tex;mode=display">\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}=\left(\sum\limits_{u=t}^T\underbrace{\frac{\partial \ell}{\partial \mathbf{o}_i^{(T+t-u)}}\mathbf{W}_{qh}\odot diag\left(\phi'\left(\mathbf{h}_i^{(t)}\right)\right)\left(\mathbf{W}_{hh}\right)^{T-u}}_{\delta_{T,u}}\right)\left(\frac{\partial f(t)}{\partial \mathbf{W}_{h}}+\sum_{u=1}^{t-1}\left[\mathbf{W}_{hh}^{t-u} \frac{\partial f(u)}{\partial \mathbf{W}_{h}}\right]\right)</script><p>当时序窗口 $T$ 很大时， 对于较早期的 $t$</p><p>若 $[\mathbf{W}_{hh}]_{ij}&gt;1$ ，当 $T-u\rightarrow \infty$ 时，$\delta_{T,u}\rightarrow \infty$</p><ul><li>当 $T-u$ 间隔比较大时，梯度也变得很大，造成系统不稳定，称为梯度爆炸问题</li></ul><p>若 $[\mathbf{W}_{hh}]_{ij}&lt;1$ ，当 $T-u\rightarrow \infty$ 时，$\delta_{T,u}\rightarrow 0$</p><ul><li><p>当 $T-u$ 间隔比较大时，梯度将变得很小，出现梯度消失问题</p></li><li><p>在循环神经网络中，梯度消失不是指 $\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}$ 消失，而是 $\delta_{T,u}\rightarrow 0$</p><script type="math/tex;mode=display">\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}=\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}\frac{\partial \mathbf{h}_i^{(T)}}{\partial \mathbf{W}_{hx}}=\left(\sum_{u=t}^T\delta_{T,u}\right)\frac{\partial \mathbf{h}_i^{(T)}}{\partial \mathbf{W}_{hx}}</script><p>$\frac{\partial \ell\left(\mathbf{o}^{(t)}_i,\mathbf{y}_i^{(t)}\right)}{\partial \mathbf{W}_h}$ 是求和得到的，为0的项不影响</p><p>即 <strong>参数 $\mathbf{W}_{h}$ 的更新主要靠时刻 $T$ 和$u$个相邻状态来更新</strong>，长距离的状态对参数 $\mathbf{W}_{h}$ 是没有影响的</p></li></ul><p>虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系，但由于梯度爆炸或梯度消失问题，实际上只能学习到短期的依赖关系</p><p>简单的神经网络很难建模长距离的依赖关系，称为 <strong>长程依赖问题</strong></p><h3 id="8-3-3-实现"><a href="#8-3-3-实现" class="headerlink" title="8.3.3 实现"></a>8.3.3 实现</h3><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line"><span class="comment"># batch_size 批量大小，num_steps：时序窗口</span></span><br><span class="line"><span class="comment"># 返回词表和语料集的批量迭代器</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p><code>train_iter</code> 在每次被遍历时，都会调用批量生成函数，即每次迭代都是重新生成的训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    X = F.one_hot(X.T, <span class="built_in">len</span>(vocab))</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>从迭代起中拿到的 $X\in \R^{B\times T}$ ，$X_{i,j}\in \R$ 为 <code>vocab</code> 中的数字索引。</p><p>在转换为 <em>one-hot</em> 向量之前，需要将其转置，变为 $X^T\in \R^{T\times B}$ 。一次矩阵乘法，可以让整个批量下的所有(词元)状态都前进一个时间步，确保两步之间的状态是连续的。</p><p>这个过程可以理解为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从迭代器中拿到的是(批量大小,时间步长) (2,5)</span></span><br><span class="line">X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 若词表长为28，将X转为 (时间步长,批量大小,词表大小)</span></span><br><span class="line">F.one_hot(X.T, <span class="number">28</span>).shape</span><br><span class="line"><span class="comment"># torch.Size([5, 2, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个参数：张量，张量表示待编码的内容</span></span><br><span class="line"><span class="comment"># 第二个参数：1*张量长度</span></span><br><span class="line">F.one_hot(torch.tensor([<span class="number">0</span>, <span class="number">2</span>]), <span class="built_in">len</span>(vocab))</span><br><span class="line"><span class="comment"># tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span></span><br><span class="line"><span class="comment">#          0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span></span><br><span class="line"><span class="comment">#          0, 0, 0, 0]])</span></span><br></pre></td></tr></table></figure><h4 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="comment"># 词表vocab中的每个状态用one-hot表示，需要vocab_size个元素的向量</span></span><br><span class="line">    <span class="comment">#  输入一个词，则相当于输入一个one-hot向量，所以输入为vocab_size个元素的向量</span></span><br><span class="line">    <span class="comment">#  输出是对输入在时序上的预测，也相当于一个输出一个one-hot向量</span></span><br><span class="line">    <span class="comment"># 自回归模型，所以num_inputs=num_outputs</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最简单的参数初始化，方差为0.01，均值为0的正态分布</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># num_hiddens：隐状态神经元数量</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数：隐变量到输出</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h4><p><strong>初始隐状态</strong></p><p>对于每个子序列的初始隐状态 $\mathbf{h}_i^{(0)}\in \R^{1\times h}$ ，是不能通过状态循环更新公式迭代，所以必须直接初始化</p><p>由于一个时间步长的状态是按批量更新的，且一个批量有 $B$ 个子序列，所以批量的初始隐状态为 $\mathbf{H}^{(0)}\in \R^{B\times h}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p><strong>正向传播</strong></p><p>整个批量子序列逐时间步更新隐状态 $\mathbf{H}\in \R^{T\times B\times V}$ ，此处非线性函数使用 $\tanh$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># inputs形状：(时间步长,批量大小,词表大小)</span></span><br><span class="line">    <span class="comment"># X的形状：(批量大小,词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> i,X <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        <span class="comment"># 预测输出Y的形状：(批量大小,词表大小)</span></span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">        <span class="comment">#print(i,&quot;: X形状-&quot;,X.shape,&quot;,Y形状-&quot;,Y.shape)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回预测输出和更新后的状态</span></span><br><span class="line">    <span class="comment">#  outputs是一个批量B个子序列T个时间步的预测输出 形状为 (时间步长,批量大小,词表大小)</span></span><br><span class="line">    <span class="comment">#  将outputs在0轴拼接后，变为(时间步长*批量大小,词表大小)</span></span><br><span class="line">    <span class="comment"># H是同批量所有子序列最后一个时间步的预测输出相应的隐状态</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><p>批量不同，意味着输入 $\mathbf{X}$ 不同，相应隐变量 $\mathbf{H}$ 也不同，一个批量正向传播过程中，隐变量 $\mathbf{H}$ 会更新 $T$ 次，而最后一个输入 $\mathbf{x}_i^{(T)}$ 的隐变量 $\mathbf{h}_i^{(T)}$ 也会被输出，当两个批量的子序列连续时，可以用其作为后一个批量的起始隐状态</p><p><strong>RNN模型封装</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>: <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span><br><span class="line"><span class="params">                 get_params, init_state, forward_fn</span>):</span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 此处X为批量大小*时间步数，即一个批量的子序列数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># 后续是按批量逐时间步进行正向传播和反向求导，</span></span><br><span class="line">        <span class="comment">#   当X转置变为 (时间步长,批量大小,词表大小)后，这个操作变为连续内存空间取值</span></span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line">    <span class="comment"># 对batch_size 个h0随机初始化 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">512</span></span><br><span class="line"><span class="comment"># len(vocab) one-hot向量的长度</span></span><br><span class="line"><span class="comment"># num_hiddens 隐状态神经元数量</span></span><br><span class="line"><span class="comment"># rnn 模型的前向传播函数 </span></span><br><span class="line"><span class="comment"># init_rnn_state 模型的 h_0 隐状态初始化</span></span><br><span class="line">net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,</span><br><span class="line">                      init_rnn_state, rnn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># X(批量大小,时间步长)=((2, 5))</span></span><br><span class="line">state = net.begin_state(X.shape[<span class="number">0</span>], d2l.try_gpu())</span><br><span class="line">Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class="line">Y.shape, new_state[<span class="number">0</span>].shape</span><br><span class="line"><span class="comment">#	(torch.Size([10, 28]), torch.Size([2, 512]))</span></span><br></pre></td></tr></table></figure><p>5个时间步长组成一个子序列，有2个子序列组成一个批量，隐状态神经元数量为512，则期望的输出为 $(T\times B,V)=(5\times 2,28)$ ，预测输出对应的隐状态 $H^{(T)}\in \R^{B\times V}$</p><h4 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h4><p><code>prefix</code> 为待预测的输入，需要使用 <code>prefix</code> 生成时序最后一个输出 <code>token</code> 的隐状态 $\mathbf{h}^{(T)}$ ，这个阶段称为预热</p><p>预热完成获取 <code>prefix</code> 的历史信息，才能对 <code>prefix</code> 进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 功能：遍历prefix做预热，将prefix的历史信息保存在隐状态state中，</span></span><br><span class="line"><span class="comment">#     基于prefix的最后一个token与其隐状态，才能开始对prefix生成指定数量的预测</span></span><br><span class="line"><span class="comment"># prefix：预测的前缀，相当于输入</span></span><br><span class="line"><span class="comment"># num_preds：需要生成多少个预测词</span></span><br><span class="line"><span class="comment"># net：调用那个模型</span></span><br><span class="line"><span class="comment"># vocab：词表</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始隐状态</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    <span class="comment"># 获取前缀第一个字符 &#x27;t&#x27; 在词表中对应的数字索引 [[3]]，放入outputs</span></span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取outputs中的最后一个，将最近预测的词作为下一时刻的输入</span></span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 从第二个字符 / 词元 开始输入网络</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:  <span class="comment"># 预热期</span></span><br><span class="line">        <span class="comment"># _：预热期不接收输出</span></span><br><span class="line">        <span class="comment"># state:保存本次预测的隐藏层h，h保存的时序信息可用作下次起始隐藏层</span></span><br><span class="line">        <span class="comment"># 预热期遍历prefix，将prefix的时序信息放入隐藏状态state中</span></span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        <span class="comment"># outputs中将逐字符追加 prefix，即真实的输出</span></span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):  <span class="comment"># 预测num_preds步</span></span><br><span class="line">        <span class="comment"># 用最后一个预测输出作为下一个时刻的输入，即outputs的最后一个</span></span><br><span class="line">        <span class="comment">#  用包含历史信息的最后一个隐状态作为初始状态，</span></span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># len(vocab)=len(y[i]) </span></span><br><span class="line">        <span class="comment">#  y[i]表示这一批量中第i个子序列的预测输出，</span></span><br><span class="line">        <span class="comment">#  其中，每个元素y[i][j]表示这个子序列的预测输出属于vocab[j]的可能性</span></span><br><span class="line">        <span class="comment"># 将预测输出转为索引值放入outputs中</span></span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于&#x27;time traveller &#x27;，使用net对其生成10个预测输出</span></span><br><span class="line">predict_ch8(<span class="string">&#x27;time traveller &#x27;</span>, <span class="number">10</span>, net, vocab, d2l.try_gpu())</span><br></pre></td></tr></table></figure><h4 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_clipping</span>(<span class="params">net, theta</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        <span class="comment"># if p.requires_grad 将需要梯度的参数取出，即T层W和b中所有可被训练的参数</span></span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    <span class="comment"># 将所有层参数的梯度的模</span></span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p><strong>一个epoch</strong></p><p>序列数据的不同采样方式会导致隐状态初始化的区别</p><ul><li><p>当使用顺序分区，即下一个批量数据的第 $i$ 个子序列与当前批量的第 $i$ 个子序列相邻，因此当前批量的最后一个 <code>token</code> 的隐状态将用于初始化下一个批量子序列第一个 <code>token</code> 的隐状态。</p><p>同时为了不增加梯度计算的复杂度，在处理每个批量前，先分离梯度，使隐状态的梯度计算总是在一个批量的时间窗口内</p></li><li><p>当使用随机采样时，每个子序列都是随机位置抽取的，所以需要为每个子序列初始化隐状态</p></li></ul><p>更新模型参数前，需要裁剪梯度，即使训练过程中某个点发生梯度爆炸，模型也不会发散</p><p>用困惑度来评价模型，确保不同长度的序列有可比性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和,词元数量</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 在第一次迭代或使用随机抽样时初始化隐状态</span></span><br><span class="line">            <span class="comment">#  若多个批量的子序列不是连续的，</span></span><br><span class="line">            <span class="comment">#    则上个批量最后一个预测输出的隐状态值不应该用作当前批量隐状态的初始化</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 若是批量是连续采样，且不是第一个批量，则将保留上个批量的子序列最后一个预测输出的隐状态</span></span><br><span class="line">            <span class="comment">#  将这个隐状态用于当前批量子序列的隐状态初始化时，将其梯度detach()</span></span><br><span class="line">            <span class="comment">#   因为当梯度反传时，不再需要计算这个隐状态之前的梯度</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># state对于nn.GRU是个张量</span></span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        <span class="comment"># Y[i]一个批量中第i个子序列的真实输出序列，Y：批量大小*时间步长</span></span><br><span class="line">        <span class="comment">#  Y.T将真实的输出序列与X.T同步 时间步长*批量</span></span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        <span class="comment"># 单纯从损失函数的视角看，RNN也是一个多分类问题，</span></span><br><span class="line">        <span class="comment">#  只不过类别数变为 len(vocab)，一待分类的数据量为 时间步长*批量大小</span></span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 因为已经调用了mean函数</span></span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="comment"># 若不加exp，metric[0] / metric[1]为平均交叉熵损失，</span></span><br><span class="line">    <span class="comment">#   exp(metric[0] / metric[1])为困惑度</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure><p><strong>多轮epoch</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span><br><span class="line"><span class="params">              use_random_iter=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 每个epoch调用一次数据迭代器生成函数，相当于</span></span><br><span class="line">        <span class="comment"># 每个epoch都是对corpus的重新采样，迭代器会被放入同一块内存，即train_iter中</span></span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="comment"># 每10个epoch输出一次</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 困惑度为1，相当于将整本书都记住了</span></span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240405013034969.png" alt="image-20240405013034969"></p><p><strong>随机采样的结果</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 子序列随机采样组成批量，</span></span><br><span class="line"><span class="comment">#   每个小批量的初始隐藏状态都是随机的，且每次只看一个短的子序列，使训练的随机性更高</span></span><br><span class="line"><span class="comment">#   训练时间变长，训练结果随机性更大，困惑度变大</span></span><br><span class="line">net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,</span><br><span class="line">                      init_rnn_state, rnn)</span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),</span><br><span class="line">          use_random_iter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240405013044736.png" alt="image-20240405013044736"></p><h3 id="8-3-4-简洁实现"><a href="#8-3-4-简洁实现" class="headerlink" title="8.3.4 简洁实现"></a>8.3.4 简洁实现</h3><h4 id="导入数据-1"><a href="#导入数据-1" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="模型定义-1"><a href="#模型定义-1" class="headerlink" title="模型定义"></a>模型定义</h4><p><strong>定义一个隐状态层</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表大小/输出分类数，循环层神经元的数量</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure><p>初始化隐状态，形状是 $(隐状态层数，批量大小，隐状态神经元数)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">state.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, 32, 256])</span></span><br></pre></td></tr></table></figure><ul><li><p>通过一个隐状态和一个输入，就可以更新隐状态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">Y.shape, state_new.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y：时间步长*批量大小*隐藏层神经元数量</span></span><br><span class="line"><span class="comment"># 并没有做cat，若cat，则Y形状为(时间步长*批量大小,隐藏层神经元数量)</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>RNN模型定义</strong></p><p>与原实现不同的在于，循环层 <code>nn.RNN()</code> 的输出只是循环层的隐状态张量，要得到预测输出，还需要一个 <code>nn.linear</code> 的预测输出层 <code>nn.Linear(self.num_hiddens, self.vocab_size)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="comment"># 定义输出层，即 Y=tanh(hW_&#123;hq&#125;)+b_q</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Y.shape输出的是 (时间步长,批量大小,词表大小) ，Y.shape[-1]为词表大小</span></span><br><span class="line">        <span class="comment">#  全连接输出层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># 完成隐变量的初始化</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line">                        batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure><h4 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = d2l.try_gpu()</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">d2l.predict_ch8(<span class="string">&#x27;time traveller&#x27;</span>, <span class="number">10</span>, net, vocab, device)</span><br></pre></td></tr></table></figure><p><img src="/posts/1556323108/image-20240405014237128.png" alt="image-20240405014237128"></p><p>吴恩达 RNN，六二大人，李宏毅</p><p>预训练骨干网络：科研团队，YOLO等框架，</p><p>模型应用：个人，YOLO结合transformer解决图像分割、目标检测、语义分割等问题</p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/1556323108/" title="8.动手学深度学习-循环神经网络">https://amostian.github.io/posts/1556323108/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/2929352505/" rel="prev" title="9.动手学深度学习-计算性能"><i class="fa fa-chevron-left"></i> 9.动手学深度学习-计算性能</a></div><div class="post-nav-item"></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-text">8.1 序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-1-%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="nav-text">8.1.1 序列数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E4%BE%8B%E5%AD%90"><span class="nav-text">序列数据例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-2-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-text">8.1.2 序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE"><span class="nav-text">归纳偏置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%81%87%E8%AE%BE"><span class="nav-text">马尔科夫假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BD%9C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B"><span class="nav-text">潜变量模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-3-%E5%9F%BA%E4%BA%8E%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%81%87%E8%AE%BE%E7%9A%84%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-text">8.1.3 基于马尔科夫假设的自回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90"><span class="nav-text">序列数据生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90"><span class="nav-text">序列数据集生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-text">回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B"><span class="nav-text">预测</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8D%95%E6%AD%A5%E9%A2%84%E6%B5%8B%E8%A1%A8%E7%8E%B0"><span class="nav-text">模型单步预测表现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B%E8%A1%A8%E7%8E%B0"><span class="nav-text">多步预测表现</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B%E6%80%BB%E7%BB%93"><span class="nav-text">多步预测总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-4-%E6%94%B9%E8%BF%9B"><span class="nav-text">8.1.4 改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-text">8.2 语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-1-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">8.2.1 文本预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E6%96%87%E4%BB%B6%E5%B0%86%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%85%A5%E5%86%85%E5%AD%98"><span class="nav-text">从文件将数据集读入内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%86%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%86%E5%88%86%E4%B8%BA%E8%AF%8D%E5%85%83"><span class="nav-text">将字符串拆分为词元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E8%AF%8D%E8%A1%A8"><span class="nav-text">构建词表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%81%E8%A3%85"><span class="nav-text">封装</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-2-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-text">8.2.2 语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E6%95%B0%E5%BB%BA%E6%A8%A1"><span class="nav-text">计数建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%BB%BA%E6%A8%A1"><span class="nav-text">马尔科夫建模</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E8%AF%AD%E6%B3%95%E4%B8%8E%E8%AF%8D%E9%A2%91%E6%98%AF%E5%90%A6%E6%BB%A1%E8%B6%B3%E5%8F%8C%E5%AF%B9%E6%95%B0%E6%9B%B2%E7%BA%BF"><span class="nav-text">多元语法与词频是否满足双对数曲线</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-3-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">8.2.3 数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-text">实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%81%E8%A3%85%E8%BF%94%E5%9B%9E%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="nav-text">封装返回数据集迭代器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-RNN"><span class="nav-text">8.3 RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">8.3.1 基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B"><span class="nav-text">隐变量模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-text">隐变量模型的理解</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN%E7%9A%84%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86"><span class="nav-text">RNN的通用近似定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-text">应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-2-RNN%E6%A8%A1%E5%9E%8B"><span class="nav-text">8.3.2 RNN模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%8D%E5%85%83-%E7%8A%B6%E6%80%81%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="nav-text">(词元)状态的表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">训练数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E8%BE%93%E5%87%BA"><span class="nav-text">预测输出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-text">参数学习</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-text">输出层参数学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-text">循环层参数学习</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97-frac-partial-mathbf-h-i-t-partial-mathbf-W-h"><span class="nav-text">计算 $\frac{\partial \mathbf{h}_i^{(t)}}{\partial \mathbf{W}_h}$</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97-frac-partial-ell-partial-mathbf-h-i-t"><span class="nav-text">计算 $\frac{\partial \ell}{\partial \mathbf{h}_i^{(t)}}$</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%88%E5%B9%B6"><span class="nav-text">合并</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">误差反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E8%A7%A3%E5%86%B3"><span class="nav-text">数值稳定性解决</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">误差正向传播</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BTPP-amp-RTRL"><span class="nav-text">BTPP&amp;RTRL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN%E7%9A%84%E9%95%BF%E7%A8%8B%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98"><span class="nav-text">RNN的长程依赖问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-3-%E5%AE%9E%E7%8E%B0"><span class="nav-text">8.3.3 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-text">导入数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-text">初始化模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B-1"><span class="nav-text">预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA"><span class="nav-text">梯度裁剪</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-4-%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-text">8.3.4 简洁实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE-1"><span class="nav-text">导入数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89-1"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="nav-text">训练与预测</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">363</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">58</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">74</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">879.2k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">36:47</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>