<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="5-基于策略梯度的RL"><meta property="og:url" content="https://amostian.github.io/posts/4136854086/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240307100321679.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240312103943638.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240312104729196.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240312104754749.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240312104820120.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240312105023700.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240307164637524.png"><meta property="og:image" content="https://amostian.github.io/posts/4136854086/image-20240512000149033.png"><meta property="article:published_time" content="2024-03-07T01:44:48.000Z"><meta property="article:modified_time" content="2024-03-12T09:34:01.000Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="强化学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/4136854086/image-20240307100321679.png"><link rel="canonical" href="https://amostian.github.io/posts/4136854086/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>5-基于策略梯度的RL | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">68</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">83</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">236</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/4136854086/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">5-基于策略梯度的RL</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-03-07 09:44:48" itemprop="dateCreated datePublished" datetime="2024-03-07T09:44:48+08:00">2024-03-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-03-12 17:34:01" itemprop="dateModified" datetime="2024-03-12T17:34:01+08:00">2024-03-12</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">10.6k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>28 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><ul><li>策略梯度<ul><li>最优梯度的度量</li><li>度量的梯度</li></ul></li><li>基于蒙特卡洛的策略梯度——REINFORCE<ul><li>梯度上升算法</li></ul></li><li>AC方法<ul><li>QAC</li><li>A2C：通过引入偏置量减少估计的方差</li><li>off-policy AC：将on-policy方法转换为 off-policy方法<ul><li>重要性采样</li></ul></li><li>DPG：随机策略变为确定性策略</li></ul></li></ul><h2 id="5-1-策略梯度"><a href="#5-1-策略梯度" class="headerlink" title="5.1 策略梯度"></a>5.1 策略梯度</h2><p>之前的方法以策略为核心，基于价值产生策略，并用表格表示策略</p><blockquote><p><strong>策略梯度</strong> (policy gradient) 是基于策略的方法，用策略函数近似策略</p><p><strong>利用机器学习模型的泛化能力，将已知状态上的策略泛化到未知状态上的策略</strong></p></blockquote><p>目标函数是策略的函数，通过优化目标函数直接获取最优策略</p><ul><li>A-C方法，实际上是将策略梯度与价值函数结合的方法</li></ul><h3 id="5-1-1-策略的表示"><a href="#5-1-1-策略的表示" class="headerlink" title="5.1.1 策略的表示"></a>5.1.1 策略的表示</h3><h4 id="表格型策略"><a href="#表格型策略" class="headerlink" title="表格型策略"></a>表格型策略</h4><p>策略为在每个状态下 $s\in \mathcal{S}$ 采取每个动作 $a\in \mathcal{A}(s)$ 的可能性可以用表格 $\pi(a\vert s)$ 表示，在表格中使用 $(s,a)$ 获取每个策略</p><div class="table-container"><table><thead><tr><th></th><th>$a_1$</th><th>$a_2$</th><th>$a_3$</th><th>$a_4$</th><th>$a_5$</th></tr></thead><tbody><tr><td>$s_1$</td><td>$\pi(a_1\vert s_1)$</td><td>$\pi(a_2\vert s_1)$</td><td>$\pi(a_3\vert s_1)$</td><td>$\pi(a_4\vert s_1)$</td><td>$\pi(a_5\vert s_1)$</td></tr><tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td></tr><tr><td>$s_9$</td><td>$\pi(a_1\vert s_9)$</td><td>$\pi(a_2\vert s_1)$</td><td>$\pi(a_3\vert s_1)$</td><td>$\pi(a_4\vert s_1)$</td><td>$\pi(a_5\vert s_1)$</td></tr></tbody></table></div><p>缺点：当状态空间很大或无穷、状态连续变化时，用表格表示策略就非常低效，体现在存储与泛化能力的缺陷上</p><h4 id="函数型策略"><a href="#函数型策略" class="headerlink" title="函数型策略"></a>函数型策略</h4><p>用带参数的函数拟合策略，也可以表示为 $\pi_{\theta}(a\vert s),\pi(a,s,\theta),\pi_{\theta}(a,s)$</p><script type="math/tex;mode=display">\pi(a\vert s,\theta),\theta\in \R^m</script><p><img src="/posts/4136854086/image-20240307100321679.png" alt="image-20240307100321679"></p><p>(a) 输入是状态和动作，输出是当前状态下采取这个动作使累积奖励最大化的可能性</p><p>(b) 输入是状态，输出是当前状态下采取各个动作使累积奖励最大化的可能性</p><p>最优的策略可以通过最优化度量标量的目标函数获取，这种方法称为 <strong>策略梯度</strong></p><ul><li>更适合处理高维度或连续的动作空间</li><li>具有更强大的泛化能力</li><li>具有更好的收敛性质和稳定性</li><li>能够学习随机策略</li></ul><p>缺点：</p><ul><li>通常会收敛到局部最优，而非全局最优，所以需要加噪音扰动</li><li>评价一个策略通常不高效，且方差较大（受数据分布影响大，策略一直变，所以数据分布一直变）</li></ul><h4 id="表格型VS函数型"><a href="#表格型VS函数型" class="headerlink" title="表格型VS函数型"></a>表格型VS函数型</h4><p>最优策略的定义：</p><ul><li>表格型：若一个策略 $\pi^<em>$ 使每个状态价值都是最大的，即 $\mathbf{V}_{\pi^</em>}(S)\ge\mathbf{V}_{\pi}(S),\forall \pi\in \Pi$</li><li>函数型：指定一个标量的策略度量指标目标函数，使目标函数最大化的策略就是最优策略</li></ul><p>获取一个动作的概率：</p><ul><li><p>表格型：通过 $(s,a)$ 可查表获取</p></li><li><p>函数型：计算策略函数的值 $\pi(a\vert s,\theta)$</p><p>参数每传播一次，都需要计算一次</p></li></ul><p>策略更新：</p><ul><li>表格型：直接修改 $\pi(a\vert s)$ 的值</li><li>函数型：通过修改策略函数的参数 $\theta$ 间接影响策略</li></ul><h4 id="策略梯度基本思路"><a href="#策略梯度基本思路" class="headerlink" title="策略梯度基本思路"></a>策略梯度基本思路</h4><p><strong>所有的策略度量指标 $J(\pi)$ 都是 $\pi$ 的函数，而 $\pi(a\vert s,\theta)$ 是关于参数 $\theta$ 的函数 ，不同的参数值 $\theta$ 会影响策略优劣的度量值，因此，通过最优化 $\theta$ 的值来最大化这些度量指标，可以找到最优的策略</strong></p><p>最优化算法为梯度上升法</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha\bigtriangledown_{\theta}J(\theta_{t})</script><ul><li>如何定义策略度量指标</li><li>如何计算策略度量指标的梯度</li></ul><h3 id="5-1-2-策略度量指标"><a href="#5-1-2-策略度量指标" class="headerlink" title="5.1.2 策略度量指标"></a>5.1.2 策略度量指标</h3><h4 id="平均状态价值"><a href="#平均状态价值" class="headerlink" title="平均状态价值"></a>平均状态价值</h4><script type="math/tex;mode=display">\begin{aligned}
\overline{V}_{\pi}&=\sum\limits_{s\in \mathcal{S}}d(s)V_{\pi}(s)=\mathbf{d}^T\mathbf{V}_{\pi}\\
&=E[V_{\pi}(S)]
\end{aligned}</script><ul><li>$\overline{V}_{\pi}$ 为在策略 $\pi$ 下加权平均的状态价值</li><li>$d(s)\ge 0,\sum\limits_{s\in \mathcal{S}}d(s)=1$ 是状态 $s$ 的权重，也可以理解为状态的概率分布 $S\sim d$</li><li>向量形式下，$\mathbf{d}=\begin{bmatrix}\vdots\\d(s)\\\vdots\end{bmatrix}\in \R^{\vert \mathcal{S}\vert},\mathbf{V}_{\pi}=\begin{bmatrix}\vdots\\V_{\pi}(s)\\\vdots\end{bmatrix}\in\R^{\vert \mathcal{S}\vert}$</li></ul><p>若求解加权平均，需要知道状态 $S$ 服从的分布 $d$</p><h5 id="d-与策略-pi-无关"><a href="#d-与策略-pi-无关" class="headerlink" title="$d$ 与策略 $\pi$ 无关"></a>$d$ 与策略 $\pi$ 无关</h5><blockquote><p>将分布 $d$ 记为 $d_0$ ，此时，平均状态价值记为 $\overline{V}_{\pi}^0$</p></blockquote><p>相对简单，因为度量指标函数的梯度计算相对简单</p><p>一种情况是每个状态都是同等重要的，因此将 $d_0$ 视为均匀分布，$d_0(s)=\frac{1}{\vert \mathcal{S}\vert}$</p><p>另一种情况是只关注特定的状态 $s_0$ ， 如在一些任务中所有回合都从相同的起始状态 $s_0$ 出发，实际上就是最大化从 $s_0$ 出发得到的回报，因此 $d_0(s_0)=1,d_0(s\neq s_0)=0\Rightarrow\overline{V}_{\pi}=V_{\pi}(s_0)$</p><h5 id="d-与策略-pi-有关"><a href="#d-与策略-pi-有关" class="headerlink" title="$d$ 与策略 $\pi$ 有关"></a>$d$ 与策略 $\pi$ 有关</h5><blockquote><p>状态的分布与策略 $\pi$ 有关，即 $d$ 是策略 $\pi$ 下的稳态分布</p></blockquote><ul><li>状态分布概率值是状态转移矩阵特征值为1的特征向量<script type="math/tex;mode=display">\mathbf{d}_{\pi}^TP_{\pi}=\mathbf{d}_{\pi}^T</script></li></ul><h4 id="平均单步立即奖励"><a href="#平均单步立即奖励" class="headerlink" title="平均单步立即奖励"></a>平均单步立即奖励</h4><script type="math/tex;mode=display">\overline{r}_{\pi}=\sum\limits_{s\in\mathcal{S}}d_{\pi}(s)r_{\pi}(s)=E[r_{\pi}(s)],S\sim d_{\pi}</script><p>其中，$r_{\pi}(s)=\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta)r(s,a)$ ，$r(s,a)=E[R\vert s,a]=\sum\limits_{r’}r’P(r’\vert s,a)$</p><h4 id="两种度量指标的关系"><a href="#两种度量指标的关系" class="headerlink" title="两种度量指标的关系"></a>两种度量指标的关系</h4><p>关于策略的度量指标，可以定义在折扣奖励 $\gamma \in [0,1)$ 下，也可以定义在非折扣奖励 $\gamma=1$ 下</p><p>对于 平均单步立即奖励 $\overline{r}_{\pi}$ ，只是对立即奖励的平均，不求回报所以也不需要考虑折扣因子，此时 $\gamma=1$</p><p>$\overline{r}_{\pi}$ 是一种非常短视的度量指标，只考虑立即奖励，相反，$\overline{V}_{\pi}$ 是一种相对远视的度量指标，考虑到所有步的总奖励，虽然二者不相等，但成正比</p><script type="math/tex;mode=display">\overline{r}_{\pi}=(1-\gamma)\overline{V}_{\pi}</script><p>一个达到最优，另一个也能达到极值</p><p>证明：贝尔曼方程 $V_{\pi}=r_{\pi}+\gamma P_{\pi}V_{\pi}$ ，两边同乘 $d^T_{\pi}$</p><script type="math/tex;mode=display">\overline{V}_{\pi}=\overline{r}_{\pi}+\gamma d_{\pi}^TP_{\pi}V_{\pi}=\overline{r}_{\pi}+\gamma d_{\pi}^TV_{\pi}=\overline{r}_{\pi}+\gamma \overline{V}_{\pi}</script><h4 id="等价定义"><a href="#等价定义" class="headerlink" title="等价定义"></a>等价定义</h4><p><img src="/posts/4136854086/image-20240312103943638.png" alt="image-20240312103943638"></p><h5 id="平均状态价值-1"><a href="#平均状态价值-1" class="headerlink" title="平均状态价值"></a>平均状态价值</h5><p>在策略 $\pi$ 下有从状态 $S_0$ 开始的轨迹 $(A_{0},R_{1},S_1,A_1,R_2,\cdots)$ ，满足 $A_{t}=\pi(S_t,\theta)$ ，$R_{t+1},S_{t+1}\sim P(R_{t+1},S_{t+1}\vert S_t,A_t)$</p><p>对于</p><script type="math/tex;mode=display">\begin{aligned}
J(\theta)&=E\left[\sum\limits_{t=0}^{\infty}\gamma^tR_{t+1}\right]\\
&=\sum\limits_{s\in\mathcal{S}}d_{\pi}(s)E\left[\sum\limits_{t=0}^{\infty}\gamma^tR_{t+1}\bigg\vert S_0=s\right]\\
&=\sum\limits_{s\in\mathcal{S}}d_{\pi}(s)V_{\pi}(s)\\
&=\overline{V}_{\pi}
\end{aligned}</script><h5 id="平均单步立即奖励-1"><a href="#平均单步立即奖励-1" class="headerlink" title="平均单步立即奖励"></a>平均单步立即奖励</h5><p>假设在策略 $\pi$ 下生成了起始状态为 $s_0$ 的轨迹，奖励序列为 $\{R_{t+1}\}_{t=0}^\infty$，这个轨迹的平均单步奖励为</p><script type="math/tex;mode=display">\begin{aligned}
\overline{r}_{\pi}&=\lim\limits_{n\rightarrow \infty}\frac{1}{n}E\left[R_{t+1}+R_{t+2}+\cdots+R_{t+n}\vert S_0=s_0\right]\\
&=\lim\limits_{n\rightarrow \infty}\frac{1}{n}E\left[\sum\limits_{t=0}^{n-1}R_{t+1}\bigg\vert S_0=s_0\right]\\
&\iff \lim\limits_{n\rightarrow \infty}\frac{1}{n}E\left[\sum\limits_{t=0}^{n-1}R_{t+1}\right]\\
&=\sum\limits_{s}d_{\pi}(s)r_{\pi}(s)
\end{aligned}</script><ul><li>起始状态 $s_0$ 是什么不重要</li></ul><p>证明：</p><p>首先，证明对于任意的起始状态 $s_0$ ，有第一个等号成立</p><script type="math/tex;mode=display">\begin{aligned}
\lim\limits_{n\rightarrow \infty}\frac{1}{n}E\left[\sum\limits_{t=0}^{n-1}R_{t+1}\vert S_0=s_0\right]&=\lim\limits_{n\rightarrow \infty}\frac{1}{n}\sum\limits_{t=0}^{n-1}E\left[R_{t+1}\vert S_0=s_0\right]\\
&\xlongequal{\mbox{Cesaro mean}}\lim\limits_{t\rightarrow \infty}E\left[R_{t+1}\vert S_0=s_0\right]
\end{aligned}</script><ul><li>纬洛平均：对于一个序列 $\{a_k\}_{k=1}^{\infty}$ 若其是收敛的，即 $\lim\limits_{k\rightarrow \infty}a_k$ 是存在的，则 $\lim\limits_{n\rightarrow \infty}\frac{1}{n}\sum\limits_{k=1}^{\infty}a_k=\lim\limits_{k\rightarrow \infty}a_k$ ，故序列 $\left\{\frac{1}{n}\sum\limits_{k=1}^{n}a_k\right\}_{n=1}^{\infty}$ 是一个收敛序列</li></ul><p>其次，证明等价</p><p>对于 $E\left[R_{t+1}\vert S_0=s_0\right]$ ，其全期望公式</p><script type="math/tex;mode=display">\begin{aligned}
E\left[R_{t+1}\vert S_0=s_0\right]&=\sum\limits_{s\in \mathcal{S}} E\left[R_{t+1}\vert S_0=s_0,S_t=s\right]P^{(t)}(s\vert s_0)\\
&\xlongequal{马尔科夫性质}\sum\limits_{s\in \mathcal{S}} E\left[R_{t+1}\vert S_t=s\right]P^{(t)}(s\vert s_0)\\
&=\sum\limits_{s\in \mathcal{S}} r_\pi P^{(t)}(s\vert s_0)
\end{aligned}</script><ul><li>$P^{(t)}(s\vert s_0)$ 表示在第 $t$ 轮迭代中使用的状态转移矩阵<script type="math/tex;mode=display">\lim\limits_{t\rightarrow \infty}P^{(t)}(s\vert s_0)=d_{\pi}(s)</script>因此，起始状态 $s_0$ 并不重要</li></ul><p>故有等价成立</p><p>最后，对于任意的稳态分布 $d$ ，是否仍然成立</p><script type="math/tex;mode=display">\begin{aligned}
\lim\limits_{n\rightarrow \infty}\frac{1}{n}E\left[\sum\limits_{t=0}^{n-1}R_{t+1}\right]&=\lim\limits_{n\rightarrow \infty}\frac{1}{n}\sum\limits_{s\in \mathcal{S}}d(s)E\left[\sum\limits_{t=0}^{n-1}R_{t+1}\vert S_0=s\right]\\
&=\sum\limits_{s\in \mathcal{S}}d(s)\lim\limits_{n\rightarrow \infty}E\left[\sum\limits_{t=0}^{n-1}R_{t+1}\vert S_0=s\right]\\
&=\sum\limits_{s\in \mathcal{S}}d(s)\overline{r}_{\pi}\\
&=\overline{r}_{\pi}
\end{aligned}</script><p>即，对任意的稳态分布 $r(s)$ 都有上述等价形式</p><h3 id="5-1-3-策略度量指标的梯度"><a href="#5-1-3-策略度量指标的梯度" class="headerlink" title="5.1.3 策略度量指标的梯度"></a>5.1.3 策略度量指标的梯度</h3><p>所有度量指标的梯度有一个统一形式</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta)=\sum\limits_{s\in \mathcal{S}}\eta(s)\sum\limits_{a\in \mathcal{A}(s)}\bigtriangledown_\theta\pi(a\vert s,\theta)\cdot Q_{\pi}(s,a)</script><ul><li><p>$J(\theta)$ 是不同的策略度量指标 $\overline{V}_{\pi},\overline{r}_{\pi},\overline{V}_{\pi}^0$</p></li><li><p>$=$ 可以是等于、近似、成比例</p></li><li><p>$\eta$ 是状态的分布或状态的权重，在不同问题中呈现不同的分布</p><p><img src="/posts/4136854086/image-20240312104729196.png" alt="image-20240312104729196"></p><p><img src="/posts/4136854086/image-20240312104754749.png" alt="image-20240312104754749"></p><p><img src="/posts/4136854086/image-20240312104820120.png" alt="image-20240312104820120"></p><p><img src="/posts/4136854086/image-20240312105023700.png" alt="image-20240312105023700"></p></li></ul><blockquote><p>一些特殊的结果</p><script type="math/tex;mode=display">\bigtriangledown_{\theta}\overline{r}_{\pi}\simeq \sum\limits_{s\in \mathcal{S}}d_{\pi}(s)\sum\limits_{a\in\mathcal{A}(s)}\bigtriangledown_\theta\pi(a\vert s,\theta)\cdot Q_{\pi}(s,a)</script><ul><li>当折扣情况， $\simeq$ 为 $\approx$ ；当非折扣情况，$\simeq$ 为 $=$</li></ul><p>由于 平均单步立即奖励 与 平均状态价值成正比，所以</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_{\theta}\overline{V}_{\pi}&=\frac{1}{1-\gamma}\bigtriangledown_{\theta}\overline{r}_{\pi}\\
&=\sum\limits_{s\in \mathcal{S}}\rho_{\pi}(s)\sum\limits_{a\in\mathcal{A}(s)}\bigtriangledown_\theta\pi(a\vert s,\theta)\cdot Q_{\pi}(s,a)
\end{aligned}</script></blockquote><h4 id="策略梯度的计算"><a href="#策略梯度的计算" class="headerlink" title="策略梯度的计算"></a>策略梯度的计算</h4><p>为了便于计算，需要将梯度改写</p><script type="math/tex;mode=display">\bigtriangledown_\theta \ln \pi(a\vert s,\theta)=\frac{\bigtriangledown_\theta\pi(a\vert s,\theta)}{\pi(a\vert s,\theta)}\Rightarrow \bigtriangledown_\theta\pi(a\vert s,\theta)=\pi(a\vert s,\theta)\bigtriangledown_\theta \ln \pi(a\vert s,\theta)</script><p>将其代入策略度量指标的梯度</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta J(\theta)&=\sum\limits_{s\in \mathcal{S}}\eta(s)\sum\limits_{a\in \mathcal{A}(s)}\bigtriangledown_\theta\pi(a\vert s,\theta)\cdot Q_{\pi}(s,a)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta)\bigtriangledown_\theta \ln \pi(a\vert s,\theta)\cdot Q_{\pi}(s,a)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)E_{A\sim \pi(A\vert S,\theta)}\left[\bigtriangledown_\theta \ln \pi(A\vert s,\theta)\cdot Q_{\pi}(s,A)\vert S=s\right]\\
&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot Q_{\pi}(S,A)\right]
\end{aligned}</script><p>此时，目标函数可以用样本近似</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta)\approx \bigtriangledown_\theta\ln\pi(a\vert s,\theta)\cdot Q_{\pi}(s,a)</script><h5 id="注"><a href="#注" class="headerlink" title="注"></a>注</h5><p>因为需要计算 $\ln\pi(a\vert s,\theta)$ ，必须保证 $\pi(a\vert s,\theta)&gt;0,\forall a,s,\theta$</p><p>在RL中，若不做处理，这个条件并不满足，如贪心策略或确定性策略，有些动作 $\pi(a)=0$</p><p>为确保满足条件，需要对所有的 $\pi(a\vert s,\theta)$ 通过 <em>Softmax</em> 函数归一化，将其取值变为 $(0,1)$</p><blockquote><p><em>Softmax</em> ：对于一个特征向量 $X=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}$ ，$z_i=\frac{e^{x_i}}{\sum\limits_{j=1}^ne^{x_j}}\in (0,1),\sum\limits_{i=1}^nz_i=1$</p></blockquote><p>决策值可以归一化为</p><script type="math/tex;mode=display">\pi(a\vert s,\theta)=\frac{e^{h(s,a,\theta)}}{\sum\limits_{a'\in \mathcal{A}(s)}e^{h(s,a',\theta)}}</script><p>其中，$\pi(a\vert s,\theta)$ 的值由函数 $h(s,a,\theta)$ 确定，即 $h(\cdot)$ 为参数为 $\theta$ 的策略函数，输入状态会给出该状态下执行某个动作的概率</p><p><img src="/posts/4136854086/image-20240307164637524.png" alt="image-20240307164637524"></p><p>策略函数 $h(\cdot)$ 可以用神经网络实现，输入为 $s$ ，参数为 $\theta$，输出由 $\vert \mathcal{A}(s)\vert$ 个，每个输出对应采取每个动作的概率 $\pi(a\vert s,\theta)$ 且 $\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta)=1$，此时这个神经网络的输出层为 <em>Softmax</em></p><ul><li>由于 $\pi(a\vert s,\theta)&gt;0,\forall a$ ，所以策略是随机性及探索性的</li><li>策略梯度也可改为确定性策略</li></ul><p>softmax策略的梯度</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta\ln\pi(a\vert s,\theta)&=\bigtriangledown_\theta h(s,a,\theta)-\frac{1}{\sum\limits_{a\in \mathcal{A}(s)}e^{h(s,a,\theta)}}\sum\limits_{a\in \mathcal{A}(s)}e^{h(s,a,\theta)}\bigtriangledown_\theta h(s,a,\theta)\\
&=\bigtriangledown_\theta h(s,a,\theta)-E_{a\sim \pi(a\vert s,\theta)}\left[\bigtriangledown_\theta h(s,a,\theta)\right]
\end{aligned}</script><p>因此，策略网络的梯度为</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta J(\theta)&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot Q_{\pi}(S,A)\right]\\
&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\left(\bigtriangledown_\theta h(S,A,\theta)-E_{a\sim \pi(A\vert S,\theta)}\left[\bigtriangledown_\theta h(S,A,\theta)\right]\right)Q_{\pi}(S,A)\right]
\end{aligned}</script><h3 id="5-1-4-与基于价值的学习对比"><a href="#5-1-4-与基于价值的学习对比" class="headerlink" title="5.1.4 与基于价值的学习对比"></a>5.1.4 与基于价值的学习对比</h3><p><strong>基于价值的学习</strong>由一个 $w$ 为参数的价值函数 $Q(s,a,w)$</p><p>优化目标为最小化TD误差：</p><script type="math/tex;mode=display">J(w)=E_{\pi}\left[\frac{1}{2}\left(r_{t+1}+\gamma \max\limits_{a'\in \mathcal{A}(s')}\hat{Q}(s',a',w)-\hat{Q}(s,a,w)\right)^2\right]</script><p>更新方式</p><script type="math/tex;mode=display">w_{t+1}=w_{t}+\alpha_t\left[r_{t+1}+\gamma \max\limits_{a\in \mathcal{A}(s_{t+1})} \hat{Q}_{\pi}\left(s_{t+1},a,w_t\right)-\hat{Q}\left(s_t,a_t,w_t\right)\right]\bigtriangledown_w\hat{Q}\left(s_t,a_t,w_t\right)</script><p><strong>基于策略梯度的学习</strong> 由一个 $\theta$ 为参数的策略函数 $\pi(a\vert s,\theta)$</p><p>优化目标为最大化策略度量指标</p><script type="math/tex;mode=display">\max\limits_{\theta}J(\theta)=E_{\pi_\theta}\left[\pi(a\vert s,\theta)\hat{\delta}_{\pi}(s,a)\right]</script><p>更新方式</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[ \bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \hat{\delta}_{t}^{\pi}(s_t,a_t)\right]</script><h2 id="5-2-基于蒙特卡洛的策略梯度——REINFORCE"><a href="#5-2-基于蒙特卡洛的策略梯度——REINFORCE" class="headerlink" title="5.2 基于蒙特卡洛的策略梯度——REINFORCE"></a>5.2 基于蒙特卡洛的策略梯度——REINFORCE</h2><p>不管采用哪种策略度量指标，对策略函数的优化方法通过梯度上升法最大化 $J(\theta)$</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha\bigtriangledown_\theta J(\theta)\\
&=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot Q_{\pi}(S,A)\right]\\
&\xlongequal{随机梯度上升法}\theta_{t}+\alpha\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot Q_{\pi}(s_t,a_t)
\end{aligned}</script><p>此外，真实的动作价值 $Q_{\pi}(s_t,a_t)$ 也是未知的，需要近似</p><ul><li><p>基于MC方法去近似动作价值，称为 <strong>REINFORCE</strong> 算法</p><p>用累积奖励值的无偏采样 $Q_{t}(s_t,a_t)=G_t$ 去近似 $Q_{\pi}(s_t,a_t)$</p></li><li><p>基于TD方法去近似，称为AC算法</p></li></ul><h3 id="5-2-3-伪代码"><a href="#5-2-3-伪代码" class="headerlink" title="5.2.3 伪代码"></a>5.2.3 伪代码</h3><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：带参策略函数 \pi(a\vert s,\theta),\gamma\in (0,1),\alpha>0\\
&目标：寻找最优策略即最大化目标函数 J(\theta)\\
&对于第 k 轮迭代:\\
&\quad 以s_0为起始状态，基于策略 \pi(\theta_{t})生成一个回合(s_0,a_0,r_1,\cdots,s_{T-1},a_{T-1},r_T)\\
&\quad 对于t=0,1,\cdots,T-1:\\
&\qquad 价值更新：Q_t(s_t,a_t)=\sum\limits_{k=t+1}^T\gamma^{k-t-1}r_k\\
&\qquad 策略更新：\theta_{t+1}=\theta_{t}+\alpha\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot Q_{t}(s_t,a_t)\\
&\quad \theta_{t}=\theta_{T}
\\
\hline
\end{array}</script><h4 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h4><p>从 $(s_t,a_t)$ 出发，采集一个回合</p><ul><li><p>采集状态样本： $S\sim d$ ，其中分布 $d$ 状态为在策略 $\pi$ 下的稳态分布</p></li><li><p>采集动作样本：$A\sim \pi(A\vert S,\theta)$ ，$a_t$ 是策略 $\pi(a\vert s_t,\theta_{t})$ 的采样</p></li></ul><p>由采样也可看出，<strong>REINFORCE</strong> 算法为同策略 <em>on-policy</em> 算法</p><h4 id="REINFORCE是离线方法"><a href="#REINFORCE是离线方法" class="headerlink" title="REINFORCE是离线方法"></a>REINFORCE是离线方法</h4><p>基于蒙特卡洛方法，必须将所有的回合数据采集完后才能开始运行，所以策略 $\theta_{t+1}$ 更新后，并没有立即产生数据，即REINFORCE算法是一种离线算法</p><p><strong>缺点</strong>：</p><ul><li><p>基于片段式数据的任务，任务需要终止状态，才能去计算回报</p></li><li><p>低数据利用效率：需要大量的训练数据</p></li><li><p><strong>高训练方差</strong></p><p>从单个或多个片段中采样到的回报去估计动作价值有很高的方差</p></li></ul><h3 id="5-2-2-算法分析"><a href="#5-2-2-算法分析" class="headerlink" title="5.2.2 算法分析"></a>5.2.2 算法分析</h3><p>由于 $\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})=\frac{\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})}{\pi(a_t\vert s_t,\theta_{t})}$</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot Q_{\pi}(s_t,a_t)\\
&=\theta_{t}+\alpha\left(\frac{\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})}{\pi(a_t\vert s_t,\theta_{t})}\right)\cdot Q_{\pi}(s_t,a_t)\\
&=\theta_{t}+\alpha\underbrace{\left(\frac{Q_{\pi}(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}\right)}_{\beta_{t}}\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})\\
&=\theta_{t}+\alpha\beta_t\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})
\end{aligned}</script><p>其中，$\alpha\beta_t$ 作为梯度上升法的步长，必须足够小</p><ul><li>若 $\beta_t&gt;0$ ，则在 $s_t$ 时选择 $a_t$ 的可能性被提高，即 $\pi(a_t\vert s_t,\theta_{t+1})&gt;\pi(a_t\vert s_t,\theta_{t})$ ，且 $\beta_t$ 越大，增大程度越大</li><li>若 $\beta_t&lt;0$ ，则 $\pi(a_t\vert s_t,\theta_{t+1})&lt;\pi(a_t\vert s_t,\theta_{t})$</li></ul><blockquote><p>对于任意的$\theta_{t}$ 与 $\theta_{t+1}$ ，当 $\theta_{t+1}-\theta_{t}$ 足够小时，</p><script type="math/tex;mode=display">\pi(a_t\vert s_t,\theta_{t+1})\approx \pi(a_t\vert s_t,\theta_{t})+\left(\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})\right)^T\left(\theta_{t+1}-\theta_{t}\right)</script><p>从微分角度看，随着 $\theta_{t+1}-\theta_{t}\rightarrow 0$ ，这个近似变为等式</p><script type="math/tex;mode=display">\begin{aligned}
\pi(a_t\vert s_t,\theta_{t+1})&\approx \pi(a_t\vert s_t,\theta_{t})+\left(\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})\right)^T\left(\theta_{t+1}-\theta_{t}\right)\\
&\xlongequal{\theta_{t+1}-\theta_{t}\rightarrow 0}\pi(a_t\vert s_t,\theta_{t})+\left(\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})\right)^T\left(\alpha\beta_t\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})\right)\\
&=\pi(a_t\vert s_t,\theta_{t})+\alpha\beta_t\Vert \bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})\Vert_2
\end{aligned}</script><p>其中，梯度上升法步长 $\alpha&gt;0$ ，由此可见，当 $\beta_t&gt;0$ 时，$\pi(a_t\vert s_t,\theta_{t+1})&gt;\pi(a_t\vert s_t,\theta_{t})$ ；当 $\beta_t&lt;0$ ，$\pi(a_t\vert s_t,\theta_{t+1})&lt;pi(a_t\vert s_t,\theta_{t})$</p></blockquote><h4 id="beta-t-frac-Q-pi-s-t-a-t-pi-a-t-vert-s-t-theta-t-能很好平衡探索与利用"><a href="#beta-t-frac-Q-pi-s-t-a-t-pi-a-t-vert-s-t-theta-t-能很好平衡探索与利用" class="headerlink" title="$\beta_t=\frac{Q_{\pi}(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}$ 能很好平衡探索与利用"></a>$\beta_t=\frac{Q_{\pi}(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}$ 能很好平衡探索与利用</h4><p><strong>利用</strong> ：若某个动作价值大， $Q_{\pi}(s_t,a_t)$ 越大，则策略更新后，在 $s_t$ 下选择 $a_t$ 的可能性越大</p><blockquote><p>$\beta_t\propto Q_{\pi}(s_t,a_t)$ ，若 $Q_{\pi}(s_t,a_t)$ 增大，则 $\beta_t$ 会增大，引起 $\pi(a_{t}\vert s_t,\theta_{t+1})$ 增大</p></blockquote><p><strong>探索</strong> ：若在状态 $s_t$ 下，选择动作 $a_t$ 的可能性很小，则策略更新后，会增大选择 $a_t$ 的可能性</p><blockquote><p>$\pi(a_t\vert s_t,\theta_{t})$ 越小，则 $\beta_t$ 越大，从而 $\pi(a_{t}\vert s_t,\theta_{t+1})$ 会增大</p></blockquote><h2 id="5-3-AC方法"><a href="#5-3-AC方法" class="headerlink" title="5.3 AC方法"></a>5.3 AC方法</h2><blockquote><p>将价值近似函数引入到策略梯度中，得到了 Actor-Critic 方法</p><p>Actor：策略更新，策略会被应用于决策/动作选择</p><ul><li>生成使评论家满意的策略</li></ul><p>Critic：策略评估/价值评估，用度量指标衡量策略的优劣</p><ul><li>学会准确估计演员策略所采取动作的价值函数</li></ul></blockquote><ul><li>QAC</li><li>A2C：通过引入偏置量减少估计的方差</li><li>off-policy AC：将on-policy方法转换为 off-policy 方法<ul><li>重要性采样</li></ul></li><li>DPG：随机策略变为确定性策略</li></ul><p>策略梯度方法的步骤：</p><ol><li><p>用于度量策略好坏的策略度量指标函数/目标函数 $J(\theta)$ ，如：$\overline{V}_{\pi},\overline{r}_{\pi}$</p></li><li><p>Critic（策略评估）：最小化价值函数与价值的损失</p><script type="math/tex;mode=display">Q(s,a)\simeq \hat{Q}_{w}(s,a)= r(s,a)+\gamma E_{s'\sim P(s'\vert s,a),a'\sim \pi_{w}(a'\vert s')}\left[Q_{w}(s',a')\right]\\
w_{t+1}=w_{t}+\alpha_w \left[r_{t+1}+\gamma \hat{Q}_t(s_{t+1},a_{t+1},w_{t})-\hat{Q}_t(s_t,a_t,w_{t})\right]\bigtriangledown_w \hat{Q}(s_t,a_t,w_{t})</script></li><li><p>Actor（策略更新）</p><p>通过梯度上升法最大化 $J(\theta)$</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha\bigtriangledown_\theta J(\theta_{t})\\
&=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot Q_{\pi}(S,A)\right]
\end{aligned}</script><p>为便于计算，使用随机梯度上升法</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta)\cdot Q_{t}(s_t,a_t)</script></li></ol><p>策略梯度的方法为 <strong>Actor</strong> ，其中 $Q_t(s_t,a_t)$ 作为价值评估/策略评估，对应 <strong>Critic</strong></p><p>对于 $\hat{Q}_{w}(s,a)$ 的获取，若使用TD方法，这类算法统称为 AC 方法</p><h3 id="5-3-1-QAC"><a href="#5-3-1-QAC" class="headerlink" title="5.3.1 QAC"></a>5.3.1 QAC</h3><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：带参策略函数 \pi(a\vert s,\theta),\gamma\in (0,1),\alpha>0\\
&目标：寻找最优策略即最大化目标函数 J(\theta)\\
& 对于每个回合的t时刻：\\
&\quad 基于策略 \pi(a\vert s_t,\theta_{t})生成动作 a_t,获得r_{t+1},s_{t+1},再基于\pi(a\vert s_{t+1},\theta_{t}) 生成a_{t+1}\\
&\qquad 即(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})\\
&\quad Critic(价值更新):\\
&\qquad w_{t+1}=w_{t}+\alpha_w \left[r_{t+1}+\gamma \hat{Q}_t(s_{t+1},a_{t+1},w_{t})-\hat{Q}_t(s_t,a_t,w_{t})\right]\bigtriangledown_w \hat{Q}(s_t,a_t,w_{t})\\
&\quad Actor(策略更新):\\
&\qquad \theta_{t+1}=\theta_{t}+\alpha_{\theta}\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta)\cdot \hat{Q}_{t}(s_t,a_t,w_{t+1})
\\
\hline
\end{array}</script><ul><li>实质上，是 <strong>基于价值近似函数的Sarsa+策略梯度</strong></li></ul><h4 id="QAC是同策略-on-policy-算法"><a href="#QAC是同策略-on-policy-算法" class="headerlink" title="QAC是同策略 on-policy 算法"></a>QAC是同策略 on-policy 算法</h4><p>在求最优化时，由于存在一个期望 $\bigtriangledown_\theta J(\theta_{t})=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)Q_{\pi}(S,A)\right]$ ，故将算法修改为随机梯度上升法</p><p>动作需要按照策略 $\pi^{(t)}$ 的分布采样，所以 $\pi^{(t)}$ 是探索策略，同时又是不断改进的策略，所以是目标策略，因此 QAC 是同策略算法</p><h4 id="QAC是随机性策略"><a href="#QAC是随机性策略" class="headerlink" title="QAC是随机性策略"></a>QAC是随机性策略</h4><p>采取每个动作 $a_t\in \mathcal{A}(s_t)$ 的概率 $\pi(a_t\vert s_t,\theta_{t})&gt;0$</p><p>所以，策略本身就具有一定的探索能力 $\pi(a\vert s,\theta)$ ，不需要 $\varepsilon-贪心$ 去探索</p><h3 id="5-3-2-A2C"><a href="#5-3-2-A2C" class="headerlink" title="5.3.2 A2C"></a>5.3.2 A2C</h3><p>advantage actor-critic(A2C)：核心思想是通过引入一个偏置量 bias 减小方差，即</p><p><strong>策略梯度对于额外的偏置量是不变的</strong></p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta J(\theta)&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot Q_{\pi}(S,A)\right]\\
&=E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot \left(Q_{\pi}(S,A)-b(S)\right)\right]
\end{aligned}</script><p>额外的偏置量是关于状态变量 $S$ 的偏置量</p><ul><li><p>为什么引入偏置量不改变策略梯度</p><p>相当于证明 $E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)b(S)\right]=0$</p><script type="math/tex;mode=display">\begin{aligned}
E_{S\sim \eta,A\sim\pi(A\vert S,\theta)}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta)b(S)\right]&=\sum\limits_{s\in \mathcal{S}}\eta(s)\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta)\bigtriangledown_\theta\ln\pi(A\vert S,\theta)b(s)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta)\frac{\bigtriangledown_\theta\pi(a\vert s,\theta)}{\pi(a\vert s,\theta)}b(s)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)\sum\limits_{a\in \mathcal{A}(s)}\bigtriangledown_\theta\pi(a\vert s,\theta)b(s)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)b(s)\sum\limits_{a\in \mathcal{A}(s)}\bigtriangledown_\theta\pi(a\vert s,\theta)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)b(s)\bigtriangledown_\theta\left(\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta)\right)\\
&=\sum\limits_{s\in \mathcal{S}}\eta(s)b(s)\bigtriangledown_\theta 1=0
\end{aligned}</script></li><li><p>为什么引入偏置量能减小估计方差</p><p>令 $\bigtriangledown_\theta J(\theta)=E[X]$ ，其中 $X(S,A)=\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot \left(Q_{\pi}(S,A)-b(S)\right)$</p><p>$X$ 的期望 $E[X]$ 与 $b(S)$ 无关，方差 $var(X)$ 与 $b(S)$ 有关</p><p>使用方差的迹评价方差大小，$tr[var(X)]=E[X^TX]-\overline{X}^T\overline{X}$</p><p>已知 $\overline{X}^T\overline{X}=\left(E[X]\right)^TE[X]$ 与偏置量 $b(S)$ 无关</p><script type="math/tex;mode=display">\begin{aligned}
E[X^TX]&=E\left[\left(\bigtriangledown_\theta\ln\pi\right)^T\bigtriangledown_\theta\ln\pi\cdot \left(Q_{\pi}(S,A)-b(S)\right)^2\right]\\
&=E\left[\Vert \bigtriangledown_\theta\ln\pi\Vert^2_2 \cdot \left(Q_{\pi}(S,A)-b(S)\right)^2\right]\\
&\xlongequal{S\sim \eta,A\sim \pi} \sum\limits_{s\in \mathcal{S}}\eta(s)E_{A\sim \pi}\left[\Vert \bigtriangledown_\theta\ln\pi\Vert^2_2 \cdot \left(Q_{\pi}(S,A)-b(S)\right)^2\right]
\end{aligned}</script><p><strong>可见偏置量 $b(S)$ 对策略度量函数梯度的方差有影响</strong>，</p><p>为使 $var(X)$ 最小化，最优的偏差应使 $\bigtriangledown_bE[X^TX]=0$ ，即</p><script type="math/tex;mode=display">\begin{aligned}
&E_{A\sim \pi}\left[\Vert \bigtriangledown_\theta\ln\pi\Vert^2_2 \cdot \left(Q_{\pi}(s,A)-b(s)\right)\right]=0,s\in \mathcal{S}\\
\Rightarrow&b^*=\frac{E_{A\sim \pi}\left[\Vert \bigtriangledown_\theta\ln\pi\Vert^2_2\right]Q_{\pi}(s,A)}{E_{A\sim \pi}\left[\Vert \bigtriangledown_\theta\ln\pi\Vert^2_2\right]},s\in \mathcal{S}
\end{aligned}</script><p>且 $b(S)=0$ 不是好的偏置</p><ul><li>对于 <strong>REINFORCE</strong> 与 <strong>QAC</strong> 算法，其偏置量 $b(S)=0$ ，并不是好的偏置</li></ul></li></ul><p>减小方差的意义：采样时会有更小的误差，在均值相同的情况下，采样时方差小的样本集，每个样本 $X$ 都接近平均值。即使用RM算法近似策略梯度时，随机采集到的每个样本都能接近期望，使策略梯度受随机采样的影响程度最小，进而使得最优化的结果受采样的影响程度最小</p><p><strong>因此，在A2C中，算法目标为对 $\theta$ 最优化，使得 $J(\theta)$ 最大，同时，选择一个最优的偏置 $b(S)$ 最小化策略梯度方差 $var(X),X=\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot \left(Q_{\pi}(S,A)-b(S)\right)$</strong></p><script type="math/tex;mode=display">b^*(s)=\frac{E_{A\sim\pi(A\vert s,\theta_{t})}\left[\Vert\bigtriangledown_{\theta_{t}} \ln \pi(A\vert s,\theta_{t})\Vert^2\cdot Q(s,A)\right]}{E_{A\sim\pi(A\vert s,\theta_{t})}\left[\Vert \bigtriangledown_\theta \ln \pi(A\vert s,\theta_{t})\Vert^2\right]},s\in \mathcal{S}</script><p>尽管存在最优的偏置量，但为了计算方便，移除权重 $\Vert\bigtriangledown_\theta \ln \pi(A\vert s,\theta_{t})\Vert^2$ ，仅使用次优偏置量</p><script type="math/tex;mode=display">b(s)=E_{A\sim\pi(A\vert S,\theta_{t})}\left[Q(s,A)\right]=V_{\pi(\theta_{t})}(s)</script><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>令偏置量 $b(s)=V_{\pi}(s)$</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta_{t})\cdot \left(Q_{\pi}(S,A)-b(S)\right)\right]\\
&=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta_{t})\cdot \left(Q_{\pi}(S,A)-V_{\pi}(S)\right)\right]\\
&\xlongequal{\delta_{\pi}(S,A)=Q_{\pi}(S,A)-V_{\pi}(S)}\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta_{t})\cdot \delta_{\pi}(S,A)\right]
\end{aligned}</script><p>$\delta_\pi(S,A)$ 称为优势函数，因为 $V_{\pi}(s)=\sum\limits_{a\in \mathcal{A}(s)}\pi(a\vert s,\theta_{t})Q_{\pi}(s,a)$ ，即状态价值为动作价值的加权平均，若某个动作价值比均值大，说明这个动作是比较好的，其 $Q(s,a)$ 比均值大，$\delta_{\pi}(s,a)&gt;0$ ，这个动作具有一定优势</p><ul><li>使用 $\delta$ 代替动作价值 $Q$ ，因为我们在乎的不是动作价值的绝对大小，而是动作价值间的相对大小</li></ul><p>将梯度上升法改为随机梯度上升法</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[\bigtriangledown_\theta\ln\pi(A\vert S,\theta_{t})\cdot \left[Q_{\pi}(S,A)-V_{\pi}(S)\right]\right]\\
&=\theta_{t}+\alpha E_{S\sim \eta,A\sim\pi(A\vert S,\theta_{t})}\left[\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \left[\hat{Q}_{t}(s_t,a_t,w_{t})-\hat{V}_{t}(s_t,w_{t})\right]\right]\\
&=\theta_{t}+\alpha \bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \hat{\delta}_{t}(s_t,a_t)
\end{aligned}</script><p>另外，由动作价值定义：</p><script type="math/tex;mode=display">E\left[Q_{\pi}(S,A)-V_{\pi}(S)\vert S=s_t,A=a_t\right]=E\left[R'+\gamma V_{\pi}(S')-V_{\pi}(S)\vert S=s_t,A=a_t\right]</script><p>优势函数可以用TD误差来近似：</p><script type="math/tex;mode=display">\begin{aligned}
&\hat{\delta}(s_t,a_t,w_{t})=\hat{Q}_{t}(s_t,a_t,w_{t})-\hat{V}_{t}(s_t,w_{t})\\
\Longrightarrow&\hat{\delta}(s_t,w_{t})=r_{t+1}+\gamma \hat{V}_t(s_{t+1},w_{t})-\hat{V}_{t}(s_t,w_{t})
\end{aligned}</script><p>因此，我们仅需要一个神经网络来近似 $V_{\pi}(S)$ 而不再需要动作价值网络 $Q_{\pi}(S,A)$</p><h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：带参策略函数 \pi(a\vert s,\theta),\gamma\in (0,1),\alpha>0\\
&目标：寻找最优策略即最大化目标函数 J(\theta)\\
& 对于每个回合的t时刻：\\
&\quad 基于策略 \pi(a\vert s_t,\theta_{t})生成动作 a_t,获得r_{t+1},s_{t+1}\\
&\quad TD误差：\\
&\qquad \hat{\delta}_t=r_{t+1}+\gamma \hat{V}(s_{t+1},w_{t})-\hat{V}(s_t,w_{t})\\
&\quad Critic(价值更新):\\
&\qquad w_{t+1}=w_{t}+\alpha_w \hat{\delta}_t\bigtriangledown_w \hat{V}(s_t,w_{t})\\
&\quad Actor(策略更新):\\
&\qquad \theta_{t+1}=\theta_{t}+\alpha_{\theta}\hat{\delta}_t\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta)
\\
\hline
\end{array}</script><h4 id="A2C是同策略算法"><a href="#A2C是同策略算法" class="headerlink" title="A2C是同策略算法"></a>A2C是同策略算法</h4><p>在一步迭代后，$\theta_{t}$ 会更新为 $\theta_{t+1}$ ，同时，会基于更新后的策略 $\pi(a\vert s_t,\theta_{t})$ 生成下一步经验，因此策略 $\pi$ 既是目标策略又是探索策略</p><h4 id="A2C是随机性策略"><a href="#A2C是随机性策略" class="headerlink" title="A2C是随机性策略"></a>A2C是随机性策略</h4><p>由于策略梯度算法需要经过 <em>Softmax</em> 层，所以每个动作的概率 $\pi(a\vert s)&gt;0$ ，即随机性策略</p><h4 id="mbox-step-size-frac-delta-t-s-t-a-t-pi-a-t-vert-s-t-theta-t-能很好平衡探索与利用"><a href="#mbox-step-size-frac-delta-t-s-t-a-t-pi-a-t-vert-s-t-theta-t-能很好平衡探索与利用" class="headerlink" title="$\mbox{step size}=\frac{\delta_t(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}$ 能很好平衡探索与利用"></a>$\mbox{step size}=\frac{\delta_t(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}$ 能很好平衡探索与利用</h4><p>由于 $\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})=\frac{\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})}{\pi(a_t\vert s_t,\theta_{t})}$</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \hat{\delta}_{t}(s_t,a_t)\\
&=\theta_{t}+\alpha\left(\frac{\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})}{\pi(a_t\vert s_t,\theta_{t})}\right)\cdot \hat{\delta}_{t}(s_t,a_t)\\
&=\theta_{t}+\alpha\underbrace{\left(\frac{\hat{\delta}_{t}(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}\right)}_{\mbox{step size}}\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})
\end{aligned}</script><p><strong>利用</strong></p><p>step size 比较大，也就是优势函数 $\hat{\delta}_{t}(s_t,a_t)$ 比较大，即 $a_t$ 的动作价值 $Q_{\pi_{\theta_{t}}}(s_t,a_t)$ 明显大于 $s_t$ 下的平均动作价值 $V_{\pi}(s)$ ，则策略更新后，在 $s_t$ 下选择 $a_t$ 的可能性越大</p><blockquote><p>$\mbox{step size}\propto \hat{\delta}_{t}(s_t,a_t)$ ，若 $\hat{\delta}_{t}(s_t,a_t)$ 增大，则 $\mbox{step size}$ 会增大，策略更新时会朝着 $\pi(a_{t}\vert s_t,\theta_{t+1})$ 增大的方向更新</p></blockquote><p><strong>探索</strong> ：若在状态 $s_t$ 下，选择动作 $a_t$ 的可能性很小，则策略更新后，会增大选择 $a_t$ 的可能性</p><blockquote><p>$\pi(a_t\vert s_t,\theta_{t})$ 越小，则 $\mbox{step size}$ 越大，从而 $\pi(a_{t}\vert s_t,\theta_{t+1})$ 会增大</p></blockquote><h3 id="5-3-3-异策略AC算法"><a href="#5-3-3-异策略AC算法" class="headerlink" title="5.3.3 异策略AC算法"></a>5.3.3 异策略AC算法</h3><p><strong>REINFORCE</strong> ，<strong>QAC</strong> ，<strong>A2C</strong> 都是同策略算法，因为在计算策略度量指标梯度时，涉及到期望的计算</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta)=E_{S\sim \eta,A\sim \pi}[*]</script><p>基于RM算法，改为随机梯度下降法，由于在采样过程中，由于动作 $A\sim \pi(A\vert S,\theta_{t})$ 策略 $\pi$ 作为探索策略同时也是目标策略，所以他们都是同策略算法</p><p>为复用通过其他方法得到的一些经验，通过 <strong>重要性采样</strong> 技巧，可将其改为异策略AC算法</p><ul><li>同理，重要性采样可用于任何估计期望的算法中</li></ul><p>对于异策略算法，我们希望估计 $E_{A\sim \pi}[*],(A\sim p_0)$ ，其中 $\pi$ 是目标策略，而样本基于探索策略 $\mu,(\{a_i\}\sim p_1)$</p><h4 id="异策略策略梯度"><a href="#异策略策略梯度" class="headerlink" title="异策略策略梯度"></a>异策略策略梯度</h4><ol><li>策略梯度表达式</li><li>使用梯度上升的方法优化</li></ol><h5 id="异策略梯度表达式"><a href="#异策略梯度表达式" class="headerlink" title="异策略梯度表达式"></a>异策略梯度表达式</h5><p>设 $\mu$ 为探索策略用于生成经验样本，目标是使用这些经验样本去更新目标策略 $\pi$ ，使得策略度量指标 $J(\theta)$ 最小化</p><script type="math/tex;mode=display">J(\theta)=\sum\limits_{s\in \mathcal{S}}d_{\mu}(s)V_{\pi}(s)=E_{S\sim d_\mu}[V_{\pi}(S)]</script><p>其中，$d_{\mu}$ 是状态 $S$ 在策略 $\mu$ 下的稳态分布</p><p><strong>异策略梯度</strong></p><p>在折扣奖励情况下 $\gamma\in (0,1)$ ，目标函数 $J(\theta)$ 的梯度为</p><script type="math/tex;mode=display">\bigtriangledown_{\theta}J(\theta)=E_{S\sim \eta,A\sim \mu}\left[\underbrace{\frac{\pi(A\vert S,\theta)}{\mu(A\vert S)}}_{重要性权重}\bigtriangledown_\theta\ln\pi(A\vert S,\theta)Q_{\pi}(S,A)\right]</script><ul><li>$\mu$ 为探索策略，$\eta$ 为状态服从的分布<script type="math/tex;mode=display">\eta(s)=\sum\limits_{s'\in \mathcal{S}}d_{\mu}(s')Pr_{\pi}(s\vert s')</script>$Pr_{\pi}(s\vert s’)=\sum\limits_{k=0}^{\infty}\gamma^k\left[P_{\pi}^k\right]_{s’s}=[(I-\gamma P_{\pi})^{-1}]_{s’s}$ ，表示基于策略 $\pi$ 从状态 $s’$ 转移到 $s$ 的概率，其中 $[\cdot]_{s’s}$ 表示矩阵第 $s’$ 行，第 $s$ 列的值</li></ul><h5 id="异策略梯度优化"><a href="#异策略梯度优化" class="headerlink" title="异策略梯度优化"></a>异策略梯度优化</h5><p>异策略梯度仍引入偏置量 $b(S)$ ，有</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta)=E_{S\sim \eta,A\sim \mu}\left[\frac{\pi(A\vert S,\theta)}{\mu(A\vert S)}\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot \left(Q_{\pi}(S,A)-b(S)\right)\right]</script><p>为减少估计方差，令偏置量 $b(S)=V_{\pi}(S)$</p><script type="math/tex;mode=display">\bigtriangledown_\theta J(\theta)=E_{S\sim \eta,A\sim \mu}\left[\frac{\pi(A\vert S,\theta)}{\mu(A\vert S)}\bigtriangledown_\theta\ln\pi(A\vert S,\theta)\cdot \left(Q_{\pi}(S,A)-V_{\pi}(S)\right)\right]</script><p>相应的随机梯度上升算法为</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha\frac{\pi(a_t\vert s_t,\theta_{t})}{\mu(a_t\vert s_t)}\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \left(\underbrace{\hat{Q}_{t}(s_t,a_t,w_{t})-\hat{V}_{t}(s_t,w_{t})}_{\delta_t(s_t,a_t)}\right)</script><p>为了便于计算，将优势函数 $\delta_t(s_t,a_t)$ 用TD误差去近似</p><script type="math/tex;mode=display">\hat{Q}_{t}(s_t,a_t,w_{t})-\hat{V}_{t}(s_t,w_{t})\approx r_{t+1}+\gamma \hat{V}_{t}(s_{t+1},w_{t})-\hat{V}_t(s_t,w_{t})=\hat{\delta}_t(s_t,a_t)</script><p>因此，异策略梯度优化为</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha\frac{\pi(a_t\vert s_t,\theta_{t})}{\mu(a_t\vert s_t)}\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \hat{\delta}_t(s_t,a_t)</script><h5 id="充分利用的算法"><a href="#充分利用的算法" class="headerlink" title="充分利用的算法"></a>充分利用的算法</h5><p>在异策略梯度中</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{t+1}&=\theta_{t}+\alpha\frac{\pi(a_t\vert s_t,\theta_{t})}{\mu(a_t\vert s_t)}\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta_{t})\cdot \hat{\delta}_t(s_t,a_t)\\
&=\theta_{t}+\alpha\frac{\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})}{\mu(a\vert s_t)}\hat{\delta}_t(s_t,a_t)\\
&=\theta_{t}+\alpha\frac{\hat{\delta}_t(s_t,a_t)}{\mu(a\vert s_t)}\bigtriangledown_\theta\pi(a_t\vert s_t,\theta_{t})
\end{aligned}</script><p>此时，$\mu(a\vert s_t)$ 可看做是固定值，若 $\hat{\delta}_t(s_t,a_t)$ 越大，则表明动作 $a_t$ 的优势越大，策略更新时要朝着 $a_t$ 增大的方向迭代，即充分利用</p><h4 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：探索策略为\mu(a\vert s),带参目标策略函数 \pi(a\vert s,\theta_{0}),\gamma\in (0,1),\alpha>0\\
&\qquad \qquad 状态价值近似函数 \hat{V}(s,w^{(0)})\\
&目标：寻找最优策略即最大化目标函数 J(\theta)\\
& 对于每个回合的t时刻：\\
&\quad 基于探索策略 \mu(a\vert s_t)生成动作 a_t,获得r_{t+1},s_{t+1}\\
&\quad TD误差（优势函数）：\\
&\qquad \hat{\delta}_t=r_{t+1}+\gamma \hat{V}(s_{t+1},w_{t})-\hat{V}(s_t,w_{t})\\
&\quad Critic(价值更新):\\
&\qquad w_{t+1}=w_{t}+\alpha_w \hat{\delta}_t\frac{\pi(a_t\vert s_t,\theta_{t})}{\mu(a_t\vert s_t)}\bigtriangledown_w \hat{V}(s_t,w_{t})\\
&\quad Actor(策略更新):\\
&\qquad \theta_{t+1}=\theta_{t}+\alpha_{\theta}\hat{\delta}_t\frac{\pi(a_t\vert s_t,\theta_{t})}{\mu(a_t\vert s_t)}\bigtriangledown_\theta\ln\pi(a_t\vert s_t,\theta)
\\
\hline
\end{array}</script><h3 id="5-3-4-Deterministic-actor-critic-DPG"><a href="#5-3-4-Deterministic-actor-critic-DPG" class="headerlink" title="5.3.4 Deterministic actor-critic(DPG)"></a>5.3.4 Deterministic actor-critic(DPG)</h3><p>策略梯度算法，为便于求解，将策略度量函数的梯度转换为期望形式，使用RM算法去近似估计。而这个转换设计 $\ln \pi$ ，因此，$\pi(a\vert s,\theta)&gt;0,\forall a$ ，即每个动作的概率都不为0，也不会有动作的概率为1，所以 <strong>REINFORCE、QAC，A2C，异策略A2C</strong> 都是随机性策略</p><p>随机性策略的缺点是动作 $a_t$ 的个数必须有限，不能处理连续动作</p><p>若改用确定性策略，可以处理连续的动作，表示为 $a=\pi(s,\theta)$ ，相当于采取某个动作的概率为1，其余动作概率为0</p><ul><li>$\pi$ 为从状态空间 $\mathcal{S}$ 到动作空间 $\mathcal{A}$ 的映射 $\pi:\mathcal{S}\mapsto \mathcal{A}$</li><li>$\pi$ 也可以是一个神经网络，其输入是状态 $s$ ，输出是一个动作 $a$ ，其参数为 $\theta$</li></ul><p><img src="/posts/4136854086/image-20240512000149033.png" alt="image-20240512000149033"></p><h4 id="梯度的计算"><a href="#梯度的计算" class="headerlink" title="梯度的计算"></a>梯度的计算</h4><p>之前的梯度仅是针对随机策略的梯度，若策略是确定性的，需要重新计算梯度</p><p><strong>确定性策略梯度的统一形式</strong></p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta J(\theta)&=\sum\limits_{s\in \mathcal{S}}\eta(s)\bigtriangledown_{\theta}\pi(s)\left(\bigtriangledown_{a}Q_{\pi}(s,a)\right)\vert_{a=\pi(s,\theta)}\\
&=E_{s\sim \eta}\left[\bigtriangledown_{\theta}\pi(s)\left(\bigtriangledown_{a}Q_{\pi}(s,a)\right)\vert_{a=\pi(s,\theta)}\right]
\end{aligned}</script><ul><li>$\eta$ 为状态 $s$ 的分布，具体表达式由探索策略下状态的稳态分布与基于策略 $\pi$ 的状态转移确定</li><li>对于策略 $\pi$ 下的动作价值 $Q_{\pi}(s,a)$ ，先对 $a$ 求梯度，然后将所有的动作替换为 $\pi(s)$ ——二者是等价的</li></ul><h5 id="DPG天然是异策略"><a href="#DPG天然是异策略" class="headerlink" title="DPG天然是异策略"></a>DPG天然是异策略</h5><p>首先，actor 是异策略的</p><ul><li><p>求策略度量梯度时并不涉及动作的分布，因为这个动作 $a$ 会被替换为 $\pi(s)$ ，所以不需要 $A$ 对应的分布</p><p>之后使用随机梯度上升法相当于对真实的梯度进行随机采样，若在采样时，给定了一个 $s_t$ ，根据这个状态 $s_t$ 可以得到 $a_t$ ，并不需要关心这个 $a_t$ 是哪个策略得到的，因此可以使用任何的探索策略，即DPG天然是异策略的</p></li></ul><p>其次，critic 也是异策略的</p><ul><li><p>对价值函数的近似需要的经验样本是 $(s_t,a_t,r_{t+1},s_{t+1},\tilde{a}_{t+1})$ ，$\tilde{a}_{t+1}=\pi(s_{t+1})$ ，这个经验样本的生成涉及两个策略</p><p>第一个策略是对状态 $s_t$ 时生成 $a_t$ ，这个策略是探索策略，$a_{t}$ 用于与环境交互</p><p>第二个策略是对状态 $s_{t+1}$ 生成 $\tilde{a}_{t+1}$ ，这个策略必须是 $\pi$ ，因为这是 critic 要去评价的策略 ，所以 $\pi$ 是目标策略，$\tilde{a}_{t+1}$ 在下一时刻并不会用于与环境实际交互</p></li></ul><h5 id="两种常用的度量指标梯度"><a href="#两种常用的度量指标梯度" class="headerlink" title="两种常用的度量指标梯度"></a>两种常用的度量指标梯度</h5><h6 id="平均状态价值-2"><a href="#平均状态价值-2" class="headerlink" title="平均状态价值"></a>平均状态价值</h6><p>将平均状态价值的折扣情况作为策略度量指标</p><script type="math/tex;mode=display">J(\theta)=\overline{V}_{\pi}=E[V_{\pi}(S)]=\sum\limits_{s\in \mathcal{S}}d_0(s)V_{\pi}(s)</script><p>其中，$\sum\limits_{s\in \mathcal{S}}d_0(s)=1$ ，为了便于计算，状态服从与策略 $\pi$ 独立的稳态分布</p><blockquote><p>关于状态分布的选择：</p><ul><li><p>$d_0(s_0)=1$ 且 $d_0(s\neq s_0)=0$ ，其中状态 $s_0$ 是我们关注的起始状态</p><p>策略优化的目标是最大化从 $s_0$ 开始的折扣奖励</p></li><li><p>$d_0$ 是另一个不同于 $\pi$ 的探索策略 $\mu$ 下的稳态分布</p><p>与异策略有关，因为DPG算法天然是异策略的，不需要重要性采样进行转换</p></li></ul></blockquote><p>为计算目标函数的梯度，首先要计算 $\gamma\in (0,1)$ 时 $V_{\pi}(s),\forall s\in \mathcal{S}$ 的梯度</p><script type="math/tex;mode=display">\bigtriangledown_{\theta}V_{\pi}(s)=\sum\limits_{s'\in\mathcal{S}}Pr_{\pi}(s'\vert s)\bigtriangledown_{\theta}\pi(a\vert s',\theta)\left(\bigtriangledown_aQ_{\pi}(s',a)\right)\vert_{a=\pi(s')}</script><ul><li>其中，$Pr_{\pi}(s’\vert s)=\sum\limits_{k=0}^{\infty}\gamma^k\left[P^{k}_{\pi}\right]_{ss’}=\left[(I-\gamma P_{\pi})^{-1}\right]_{ss’}$ ，表示基于策略 $\pi$ 从状态 $s$ 转移到 $s’$ 的概率，其中 $[\cdot]_{ss’}$ 表示矩阵第 $s$ 行，第 $s’$ 列的值</li></ul><p>策略度量指标的梯度 $\bigtriangledown_\theta J(\theta)$ 为</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_\theta J(\theta)&=\sum\limits_{s\in \mathcal{S}}\eta_{\pi}(s)\bigtriangledown_\theta \pi(s,\theta)(\bigtriangledown_a Q_{\pi}(s,a))\vert_{a=\pi(s)}\\
&=E_{S\sim \eta_{\pi}}\left[\bigtriangledown_\theta \pi(s,\theta)(\bigtriangledown_a Q_{\pi}(s,a))\vert_{a=\pi(s)}\right]
\end{aligned}</script><p>其中，$\eta_{\pi}(s)=\sum\limits_{s’\in \mathcal{S}}d_0(s’)Pr_{\pi}(s\vert s’)$</p><ul><li>其中，$Pr_{\pi}(s\vert s’)=\sum\limits_{k=0}^{\infty}\gamma^k\left[P^{k}_{\pi}\right]_{s’s}=\left[(I-\gamma P_{\pi})^{-1}\right]_{s’s}$ ，表示基于策略 $\pi$ 从状态 $s’$ 转移到 $s$ 的概率，其中 $[\cdot]_{s’s}$ 表示矩阵第 $s’$ 行，第 $s$ 列的值</li></ul><h6 id="平均单步立即奖励-2"><a href="#平均单步立即奖励-2" class="headerlink" title="平均单步立即奖励"></a>平均单步立即奖励</h6><p>将平均单步立即奖励作为策略度量指标</p><script type="math/tex;mode=display">J(\theta)=\overline{r}_{\pi}=\sum\limits_{s\in \mathcal{S}}d_\pi(s)r_{\pi}(s)=E_{S\sim d_{\pi}}[r_{\pi}(S)]</script><p>其中，$r_{\pi}(s)=E[R\vert s,a=\pi(s,\theta)]=\sum\limits_{r}rP(r\vert s,a=\pi(s,\theta))$</p><p>策略梯度为</p><script type="math/tex;mode=display">\begin{aligned}
\bigtriangledown_{\theta}J(\theta)&=\sum\limits_{s\in \mathcal{S}}d_{\pi}(s)\bigtriangledown_\theta\pi(a\vert s,\theta)\left(\bigtriangledown_a Q_{\pi}(s,a)\right)\vert_{a=\pi(s,\theta)}\\
&=E_{S\sim d_{\pi}}\left[\bigtriangledown_\theta\pi(a\vert s,\theta)\left(\bigtriangledown_a Q_{\pi}(s,a)\right)\vert_{a=\pi(s,\theta)}\right]
\end{aligned}</script><ul><li>其中，$d_{\pi}$ 为状态 $S$ 基于策略 $\pi$ 的稳态分布</li></ul><h4 id="DPG算法"><a href="#DPG算法" class="headerlink" title="DPG算法"></a>DPG算法</h4><p>基于策略梯度，使用梯度上升法最大化策略度量函数</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha_{\theta}E_{S\sim \eta_{\pi}}\left[\bigtriangledown_\theta \pi(s,\theta)(\bigtriangledown_a Q_{\pi}(s,a))\vert_{a=\pi(s)}\right]</script><p>相应的使用随机梯度上升法去近似</p><script type="math/tex;mode=display">\theta_{t+1}=\theta_{t}+\alpha_{\theta}\bigtriangledown_\theta \pi(s_t,\theta_{t})(\bigtriangledown_a Q_{\pi}(s_t,a))\vert_{a=\pi(s_t)}</script><h4 id="伪代码-2"><a href="#伪代码-2" class="headerlink" title="伪代码"></a>伪代码</h4><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&初始化：探索策略为\mu(a\vert s),带参确定性目标策略函数 \pi(s,\theta_{0}),\gamma\in (0,1),\alpha>0\\
&\qquad \qquad 状态价值近似函数 \hat{V}(s,w^{(0)})\\
&目标：寻找最优策略即最大化目标函数 J(\theta)\\
& 对于每个回合的t时刻：\\
&\quad 基于探索策略 \mu(s_t,\theta_{t})生成动作 a_t,获得r_{t+1},s_{t+1}\\
&\quad TD误差（优势函数）：\\
&\qquad \hat{\delta}_t=r_{t+1}+\gamma \hat{Q}(s_{t+1},\pi(s_{t+1},\theta_{t}),w_{t})-\hat{Q}(s_t,a_t,w_{t})\\
&\quad Critic(价值更新):\\
&\qquad w_{t+1}=w_{t}+\alpha_w \hat{\delta}_t\bigtriangledown_w \hat{Q}(s_t,a_t,w_{t})\\
&\quad Actor(策略更新):\\
&\qquad \theta_{t+1}=\theta_{t}+\alpha_{\theta}\bigtriangledown_\theta \pi(s_t,\theta_{t})(\bigtriangledown_a \hat{Q}_{\pi}(s_t,a,w_{t+1}))\vert_{a=\pi(s_t)}
\\
\hline
\end{array}</script><p>对于探索策略 $\mu$ ，也可以将其变为 $\pi+噪音$</p><p>每次得到一个 $\pi(s)$ 后，因为 $\pi$ 本身是确定性是，不能探索，所以加上一些噪音，可以有一定的随机性，下一个动作的生成就与目标策略 $\pi$ 有了关系</p><ul><li>实质上，$\pi+噪音$ 与 $\varepsilon-贪心$ 非常类似，但这里不能用贪心算法，因为此处应对的时动作空间连续的情况，不能给无限的连续动作赋予一定探索概率</li></ul><p>此时，DPG可以变为同策略算法</p><h4 id="DPG的进一步改进"><a href="#DPG的进一步改进" class="headerlink" title="DPG的进一步改进"></a>DPG的进一步改进</h4><p>使用不同的价值近似基函数去近似 $\hat{Q}(s,a,w)$ 会得到不同的DPG改进算法</p><ul><li><p>线性函数：$\hat{Q}(s_t,a_t,w_{t})=\phi^T(s,a)\cdot w$</p><p>线性近似基函数的难点在于基函数(特征向量)的选择，由于函数结构的限制，逼近动作价值的能力有限</p></li><li><p>神经网络 ：DDPG</p></li></ul><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337976595">https://zhuanlan.zhihu.com/p/337976595</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/4136854086/" title="5-基于策略梯度的RL">https://amostian.github.io/posts/4136854086/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 强化学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/2376627899/" rel="prev" title="4-基于价值函数的RL"><i class="fa fa-chevron-left"></i> 4-基于价值函数的RL</a></div><div class="post-nav-item"><a href="/posts/3226843952/" rel="next" title="0.动手学深度学习">0.动手学深度学习 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-text">5.1 策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-1-%E7%AD%96%E7%95%A5%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="nav-text">5.1.1 策略的表示</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A8%E6%A0%BC%E5%9E%8B%E7%AD%96%E7%95%A5"><span class="nav-text">表格型策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%9E%8B%E7%AD%96%E7%95%A5"><span class="nav-text">函数型策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A8%E6%A0%BC%E5%9E%8BVS%E5%87%BD%E6%95%B0%E5%9E%8B"><span class="nav-text">表格型VS函数型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-text">策略梯度基本思路</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-2-%E7%AD%96%E7%95%A5%E5%BA%A6%E9%87%8F%E6%8C%87%E6%A0%87"><span class="nav-text">5.1.2 策略度量指标</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC"><span class="nav-text">平均状态价值</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#d-%E4%B8%8E%E7%AD%96%E7%95%A5-pi-%E6%97%A0%E5%85%B3"><span class="nav-text">$d$ 与策略 $\pi$ 无关</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-%E4%B8%8E%E7%AD%96%E7%95%A5-pi-%E6%9C%89%E5%85%B3"><span class="nav-text">$d$ 与策略 $\pi$ 有关</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E5%8D%95%E6%AD%A5%E7%AB%8B%E5%8D%B3%E5%A5%96%E5%8A%B1"><span class="nav-text">平均单步立即奖励</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E5%BA%A6%E9%87%8F%E6%8C%87%E6%A0%87%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">两种度量指标的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%89%E4%BB%B7%E5%AE%9A%E4%B9%89"><span class="nav-text">等价定义</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC-1"><span class="nav-text">平均状态价值</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E5%8D%95%E6%AD%A5%E7%AB%8B%E5%8D%B3%E5%A5%96%E5%8A%B1-1"><span class="nav-text">平均单步立即奖励</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-3-%E7%AD%96%E7%95%A5%E5%BA%A6%E9%87%8F%E6%8C%87%E6%A0%87%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="nav-text">5.1.3 策略度量指标的梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">策略梯度的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B3%A8"><span class="nav-text">注</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-4-%E4%B8%8E%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%AF%B9%E6%AF%94"><span class="nav-text">5.1.4 与基于价值的学习对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E5%9F%BA%E4%BA%8E%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E2%80%94%E2%80%94REINFORCE"><span class="nav-text">5.2 基于蒙特卡洛的策略梯度——REINFORCE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3-%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-text">5.2.3 伪代码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%87%E6%A0%B7"><span class="nav-text">采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#REINFORCE%E6%98%AF%E7%A6%BB%E7%BA%BF%E6%96%B9%E6%B3%95"><span class="nav-text">REINFORCE是离线方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90"><span class="nav-text">5.2.2 算法分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#beta-t-frac-Q-pi-s-t-a-t-pi-a-t-vert-s-t-theta-t-%E8%83%BD%E5%BE%88%E5%A5%BD%E5%B9%B3%E8%A1%A1%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8"><span class="nav-text">$\beta_t&#x3D;\frac{Q_{\pi}(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}$ 能很好平衡探索与利用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-AC%E6%96%B9%E6%B3%95"><span class="nav-text">5.3 AC方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-QAC"><span class="nav-text">5.3.1 QAC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#QAC%E6%98%AF%E5%90%8C%E7%AD%96%E7%95%A5-on-policy-%E7%AE%97%E6%B3%95"><span class="nav-text">QAC是同策略 on-policy 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#QAC%E6%98%AF%E9%9A%8F%E6%9C%BA%E6%80%A7%E7%AD%96%E7%95%A5"><span class="nav-text">QAC是随机性策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-2-A2C"><span class="nav-text">5.3.2 A2C</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-text">算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-text">伪代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A2C%E6%98%AF%E5%90%8C%E7%AD%96%E7%95%A5%E7%AE%97%E6%B3%95"><span class="nav-text">A2C是同策略算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A2C%E6%98%AF%E9%9A%8F%E6%9C%BA%E6%80%A7%E7%AD%96%E7%95%A5"><span class="nav-text">A2C是随机性策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mbox-step-size-frac-delta-t-s-t-a-t-pi-a-t-vert-s-t-theta-t-%E8%83%BD%E5%BE%88%E5%A5%BD%E5%B9%B3%E8%A1%A1%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8"><span class="nav-text">$\mbox{step size}&#x3D;\frac{\delta_t(s_t,a_t)}{\pi(a_t\vert s_t,\theta_{t})}$ 能很好平衡探索与利用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-3-%E5%BC%82%E7%AD%96%E7%95%A5AC%E7%AE%97%E6%B3%95"><span class="nav-text">5.3.3 异策略AC算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E7%AD%96%E7%95%A5%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-text">异策略策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%82%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-text">异策略梯度表达式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%82%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96"><span class="nav-text">异策略梯度优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%85%E5%88%86%E5%88%A9%E7%94%A8%E7%9A%84%E7%AE%97%E6%B3%95"><span class="nav-text">充分利用的算法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81-1"><span class="nav-text">伪代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-4-Deterministic-actor-critic-DPG"><span class="nav-text">5.3.4 Deterministic actor-critic(DPG)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">梯度的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#DPG%E5%A4%A9%E7%84%B6%E6%98%AF%E5%BC%82%E7%AD%96%E7%95%A5"><span class="nav-text">DPG天然是异策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E5%BA%A6%E9%87%8F%E6%8C%87%E6%A0%87%E6%A2%AF%E5%BA%A6"><span class="nav-text">两种常用的度量指标梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC-2"><span class="nav-text">平均状态价值</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E5%8D%95%E6%AD%A5%E7%AB%8B%E5%8D%B3%E5%A5%96%E5%8A%B1-2"><span class="nav-text">平均单步立即奖励</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DPG%E7%AE%97%E6%B3%95"><span class="nav-text">DPG算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81-2"><span class="nav-text">伪代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DPG%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%94%B9%E8%BF%9B"><span class="nav-text">DPG的进一步改进</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">236</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">68</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">83</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1252.1k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">50:25</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>