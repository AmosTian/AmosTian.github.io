<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本文 翻译自 Ketan Doshi 博客中关于 Transformers Explained Visually 的系列文章  Overview of Functionality ：Components of the architecture, and behavior during Training and Inference How it works, step-by-step ：How d"><meta property="og:type" content="article"><meta property="og:title" content="10. Transformer 原理"><meta property="og:url" content="https://amostian.github.io/posts/3545351090/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="本文 翻译自 Ketan Doshi 博客中关于 Transformers Explained Visually 的系列文章  Overview of Functionality ：Components of the architecture, and behavior during Training and Inference How it works, step-by-step ：How d"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414125225955.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414163113008.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414163021399.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414130155998.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414131310009.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414131426685.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414131809508.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414171853029.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414172005575.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/transformer_resideual_layer_norm_3.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240414135759741.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/640-17130895386055.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Attn-2.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Attn-3.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Data-1-17132884884491.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/transformer_decoding_2.gif"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Data-2.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/transformer_decoding_1.gif"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Advtg-17132885468592.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Embedding-1.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Embedding-3.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Embedding-7.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415145600033.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415145505744.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415103143870.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415151415693.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415153403457.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Output.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/transformer_decoder_output_softmax.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Loss.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240418003804764.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240418003753754.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Attn-1.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Input-1.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415161321459.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415161457102.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Linear.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415162325078.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416000914007.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416001002661.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Split-3.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/aa.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Score-1.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Score-2.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Score-3.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Score-4.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415163207665.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416134258930.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Multi-1.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415163824164.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415165016144.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240415165040121.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240417211051299.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240417211126832.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240417225220085.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240417225612311.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Data-1-171325543835312.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Data-3.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416203411111.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Attn-1-171327116618220.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416205217708.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416205330516.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/Attn-7.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240416220134327.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240422002511792.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240426190244181.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240422005628881.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240422005706897.png"><meta property="og:image" content="https://amostian.github.io/posts/3545351090/image-20240422005732358.png"><meta property="article:published_time" content="2024-04-16T17:26:16.000Z"><meta property="article:modified_time" content="2024-10-04T11:26:29.274Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/3545351090/image-20240414125225955.png"><link rel="canonical" href="https://amostian.github.io/posts/3545351090/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>10. Transformer 原理 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">65</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">82</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">216</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/3545351090/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">10. Transformer 原理</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-04-17 01:26:16" itemprop="dateCreated datePublished" datetime="2024-04-17T01:26:16+08:00">2024-04-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-10-04 19:26:29" itemprop="dateModified" datetime="2024-10-04T19:26:29+08:00">2024-10-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">13k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>20 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UJmna6_ouNzq1oiGas2Amg">本文</a> 翻译自 Ketan Doshi 博客中关于 Transformers Explained Visually 的系列文章</p><ul><li><a target="_blank" rel="noopener" href="https://ketanhdoshi.github.io/Transformers-Overview/">Overview of Functionality</a> ：<em>Components of the architecture, and behavior during Training and Inference</em></li><li><a target="_blank" rel="noopener" href="https://ketanhdoshi.github.io/Transformers-Arch/">How it works, step-by-step</a> ：<em>How data flows and what computations are performed, including matrix representations</em></li><li><a target="_blank" rel="noopener" href="https://ketanhdoshi.github.io/Transformers-Attention/">Multi-head Attention</a> ：<em>Inner workings of the Attention module throughout the Transformer</em></li><li><a target="_blank" rel="noopener" href="https://ketanhdoshi.github.io/Transformers-Why/">Why Attention Boosts Performance</a>：<em>How does Attention capture the relationships between words in a sentence</em></li></ul><p><a target="_blank" rel="noopener" href="http://fancyerii.github.io/2019/03/09/transformer-illustrated/">http://fancyerii.github.io/2019/03/09/transformer-illustrated/</a></p></blockquote><p>[TOC]</p><span id="more"></span><h2 id="10-1-架构及其训练、推理过程"><a href="#10-1-架构及其训练、推理过程" class="headerlink" title="10.1 架构及其训练、推理过程"></a>10.1 架构及其训练、推理过程</h2><p>Transformer是一种继MLP、CNN、RNN之后的深度学习模型，来自于 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 这篇论文，使用注意力机制最初是为了提高深度学习 NLP 翻译模型的性能</p><p>RNN是顺序执行的，$t$ 时刻的 <code>token</code> 处理完才能处理 $t+1$ 时刻的 <code>token</code> ，很难并行。</p><p>Transformer模型的 <em>Self-Attention</em> 机制和 <em>Position Encoding</em> 可以代替RNN</p><ul><li><strong>Self-Attention 机制允许模型并行处理输入序列的所有 <code>token</code></strong> ，更适合处理长序列</li></ul><p><strong>Self-Attention</strong> 的效果很好，在很多其他的地方也可以用Transformer模型：GPT和BERT都使用了Decoder部分，由于没有Encoder，所以Decoder只有 <em>Self-Attention</em> 层而没有 <em>Encoder-Decoder Attention</em> 层</p><h3 id="10-1-1-Transformer架构"><a href="#10-1-1-Transformer架构" class="headerlink" title="10.1.1 Transformer架构"></a>10.1.1 Transformer架构</h3><p>将模型视为一个黑盒，其输入是源序列（法语），输出是目标序列（英语）</p><p><img src="/posts/3545351090/image-20240414125225955.png" alt></p><p>模型内部采用 <strong>Encoder-Decoder</strong> 架构，分为 <strong>编码器堆栈</strong> (Encoder Stack) 和 <strong>解码器堆栈</strong> (Decoder Stack)</p><p><img src="/posts/3545351090/image-20240414163113008.png" alt="image-20240414163113008"></p><p>在 <strong>Encoder Stack</strong> 和 <strong>Decoder Stack</strong> 之前，都有各自的 <strong>词嵌入层和位置编码层</strong></p><p>在 <strong>Decoder Stack</strong> 后，有一个输出层来生成最终的输出</p><ul><li>输出层：$线性层\rightarrow Softmax层$</li></ul><p><img src="/posts/3545351090/image-20240414163021399.png" alt="image-20240414163021399"></p><h4 id="编码器-解码器堆栈"><a href="#编码器-解码器堆栈" class="headerlink" title="编码器/解码器堆栈"></a>编码器/解码器堆栈</h4><p>编码器堆栈和解码器堆栈都由很多结构相同的 <strong>Encoder块</strong> 和 <strong>Decoder块</strong> 堆叠而成</p><p>每个 <strong>Encoder 块</strong> / <strong>Decoder 块</strong> 的结构相同</p><p><img src="/posts/3545351090/image-20240414130155998.png" alt="image-20240414130155998"></p><ul><li><p>每个 <strong>Encoder块</strong> 由一个 <em>Self-Attention</em> 层和一个前馈层组成</p></li><li><p>每个 <strong>Decoder块</strong> 由一个 <em>Self-Attention</em> 层、一个 <em>Encoder-Decoder Attention</em> 层和一个前馈层组成</p><p><em>编码器-解码器注意力层</em> 使 <strong>Decoder块</strong> 在解码时会考虑最后一个 <strong>Encoder块</strong> 在时序窗口内每个时间步的输出</p></li></ul><h5 id="块间的数据流"><a href="#块间的数据流" class="headerlink" title="块间的数据流"></a>块间的数据流</h5><ul><li>Encoder块的输入是前一个Encoder块的输出，最底层Encoder的输入是原始的源输入序列</li><li>Decoder块的输入是前一个Decoder的输出+Encoder最后一层的输出，最底层的Decoder块输入是原始的目标输入序列</li></ul><p>首先获取原始字符类型的输入语料集词表，并将每个原始输入序列转换为数字索引序列，通过Embedding层将每个 <code>token</code> 转换为连续的稠密向量</p><p><img src="/posts/3545351090/image-20240414131310009.png" alt></p><p>Embedding之后的序列会输入 Encoder，经过Self-Attention层后再经过全连接层</p><p><img src="/posts/3545351090/image-20240414131426685.png" alt></p><p>在计算 $\mathbf{z}_t$ 时需要依赖序列中所有时刻的输入 $\mathbf{x}_1,\cdots,\mathbf{x}_T$ ，下图中用大 <em>Self-Attention</em> 方框表示矩阵运算</p><p>前馈层的计算完全独立，计算 $t$ 时刻的输出只需要 $\mathbf{z}_t$ 即可，所以可以并行计算。用 <em>前馈神经网络</em> 方框表示每个时刻输出的计算，但不同时刻的全连接层参数是共享的。</p><ul><li>当前Encoder块的输出 $\mathbf{r}_1,\cdots,\mathbf{r}_n$ 直接输入给下一个Encoder块</li></ul><p><img src="/posts/3545351090/image-20240414131809508.png" alt></p><h4 id="残差连接与归一化层"><a href="#残差连接与归一化层" class="headerlink" title="残差连接与归一化层"></a>残差连接与归一化层</h4><p>块内的 <em>Self-Attention</em> 层、前馈层 与 <em>Encoder-Decoder Attention</em> 层都有残差连接以及正则化层</p><p><img src="/posts/3545351090/image-20240414171853029.png" alt="image-20240414171853029"></p><p><img src="/posts/3545351090/image-20240414172005575.png" alt="image-20240414172005575"></p><p>同样，解码器也添加残差连接与批量归一化层</p><p><img src="/posts/3545351090/transformer_resideual_layer_norm_3.png" alt="img"></p><h3 id="10-1-2-Self-Attention"><a href="#10-1-2-Self-Attention" class="headerlink" title="10.1.2 Self-Attention"></a>10.1.2 Self-Attention</h3><p>Transformer 的突破性表现在于其对注意力的使用</p><h4 id="为什么需要注意力机制"><a href="#为什么需要注意力机制" class="headerlink" title="为什么需要注意力机制"></a>为什么需要注意力机制</h4><p>如翻译 “The animal didn’t cross the street because it was too tired.” 此处的 “it” 指代的是什么，需要在理解 “it” 时同时关注所有的单词，且重点是 “animal” 和 “street” 和 “tired” 。结合”tired” 的先验知识，知道指代的是 “animal”。若 “tired” 变为 “narrow” 则指代的实 “street”</p><blockquote><p>单向LSTM无法实现上述逻辑</p><ul><li>前向LSTM只对历史 <code>token</code> 的时序信息进行提取，在未看到 “tired” 时，并不能理解”it”的语义</li><li>反向LSTM只看到 “tired” 也无发生对 “it” 的语义进行编码</li></ul><p>多层双向LSTM可以编码 “it” 的语义，需要底层的LSTM同时编码了三个关注点的语义，然后用更高层的LSTM将 “it” 编码为 “animal” 的语义</p><p><img src="/posts/3545351090/image-20240414135759741.png" alt></p></blockquote><p>在以下的英文句子中，ball 与 blue 、hold 密切相关。另一方面，boy 与 blue 没有关系。</p><p><img src="/posts/3545351090/640-17130895386055.png" alt="图片"></p><p>Transformer模型将输入序列中的 <code>token</code> 与本序列中的其他 <code>token</code> 关联起来，形成 <em>Self-Attention</em></p><p>使用 <strong>Self-Attention机制</strong> 的Encoder在编码一个 <code>token</code> 时，会考虑序列的所有 <code>token</code> ，再确定对当前 <code>token</code> 进行编码。</p><h4 id="注意力分数"><a href="#注意力分数" class="headerlink" title="注意力分数"></a>注意力分数</h4><p>对于以下两个句子</p><ul><li>The <em>cat</em> drank the milk because <strong>it</strong> was hungry.</li><li>The cat drank the <em>milk</em> because <strong>it</strong> was sweet.</li></ul><p>第一个句子，<strong>it</strong> 指 <em>cat</em> ，第二个句子，<strong>it</strong> 指 <em>milk</em> 。Self-Attention 给了模型更多关于 <strong>it</strong> 意义的信息，这样 <strong>it</strong> 的语义能与正确词元关联起来</p><p>为了使模型能够处理 <code>token</code> 在不同序列中的语义，模型为每个 <code>token</code> 提供了多个注意力分数，颜色越重表示更高的注意力分数</p><p><img src="/posts/3545351090/Attn-2.png" alt="img"></p><p>在处理第一个序列中的 <strong>it</strong> 这个 <code>token</code> 时，第一个分数会突出 <em>cat</em> ，第二个分数会突出 <em>hungry</em> 。将会把 <em>cat</em> 和 <em>hungry</em> 的某些语义性质纳入到目标序列中</p><p><img src="/posts/3545351090/Attn-3.png" alt="img"></p><h3 id="10-1-3-Transformer训练过程"><a href="#10-1-3-Transformer训练过程" class="headerlink" title="10.1.3 Transformer训练过程"></a>10.1.3 Transformer训练过程</h3><p><strong>训练数据</strong></p><p>每条训练数据都包含两部分内容：</p><ul><li>输入序列（源序列）</li><li>输出序列（目标序列）</li></ul><p><strong>训练目标</strong></p><p>对训练数据中源序列和目标序列之间的序列规律进行学习。在推理（预测）过程中，给定源序列，根据学习到的序列规律，生成目标序列</p><p><img src="/posts/3545351090/Data-1-17132884884491.png" alt="Data-1"></p><ol><li>在送入第一个 <strong>Encoder块</strong> 前，源输入序列 <code>src_seq</code> 先经过 <strong>Embedding层</strong> ，并加入位置编码，产生词嵌入表示 <code>src_position_embed</code> 之后送入第一个 <strong>Encoder块</strong></li><li>按 <strong>Encoder Stack</strong> 中 <strong>Encoder 块</strong> 的顺序对 <em>词嵌入表示</em> 进行处理，产生 <strong>Encoders</strong> 的输出 <code>enc_outputs</code></li><li>在右侧的 <strong>Decoder Stack</strong> 中，目标输入序列 <code>tgt_seq</code> 首先加一个句首标记，被转换为（带位置编码）的词嵌入表示 <code>tgt_position_embed</code> ，送入第一个 <strong>Decoder 块</strong></li><li>按 <strong>Decoder Stack</strong> 中 <strong>Decoder 块</strong> 的顺序，将目标输入序列的词嵌入表示与 <strong>Encoder Stack</strong> 的输出 <code>enc_outputs</code> 一起处理，产生目标序列的解码表示 <code>dec_outputs</code></li><li>输出层将解码器输出转换为 <code>token</code> 概率和最终的输出序列 <code>out_seq</code></li><li>损失函数对输出序列 <code>out_seq</code> 与训练数据中的 <code>tgt_seq</code> 计算损失，这个损失的梯度用于反向传播中模型参数的更新</li></ol><h4 id="强制教学"><a href="#强制教学" class="headerlink" title="强制教学"></a>强制教学</h4><p><img src="/posts/3545351090/transformer_decoding_2.gif" alt="img"></p><p>训练时，解码器输入的是真实的目标输入序列，这种方法称为强制教学</p><blockquote><p>在训练时，若从Decoder上一个时间步的输出序列取最后一个 <code>token</code> 作为 Decoder 的输入进行下一个时间步的预测，损失函数将计算生成的序列与目标序列的损失。</p><p>但这种训练机制会造成误差累积，不仅会导致训练时间更长，而且会增加模型的训练难度</p></blockquote><p>而向Decoder提供真实的目标序列，实际上给了Decoder一个提示，即使上一时刻预测错误，也可以用当前时刻正确的输入预测下一时刻，避免了误差累计。</p><p>另一个好处，<strong>Decoder Stack并行地输出所有的 <code>token</code></strong> ，不需要循环，大大加快了训练速度</p><h3 id="10-1-4-Transformer推理过程"><a href="#10-1-4-Transformer推理过程" class="headerlink" title="10.1.4 Transformer推理过程"></a>10.1.4 Transformer推理过程</h3><p><strong>数据</strong></p><p>在推理过程中，数据只有源输入序列</p><p><strong>推理目标</strong></p><p>通过源输入序列产生目标序列</p><ul><li>Seq2Seq模型，<strong>Decoder</strong> 在一个时间步的完整循环中，生成当前时间步的输出 <code>token</code> ，并将其作为下一个时间步的输入 <code>token</code> ，直至生成序列结束标志。</li></ul><p>Transformer在每个时间步，<strong>Decoder 块</strong> 输入直到当前时间步生成的整个输出序列</p><p><img src="/posts/3545351090/Data-2.png" alt="img"></p><ol><li><p>在送入第一个 <strong>Encoder块</strong> 前，源输入序列 <code>src_seq</code> 先经过 <strong>Embedding层</strong> ，并加入位置编码，产生词嵌入表示 <code>src_position_embed</code> 之后送入第一个 <strong>Encoder块</strong></p></li><li><p>按 <strong>Encoder Stack</strong> 中 <strong>Encoder 块</strong> 的顺序对 <em>词嵌入表示</em> 进行处理，产生 <strong>Encoder Stack</strong> 的输出 <code>enc_outputs</code></p></li><li><p>在第一个时间步，使用只有一个序列开始词元的空序列代替训练过程中使用的目标序列。空序列转换为带位置编码的 <code>start_position_embeded</code> ，并被送入第一个 <strong>Decoder 块</strong></p></li><li><p><strong>Decoder Stack</strong> 将空序列词嵌入表示 <code>start_position_embeded</code> 与 <code>enc_outputs</code> 一起处理，产生目标序列第一个 <code>token</code> 的编码表示 <code>step1_dec_outputs</code></p></li><li><p>输出层将 <code>step1_dec_outputs</code> 转换为词概率和第一个目标词元 <code>step1_tgt_token</code></p></li><li><p>将这一步产生的 <code>step1_tgt_token</code> 填入解码器输入序列第二个时间步的位置。</p><p>在第二个时间步， <strong>Decoder Stack</strong> 的输入序列变为 <code>[&lt;bos&gt;,step1_tgt_token]</code></p></li><li><p>将新的解码器输入序列输入 <strong>Decoder Stack</strong> ，取出第二个时间步位置的 <code>token</code> 并将其附加到解码器序列中，生成第三个时间步位置的 <code>step2_tgt_token</code> ，直至 <code>stepTs_tgt_token</code> 为序列结束标志 <code>&lt;eos&gt;</code></p></li></ol><p><img src="/posts/3545351090/transformer_decoding_1.gif" alt="img"></p><h3 id="10-1-5-Transformer为什么优于RNN"><a href="#10-1-5-Transformer为什么优于RNN" class="headerlink" title="10.1.5 Transformer为什么优于RNN"></a>10.1.5 Transformer为什么优于RNN</h3><p><img src="/posts/3545351090/Advtg-17132885468592.png" alt="Advtg"></p><p>RNNs模型的限制</p><ul><li>对于长序列中相距较远的 <code>token</code> ,RNN难以处理长距离依赖关系</li><li>RNNs每个时间步处理序列的一个 <code>token</code> ，意味着完成 <code>T-1</code> 时间步的计算之前，无法进行 时间步 <code>T</code> 的计算。</li></ul><p>CNNs所有的输出都可以并行计算，但不擅长处理长距离依赖关系</p><ul><li>卷积层，只有输入数据的特征足够接近核提取的浅层特征才可以相互作用，对于与核表征的特征较远的高层特征，需要有更深层的网络</li></ul><p>Transformer依赖 Attention 机制的优势：</p><ul><li>并行地处理序列中所有的 <code>token</code> ，大大加快计算速度</li><li>输入序列中 <code>token</code> 之间的距离并不重要，擅长计算相邻 <code>token</code> 与相距远的 <code>token</code> 间的依赖关系</li></ul><h2 id="10-2-数据流"><a href="#10-2-数据流" class="headerlink" title="10.2 数据流"></a>10.2 数据流</h2><h3 id="10-2-1-词嵌入层与位置编码（输入层）"><a href="#10-2-1-词嵌入层与位置编码（输入层）" class="headerlink" title="10.2.1 词嵌入层与位置编码（输入层）"></a>10.2.1 词嵌入层与位置编码（输入层）</h3><p>输入层接收的输入形式是 <code>token</code> 被映射为词表中数字索引的序列</p><p>Transformer的输入层会提取 <code>token</code> 的两个信息：</p><ul><li><strong>Embedding</strong> 层将该 <code>token</code> 的含义进行编码</li><li><strong>Position Encoding</strong> 层将该 <code>token</code> 在序列中的位置进行编码</li></ul><p>在 <strong>Encoder Stack</strong> 和 <strong>Decoder Stack</strong> 底部各有一个 <strong>Embedding</strong> 层和 <strong>Position Encoding</strong> 层</p><p><img src="/posts/3545351090/Embedding-1.png" alt="img"></p><h4 id="嵌入层-Embedding"><a href="#嵌入层-Embedding" class="headerlink" title="嵌入层(Embedding)"></a>嵌入层(Embedding)</h4><p><img src="/posts/3545351090/Embedding-3.png" alt="img"></p><ul><li>在 <strong>Encoder Stack</strong> 中，送入Embedding层的序列被转换为 <em>输入嵌入向量</em> <code>input_embed</code></li><li>在 <strong>Decoder Stack</strong> 中，目标序列右移一个位置（移除 <code>&lt;eos&gt;</code> 词元），在第一个位置插入 <code>&lt;bos&gt;</code> 词元后送入 Embedding 层，转换为 <em>输出嵌入</em> <code>ouput_embed</code></li></ul><p>Embedding 层将序列中每个数字索引的 <code>token</code> 映射为等长的 <em>embedding向量</em> ，</p><p>若语料集中所有词元组成的词表为 $\mathbf{VC}$ ，在 <em>embedding 层</em> 将词表映射到稠密空间 $\mathbf{E}=\mathbf{VC}\cdot \mathbf{W}_{EX}$ 。embed矩阵 $\mathbf{E}\in \mathbb{R}^{vocab_size\times embed_size}\iff \mathbb{R}^{\text{len(词表)}\times \text{len(embedding向量)}}$</p><p>即 <strong>embedding 层将词元向量从离散的、高维的空间映射到连续的、低维的空间</strong></p><p>词嵌入的核心思想 <strong>词元的含义可以通过它们的上下文来定义</strong> ，通过训练过程的迭代，语义相近的词元会被映射到稠密空间中相近的点</p><h4 id="位置编码层-Position-Encoding"><a href="#位置编码层-Position-Encoding" class="headerlink" title="位置编码层(Position Encoding)"></a>位置编码层(Position Encoding)</h4><blockquote><p>RNN 在循环过程中，每个 <code>token</code> 按顺序输入，因此隐含地知道 <code>token</code> 间的相对位置</p><p>Transformer同一序列的所有 <code>token</code> 都是并行输入的，意味着位置信息会丢失，所以必须在输入层添加位置信息</p></blockquote><p>位置编码的计算独立于输入序列，是只取决于 <strong>序列的最大长度</strong> 的固定值</p><p>位置编码层会为序列的每个位置生成独一无二的位置向量，进而为序列生成位置编码矩阵 $PE\in\mathbb{R}^{seq_len\times d_{model}}$ ，为模型提供每个 <code>token</code> 的位置信息</p><p><img src="/posts/3545351090/Embedding-7.png" alt="img"></p><script type="math/tex;mode=display">PE_{pos, 2i}=\sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)\\
PE_{pos, 2i+1}=\cos\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)</script><ul><li><p>$pos$ 表示当前 <code>token</code> 在输入序列中的位置，从 $0$ 开始，到 $\max\limits_j{\{seq_j_size\}}-1$ 结束</p></li><li><p>$i$ 表示 <em>Position Encoding</em> 矩阵中的列，决定了每个 <code>token</code> 位置编码向量第 $i$ 维的值</p><ul><li><p>偶数列，使用正弦函数计算</p></li><li><p>奇数列，使用余弦函数计算</p></li></ul></li><li><p>$d_{model}$ 是编码向量的长度，与 <em>embed向量</em> 长度相同</p></li></ul><p>位置编码交织了一系列正弦曲线和余弦曲线，位置编码向量中，所有偶数索引使用正弦值，所有奇数索引使用余弦值</p><h4 id="批量数据"><a href="#批量数据" class="headerlink" title="批量数据"></a>批量数据</h4><p>深度学习模型一次处理一个批量的训练样本。</p><p>首先通过词表将序列样本映射为数字索引序列，再通过裁剪与填充将一个批量的样本预处理为等长的批量数字索引矩阵</p><p><img src="/posts/3545351090/image-20240415145600033.png" alt="image-20240415145600033"></p><p>嵌入层和位置编码层对一批序列样本的数字索引矩阵进行运算。</p><ul><li>嵌入层的输入是 $(batch_size,seq_len)$ 的批量化数字索引序列矩阵</li><li>嵌入层将每个数字索引 <code>token</code> 编码为一个 <code>embed</code> 向量，一个数字索引序列的词嵌入矩阵形状为 $(seq_len,embed_size)$</li><li>输出一个批量的词嵌入矩阵， $(batch_size,seq_len,embed_size)$ 的张量</li></ul><p>位置编码层的列数与 $embed_size$ 相等</p><ul><li>位置编码层的输入是 $(batch_size,seq_len)$ 的批量化数字索引序列矩阵</li><li>位置编码层为每个序列的每个位置生成位置编码向量，一个序列数据的位置编码矩阵形状为 $(seq_len,embed_size)$</li><li>输出是 $(batch_size,seq_len,embed_size)$ 的张量</li></ul><p><img src="/posts/3545351090/image-20240415145505744.png" alt="image-20240415145505744"></p><h4 id="输入层的输出"><a href="#输入层的输出" class="headerlink" title="输入层的输出"></a>输入层的输出</h4><p>把这个Positional Encoding与embedding的值相加，作为输入送到下一层。</p><p><img src="/posts/3545351090/image-20240415103143870.png" alt="image-20240415103143870"></p><h3 id="10-2-2-编码器"><a href="#10-2-2-编码器" class="headerlink" title="10.2.2 编码器"></a>10.2.2 编码器</h3><p><strong>Encoder Stack</strong> 通常由多个 <strong>Encoder 块</strong> 组成，按顺序连接</p><p><img src="/posts/3545351090/image-20240415151415693.png" alt="image-20240415151415693"></p><p>编码器堆栈第一个 <strong>Encoder 块</strong> 从输入层接收 <code>src_position_embed</code> 作为输入。</p><p>堆栈中其他 <strong>Encoder 块</strong> 将前一个编码器的输出 <code>enc1_outputs</code> 作为其输入</p><p>在每个 <strong>Encoder 块</strong> 内将输入传入当前编码器的 <em>Self-Attention</em> 层，<em>Self-Attention</em> 层的输出被传入 <em>前馈层</em> ，然后将其作为 <strong>Encoder 块</strong> 的输出 <code>enc2_outputs</code>，并被送入下一 <strong>Encoder 块</strong></p><ul><li>自注意力层和前馈层都会接入一个残差连接，各层之后有一个正则化层</li></ul><p>基于位置的前馈网络由一个线性层、激活函数和另一个线性层组成，这个前馈网络不会改变输入的形状</p><ul><li><p>输入形状为 $(batch_size,seq_len,embed_size)$ 的 $\mathbf{X}$ 被两层MLP转换为 $(batch_size,seq_len,ffn_num_ouputs)$ 的输出张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,ffn_num_inputs, ffn_num_hiddens, ffn_num_outputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear((ffn_num_inputs, ffn_num_hiddens))</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear((ffn_num_hiddens, ffn_num_outputs))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure></li></ul><p><strong>Encoder Stack</strong> 中最后一个 <strong>Encoder 块</strong> 的输出，会送入 <strong>Decoder Stack</strong> 的第一个 <strong>Decoder 块</strong></p><h3 id="10-2-3-解码器"><a href="#10-2-3-解码器" class="headerlink" title="10.2.3 解码器"></a>10.2.3 解码器</h3><p><strong>Decoder Stack</strong> 通常由多个 <strong>Decoder 块</strong> 组成，按顺序连接</p><p><img src="/posts/3545351090/image-20240415153403457.png" alt="image-20240415153403457"></p><p>解码器堆栈第一个 <strong>Decoder 块</strong> 从输入层接收真实的 <code>tgt_position_embed</code> 作为输入。堆栈中其他 <strong>Decoder 块</strong> 从前一个解码器接收他们的输入</p><p>对于一个 <strong>Decoder 块</strong></p><ul><li><p>输入首先进入 <em>Self_Attention</em> 层，这一层的运行方式与编码器 <em>Self_Attention</em> 层有区别：</p><ul><li><p>训练过程，解码器的自注意力层接收整个真实的输出序列，但为了避免生成当前时间步输出看到未来时间步的数据，使用 <strong>掩码机制</strong> ，在生成第 $t$ 个时间步的输出时，模型只能看到第 $1$ 到第 $t-1$ 个时间步的 <code>token</code> 序列</p></li><li><p>推理过程，每个时间步的输入，是到当前时间步的整个输出序列，也是从第 $1$ 到第 $t-1$ 个时间步的 <code>token</code> 序列</p></li></ul></li></ul><ul><li><strong>Encoder-Decoder Attention</strong> 层，工作方式与 <em>Self-Attention</em> 层类似，只是其输入来源有两处：自注意力层的输出以及编码器输出 <code>enc6_outputs</code> ；其输出被传入前馈层</li></ul><p>自注意力层、编码器-解码器注意力层和前馈层都会接入一个残差连接，各层之后都有一个正则化层</p><p>整个 <strong>Decoder 块</strong> 的输出为 <code>dec1_outputs</code> ，送入下一 <strong>Decoder 块</strong></p><h3 id="10-2-4-输出层"><a href="#10-2-4-输出层" class="headerlink" title="10.2.4 输出层"></a>10.2.4 输出层</h3><p><strong>Decoder Stack</strong> 中的最后一个 <strong>Decoder 块</strong> ，将其输出传递到输出层，输出层将其转换为最终的目标序列</p><p><img src="/posts/3545351090/Output.png" alt="img"></p><p>线性层是一个简单的全连接神经网络，将最后一个 <strong>Decoder 块</strong> 的输出映射到单词分数向量中，</p><p>目标词表中的每个 <code>token</code> 在序列中的每个位置都有一个的置信度</p><ul><li><p>若词表有 $V$ 个单词，目标序列有效长度为 $T$ ，则会生成 $T$ 个 $V$ 维的向量</p><p>第 $t$ 个向量的第 $j$ 个值表示词表中数字索引为 $j$ 的 <code>token</code> 出现在生成序列的第 $t$ 个位置的置信度</p></li></ul><p>softmax层将这些置信度变为概率值，在每个位置找到概率最高的值，将其索引转换为目标词表中相应的词元，构成模型的输出序列</p><p><img src="/posts/3545351090/transformer_decoder_output_softmax.png" alt="img"></p><h3 id="10-2-5-训练与损失函数"><a href="#10-2-5-训练与损失函数" class="headerlink" title="10.2.5 训练与损失函数"></a>10.2.5 训练与损失函数</h3><p>使用交叉熵作为损失函数，比较生成的输出序列概率分布和真实的目标序列。</p><p>生成的输出序列概率分布给出了真实序列中每个位置的真实 <code>token</code> 出现的概率</p><p><img src="/posts/3545351090/Loss.png" alt="img"></p><p>如：目标序列长度为4，在真实序列中，第一个位置的 <code>token=&quot;De&quot;</code> ，出现在该位置的概率 $p_r=1$</p><p>而在生成序列中，该 <code>token</code> 出现的概率为 $p_g$</p><p>通过计算二者交叉熵损失的梯度，可以反向传播来更新参数训练模型</p><hr><p>如：真实的句子为 “I am a student<eos>“</eos></p><p><img src="/posts/3545351090/image-20240418003804764.png" alt="image-20240418003804764"></p><p>在足够大的数据集上训练模型足够长时间后，生成的概率分布如下</p><p><img src="/posts/3545351090/image-20240418003753754.png" alt="image-20240418003753754"></p><p>由于模型一次产生一个输出，因此我们可以假设模型从该概率分布中选择概率最高的单词，并丢弃其余的单词。这是一种方法（称为贪婪解码）。</p><p>另一种方法是保留最上面的两个单词（例如“I”和“a”），然后在下一步中运行模型两次：一次假设第一个输出位置是单词“I”，另一次假设第一个输出位置是单词“a”，并且考虑到位置#1和#2，保留产生较少错误的版本。我们对位置 #2 和 #3 等重复此操作。这种方法称为“束搜索”，在我们的示例中，beam_size 为 2（意味着在任何时候，内存中都会保存两个部分假设（未完成的翻译）），top_beams 也是 2（意味着我们将返回两个翻译） ）</p><p>这些都是可以试验的超参数</p><h2 id="10-3-编码器-解码器的块内计算"><a href="#10-3-编码器-解码器的块内计算" class="headerlink" title="10.3 编码器/解码器的块内计算"></a>10.3 编码器/解码器的块内计算</h2><h3 id="10-3-1-注意力层的计算"><a href="#10-3-1-注意力层的计算" class="headerlink" title="10.3.1 注意力层的计算"></a>10.3.1 注意力层的计算</h3><p><strong>Attention</strong> 被用在三个地方</p><p><img src="/posts/3545351090/Attn-1.png" alt="img"></p><ul><li>编码器的 <em>Self-Attention</em> 层：源序列对自身的注意力分数计算</li><li>解码器的 <em>Self-Attention</em> 层：目标序列对自身的注意力分数计算</li><li>解码器的 <em>Encoder-Decoder Attention</em> 层：目标序列对源序列的注意力分数计算</li></ul><h4 id="注意力层的输入"><a href="#注意力层的输入" class="headerlink" title="注意力层的输入"></a>注意力层的输入</h4><p>序列数据经过 <strong>Embedding</strong> 层和 <strong>Position Encoding</strong> 层后，编码器输入数据 <code>position_embed</code> 向量的形状为 $(batch_size,seq_len,embed_size)$ ，包含了输入序列的位置信息和序列中每个 <code>token</code> 的语义信息。</p><p><img src="/posts/3545351090/Input-1.png" alt="img"></p><h5 id="编码器层"><a href="#编码器层" class="headerlink" title="编码器层"></a>编码器层</h5><p>在 <strong>Encoder 块</strong> 中的 <em>Self-Attention</em> 层，输入被复制三份作为 <code>Query</code> ，<code>Key</code> ，<code>Value</code> 线性层的输入，与各线性层的参数矩阵相乘，产生 QKV 矩阵，融合后生成 <em>Self-Attention</em> 层的输出</p><p><img src="/posts/3545351090/image-20240415161321459.png" alt="image-20240415161321459"></p><ul><li><p>每个 <strong>Encoder 块</strong> 的输出包含了本编码器的 <em>Self-Attention</em> 层对序列中每个 <code>token</code> 的注意力分数。</p></li><li><p>当依次通过 <strong>Encoder Stack</strong> 的所有编码器时，每个 <em>Self-Attention</em> 层将自己的注意力分数添加到本层的输出张量中。</p></li></ul><h5 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h5><p><img src="/posts/3545351090/image-20240415161457102.png" alt="image-20240415161457102"></p><p>在 <strong>Decoder 块</strong> 的 <em>Self-Attention</em> 层，通过相同的方式得到 <code>Query</code> ，<code>Key</code> ，<code>Value</code> 线性层的输入，转换为QKV矩阵后，通过正则化层，送至 <em>Encoder-Decoder Attention</em> 层。</p><ul><li><em>Self-Attention</em> 层为目标序列中的每个 <code>token</code> 生成编码表示，包含了本层对每个 <code>token</code> 的注意力分数。</li></ul><p><em>Encoder-Decoder Attention</em> 层，将编码器的最后输出 <code>enc6_outputs</code> 作为 <code>Key</code> 和 <code>Value</code> 矩阵线性层的输入，<strong>Decoder 块</strong> 的输入序列在通过 <em>Self-Attention</em> 层和 正则化层之后作为 <code>Query</code> 矩阵线性层的输入。即 <em>Encoder-Decoder Attention</em> 层同时获得目标序列的表示（来自解码器的 <em>Self-Attention</em> 层）和源序列的表征（来自编码器堆栈）</p><ul><li><em>Encoder-Decoder Attention</em> 层为每个目标序列的 <code>token</code> 生成注意力分数，也包含了源序列中注意力分数的影响。</li><li>当它通过堆栈中的所有解码器时，每个 <em>Self-Attention</em> 层和 <em>Encoder-Decoder Attention</em> 层会将自己的注意力分数添加到每个 <code>token</code> 的输出中</li></ul><h4 id="注意力层的线性层"><a href="#注意力层的线性层" class="headerlink" title="注意力层的线性层"></a>注意力层的线性层</h4><p><code>Query</code> 、<code>Key</code>、<code>Value</code> 实际上是三个独立的线性层，每个线性层都有自己的权重参数，输入数据分别与三个线性层参数相乘，产生QKV三个矩阵</p><p><img src="/posts/3545351090/Linear.png" alt="img"></p><h4 id="多头注意力-Multi-head-Attention"><a href="#多头注意力-Multi-head-Attention" class="headerlink" title="多头注意力(Multi-head Attention)"></a>多头注意力(Multi-head Attention)</h4><p>在 Transformer 中，注意力模块并行重复多次计算，每个并行计算单元被称为 <strong>注意力头</strong> 。</p><p>注意力模块将输入参数：查询(query)，键(key)，值(value) 的参数矩阵进行 $N$ 种拆分（相当于CNN中的N个通道）并通过独立的注意力头传递每种拆分，<strong>每种拆分关注不同的上下文</strong>。最后，将所有注意力计算组合在一起，产生最终的注意力分数。从而捕捉 <code>token</code> 间的联系与差异</p><p><img src="/posts/3545351090/image-20240415162325078.png" alt="image-20240415162325078"></p><h5 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h5><ul><li><code>embed_size</code> ：嵌入层向量的大小</li><li><code>Query_size=Key_size=Value_size</code> ：查询/键/值向量的长度，是用于产生QKV矩阵的三个线性层权重的长度</li><li><code>n_heads</code> ：注意力头个数</li></ul><h5 id="1-通过注意力头拆分注意力层数据"><a href="#1-通过注意力头拆分注意力层数据" class="headerlink" title="1. 通过注意力头拆分注意力层数据"></a>1. 通过注意力头拆分注意力层数据</h5><p>不同时序特征的上下文被拆分到多个注意力头中，每个注意力头都能独立地处理相应的数据</p><ul><li>数据只是 <strong>逻辑</strong> 上的拆分，对于QKV矩阵而言，并没有真正的划分为多个部分给每个注意力头</li><li>逻辑上，每个注意力头提取QKV矩阵的一部分时序特征。各注意力头共用线性层，没有单独的线性层，不同的注意力头在独属于各自的逻辑部分操作</li></ul><p>对数据的拆分分为两个步骤：</p><p><strong>1. 线性层权重矩阵 $\mathbf{W}_{Q,K,V}$ 的拆分</strong></p><p>线性层的逻辑分割，是通过将输入数据与线性层权重均匀地划分到各注意力头完成。</p><script type="math/tex;mode=display">query\_size=\frac{embed\_size}{n\_heads}</script><p><img src="/posts/3545351090/image-20240416000914007.png" alt="image-20240416000914007"></p><p>可以认为线性层权重矩阵是各个注意力头的独立权重矩阵堆叠在一起的</p><p><img src="/posts/3545351090/image-20240416001002661.png" alt="image-20240416001002661"></p><p>因此，在代码中定义 $\mathbf{W}_Q,\mathbf{W}_K,\mathbf{W}_V$ 时，可以定义为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.W_Q = nn.Linear(embedding_size, Query_size * n_heads, bias=<span class="literal">False</span>) </span><br><span class="line">self.W_K = nn.Linear(embedding_size, Key_size * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">self.W_V = nn.Linear(embedding_size, Value_size * n_heads, bias=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>所有注意力头的计算可通过一个矩阵操作实现，而不是 $N$ 个单独的计算，同时保持模型简单：所需线性层更少，同时获得了多头注意力的效果。</p><p>线性层输入 $\mathbf{X}\in \mathbb{R}^{B\times T\times E}$ ，参数矩阵为 $\mathbf{W}\in \mathbb{R}^{B\times E\times (Q_size\times N_heads)}\iff \mathbb{R}^{B\times E\times E}$ ，所以线性层的维度并未发生变化，输出为QKV矩阵， $\mathbf{Q},\mathbf{K},\mathbf{V}\in \mathbb{R}^{B\times T\times E}$</p><p><strong>2. 重塑QKV矩阵形状</strong></p><blockquote><p>经过线性层输出的QKV矩阵需要经过 <code>Reshape</code> 操作，以产生一个 Head 维度。这样每个切片对应一个注意力头的输出矩阵</p></blockquote><p><img src="/posts/3545351090/Split-3.png" alt="img"></p><p>对于批量中一个序列的Q矩阵 $[\mathbf{Q}]_i\in \mathbb{R}^{T\times E}\iff \mathbb{R}^{T\times (N_heads\times Query_size )}$ ，可以将其拆分为 $(T, N_heads, Query_size )$ ，再交换 Head 维和 T 维，得到 $[\mathbf{Q}]_i\in \mathbb{R}^{N_heads\times T\times Query_size}\Rightarrow \mathbf{Q}\in \mathbb{R}^{B\times N_heads\times T\times Query_size}$</p><p><img src="/posts/3545351090/aa.png" alt="aa"></p><p>即上图所示的操作，对应 $\mathbf{Q},\mathbf{K},\mathbf{V}$ 矩阵，在逻辑对 $embed$ 轴拆分为 $N_heads$ 个小矩阵，经过变形和轴交换操作后，实际上形成的 $[\mathbf{H}]_0$ 与 $[\mathbf{H}]_1$ 就是沿 <code>embed</code> 轴 $N_heads$ 等分</p><ul><li>$[\mathbf{H}]_{0,1,:}=[\mathbf{Q}]_{1,0:3}=\{6,7,8\}$ ：表示第 $2$ 个 <code>token</code> 在第 $1$ 个注意力头中的Q阵表示</li><li>$[\mathbf{H}]_{1,1,:}=[\mathbf{Q}]_{1,3:6}=\{9,10,11\}$：表示第 $2$ 个 <code>token</code> 在第 $2$ 个注意力头中的Q阵表示</li><li>$[\mathbf{H}]_{0,:,:}=[\mathbf{Q}]_{:,0,:}=\begin{bmatrix}0&amp;1&amp;2\\6&amp;7&amp;8\\12&amp;13&amp;14\\18&amp;19&amp;20\end{bmatrix}$ ：表示一个序列的数据在第 $1$ 个注意力头的Q阵表示</li></ul><h6 id="数据维度"><a href="#数据维度" class="headerlink" title="数据维度"></a>数据维度</h6><p>$\mathbf{Q},\mathbf{K},\mathbf{V}\in \mathbb{R}^{B\times T\times E}\iff\mathbb{R}^{B\times T\times (N\times Q_size)}$ ，对QKV矩阵逻辑拆分为多个注意力头 $\mathbf{H}_{0:N}\in \mathbb{R}^{B\times T\times Q_{size}}$</p><h5 id="2-为每个注意力头计算注意力分数"><a href="#2-为每个注意力头计算注意力分数" class="headerlink" title="2. 为每个注意力头计算注意力分数"></a>2. 为每个注意力头计算注意力分数</h5><p>通过对QKV矩阵的拆分，每个注意力头都有了自己的QKV矩阵，通过这些矩阵，计算注意力分数</p><p>在一个注意力头中对一个 <code>token</code> 的注意力分数计算步骤为：</p><ol><li><p>对该 <code>token</code> 的QK矩阵进行内积</p><p><img src="/posts/3545351090/Score-1.png" alt="img"></p></li><li><p>用掩码掩盖填充值，让其不参与注意力分数的计算</p><p><img src="/posts/3545351090/Score-2.png" alt="img"></p></li><li><p>对结果按 $Q_size$ 大小进行缩放，再应用 <em>softmax</em></p><p><img src="/posts/3545351090/Score-3.png" alt="img"></p><p>$\sqrt{Query_size}$ ：使训练过程中梯度更稳定To help stabilize gradients during training</p><ul><li>让注意力权重更加平滑，避免因某个超大值对其他词元的权重造成影响</li><li>$Query_size$ 是词向量/隐藏层的维度，可以使 $\mathbf{Q}\cdot \mathbf{K}$ 符合 $\mathcal{N}(0,1)$ 分布，类似于归一化</li></ul></li><li><p><em>softmax</em> 结果与 $V$ 阵相乘</p><p><img src="/posts/3545351090/Score-4.png" alt="img"></p></li></ol><p>在一个注意头中，对一个 <code>token</code> 注意力分数的完整计算计算过程为：</p><p><img src="/posts/3545351090/image-20240415163207665.png" alt="image-20240415163207665"></p><p>QKV的值是对序列中每个 <code>token</code> 的编码表示</p><p>注意力计算将每个 <code>token</code> 与序列中其他 <code>token</code> 关联起来，这样注意力分数就为序列的每个 <code>token</code> 编码了一个分数</p><p>每个注意力头对一个批量序列的注意力分数计算的输出形状为 $(Batch_size,N_heads,T,Query_size)$</p><h5 id="3-融合多头注意力分数"><a href="#3-融合多头注意力分数" class="headerlink" title="3. 融合多头注意力分数"></a>3. 融合多头注意力分数</h5><p>对于一个批量的序列数据，每个注意力头都有单独的注意力分数，需要将其合并为一个分数。实质上与分割操作相反，通过 <code>reshape</code> 消除 <code>N</code> 维度来完成</p><p><strong>1. 交换Head维与Seq维</strong></p><p>注意力分数矩阵的形状从 $(Batch_size,N_heads,T,Query_size)$ 变为 $(Batch_size,T,N_heads,Query_size)$</p><p><strong>2. 对注意力得分矩阵reshape，将 Head维合并入embed维</strong></p><p>将每个注意力头的得分拼接为一个注意力得分</p><p><img src="/posts/3545351090/image-20240416134258930.png" alt="image-20240416134258930"></p><h5 id="多头注意力整体计算流程"><a href="#多头注意力整体计算流程" class="headerlink" title="多头注意力整体计算流程"></a>多头注意力整体计算流程</h5><p><img src="/posts/3545351090/Multi-1.png" alt="img"></p><h6 id="注意力掩码-Attention-Masks"><a href="#注意力掩码-Attention-Masks" class="headerlink" title="注意力掩码(Attention Masks)"></a>注意力掩码(Attention Masks)</h6><p>在计算注意力分数时，Attention模块应用了一个掩码操作</p><ul><li><p>在 <strong>Encoder 块</strong> 中的 <em>Self-Attention</em> 层和 <strong>Decoder 块</strong> 中的 <em>Encoder-Decoder Attention</em> 层中：在输入序列 <code>&lt;pad&gt;</code> 对应的位置，将输出的注意力分数归零，确保 <code>&lt;pad&gt;</code> 不会对自注意力产生影响</p><p><img src="/posts/3545351090/image-20240415163824164.png" alt="image-20240415163824164"></p></li><li><p>在 <strong>Decoder 块</strong> 中的 <em>Self-Attention</em> 层：防止解码器在当前时间步预测时，“偷看”目标序列后续时间步</p><p>解码器处理源输入序列 <code>src_seq</code> 中的 <code>token</code> ，并利用它们来预测目标序列 <code>tgt_src</code> 中的 <code>token</code></p><p>训练期间，这个过程通过 <strong>强制教学</strong> 的方式进行，完整的真实目标序列被作为解码器的输入，因此，在预测第 $t$ 个时间步的 <code>token</code> 时，解码器可以使用目标序列中该 <code>token</code> 之前序列中的词元，但不能使用该 <code>token</code> 及后续的词，所以需要通过掩码操作来防止“作弊”</p><p><img src="/posts/3545351090/image-20240415165016144.png" alt="image-20240415165016144"></p><p>此时，掩码操作变为</p><p><img src="/posts/3545351090/image-20240415165040121.png" alt="image-20240415165040121"></p></li></ul><p>当计算注意力分数(Attention Score) 时，在 <em>softmax</em> 计算之前做掩码操作，被屏蔽的元素设为负无穷大 ，在 <em>softmax</em> 时会将这些值变为0</p><h6 id="多头分割可以捕获更丰富的语义特征"><a href="#多头分割可以捕获更丰富的语义特征" class="headerlink" title="多头分割可以捕获更丰富的语义特征"></a>多头分割可以捕获更丰富的语义特征</h6><p>一个 <em>embed 向量</em> 包含了一个 <code>token</code> 的语义与位置信息，在多头注意力机制下，输入序列(目标序列)的嵌入 <code>position_embed</code> 向量被拆分为多个头，意味着，<strong>嵌入向量的不同部分可以表征每个词元在不同方面的含义，而每个词元在某个序列中的具体含义与序列中其他词元有关</strong></p><p>如嵌入向量某一部分可以捕获一个名词的词性，另一部分捕获名词的单复数，在翻译中，谓语时态的使用与这些因素有关。</p><h6 id="编码器与解码器自注意力层捕获信息的差异"><a href="#编码器与解码器自注意力层捕获信息的差异" class="headerlink" title="编码器与解码器自注意力层捕获信息的差异"></a>编码器与解码器自注意力层捕获信息的差异</h6><ul><li><strong>Encoder 块</strong> 中的 <em>Self-Attention</em> 层：计算源输入序列中，每个 <code>token</code> 与其他 <code>token</code> 之间的相互作用</li><li><strong>Decoder 块</strong> 中的 <em>Self-Attention</em> 层：计算目标输入序列中，每个 <code>token</code> 与其他目标 <code>token</code> 之间的相互作用</li><li><em>Encoder-Decoder Attention</em> 层：计算源输入序列中每个 <code>token</code> 与目标输入序列中每个 <code>token</code> 之间的相互作用</li></ul><h3 id="10-3-2-前馈层压缩注意力得分"><a href="#10-3-2-前馈层压缩注意力得分" class="headerlink" title="10.3.2 前馈层压缩注意力得分"></a>10.3.2 前馈层压缩注意力得分</h3><p>简单地将多头注意力得分矩阵 $\mathbf{Z}_n$ 进行拼接，这样的特征有点多，所以Transformer又用了一个线性变换 $\mathbf{W}_o$ 将其进行压缩</p><p><img src="/posts/3545351090/image-20240417211051299.png" alt="image-20240417211051299"></p><p><img src="/posts/3545351090/image-20240417211126832.png" alt="image-20240417211126832"></p><h3 id="10-3-3-归一化层"><a href="#10-3-3-归一化层" class="headerlink" title="10.3.3 归一化层"></a>10.3.3 归一化层</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/tardis/bd/ans/3094052709?source_id=1001">https://www.zhihu.com/tardis/bd/ans/3094052709?source_id=1001</a></p></blockquote><p>归一化，能减少数值不稳定性，避免梯度消失或爆炸，使模型收敛更快</p><p><strong>BatchNorm将不同样本相同维度的特征处理为相同的分布</strong>，针对同一特征，以跨样本的方式开展归一化，因此不会破坏不同样本同一特征之间的关系（在“身高体重”的例子中，这就意味着“归一化前是高个儿的归一化后仍然是高个儿，归一化前胖的归一化后也不会变瘦”）。即归一化之后，样本之间仍有可比性</p><p><img src="/posts/3545351090/image-20240417225220085.png" alt="image-20240417225220085"></p><p>对于一个NLP模型，一个批次的输入包括 $B$ 个独立的句子，每个句子又由不定长度的词元构成，句子中的每个词元又被表达为一个定长的嵌入向量。批量中，最长的句子有 $T$ 个词元，则 $d_{model}=T$</p><p><img src="/posts/3545351090/image-20240417225612311.png" alt="image-20240417225612311"></p><p>批量正则化需要一个批量的数据，且不适用于RNN任务。</p><ul><li>无法计算，句子中某些位置是填充词元</li><li>没有意义，归一化的目的是使具有相同性质的特征转化为标准正态分布，但不同句子相同位置并不是具有相同性质的特征，即同一词元不一定位于不同句子的同一位置</li></ul><p>同一个词元在不同序列中有不同含义，在一个序列中的具体含义是与序列中其他词元比较出来的，所以我们需要保持在同一序列中词元之间的可比性</p><p>Transformer使用了 Layer Norm，是对每个样本不同特征的归一化。<strong>相当于找到序列的语义中心</strong>，把序列中所有词元都聚集在这个中心周围，而一个序列中词元之间的比较关系并不会被破坏。</p><blockquote><p>假设输入一个批量的数据 $\mathbf{X}\in \mathbb{R}^{3\times 6}$</p><p>BatchNorm会对6个特征维度计算出6个均值和方差，然后用均值和方差进行归一化</p><script type="math/tex;mode=display">\mu_j=\frac{1}{3}\sum\limits_{i=1}^3[\mathbf{X}]_{ij}\quad,j=1,\cdots,6\\
\sigma^2_j=\frac{1}{3}\sum\limits_{i=1}^3\left([\mathbf{X}]_{i,j}-\mu_j\right)^2\\
[\hat{\mathbf{X}}]_{ij}=\frac{[\mathbf{X}]_{ij}-\mu_j}{\sqrt{\sigma^2_j+\epsilon}}，i=1,\cdots,3</script><p>LayerNorm是分别对3个样本的6个特征求均值和方差得到3个均值和方差，然后进行归一化</p><script type="math/tex;mode=display">\mu_i=\frac{1}{6}\sum\limits_{j=1}^6[\mathbf{X}]_{ij}\quad,i=1,\cdots,3\\
\sigma^2_i=\frac{1}{6}\sum\limits_{j=1}^6\left([\mathbf{X}]_{ij}-\mu_i\right)^2\\
[\hat{\mathbf{X}}]_{ij}=\frac{[\mathbf{X}]_{ij}-\mu_i}{\sqrt{\sigma^2_j+\epsilon}}，j=1,\cdots,6</script></blockquote><h2 id="10-4-注意力原理"><a href="#10-4-注意力原理" class="headerlink" title="10.4 注意力原理"></a>10.4 注意力原理</h2><p>重点关注注意力层对序列中每个词元的操作，可以清楚地理解每个词元与其他词元的相互作用</p><h3 id="10-4-1-数据的转换过程"><a href="#10-4-1-数据的转换过程" class="headerlink" title="10.4.1 数据的转换过程"></a>10.4.1 数据的转换过程</h3><p>假设处理英语序列到西班牙语序列的翻译任务，源序列是 “The ball is blue” ，目标序列是 “La bola es azul” ，通过词表，将源序列与目标序列转换为数字索引序列</p><p><img src="/posts/3545351090/Data-1-171325543835312.png" alt="img"></p><ol><li><p>数字索引源序列 $\mathbf{X}$ 首先经过嵌入层和位置编码层，为序列中每个词元生成叠加了位置编码的嵌入向量，随后被传入 <strong>Encoder Stack</strong> ，在其中的注意力层进行转换</p></li><li><p>在注意力层，嵌入向量序列分别通过Query、Key、Value三个线性层，产生三个独立的QKV矩阵。</p></li><li>通过多头注意力机制，叠加当前注意力层的注意力分数</li></ol><p>忽略嵌入向量维度，转换过程如下图，这些转换过程都是可以学习的</p><p><img src="/posts/3545351090/Data-3.png" alt="img"></p><p>QKV矩阵在逻辑上被拆分到各注意力头，注意力头中的每一行表示该注意力头对该行 <code>token</code> 的关注部分</p><h3 id="10-4-2-注意力得分"><a href="#10-4-2-注意力得分" class="headerlink" title="10.4.2 注意力得分"></a>10.4.2 注意力得分</h3><p><img src="/posts/3545351090/image-20240416203411111.png" alt="image-20240416203411111"></p><p>在一个注意力头的 <em>Atten Score</em> 模块，对QKV矩阵的计算为</p><script type="math/tex;mode=display">\mathbf{Z}=softmax\left(\frac{\mathbf{Q}\cdot\mathbf{K}}{\sqrt{Query\_size}}\right)\mathbf{V}</script><ol><li><p>首先，$\mathbf{Q}\cdot \mathbf{K}$ 的内积产生一个中间矩阵，即 <strong>权重矩阵</strong> 。矩阵中每个元素都表示QK中所有 <code>token</code> 间的关系</p><p><img src="/posts/3545351090/Attn-1-171327116618220.png" alt="img"></p><ul><li><p>$[\mathbf{Q}]_i[\mathbf{K}]_j^{\mathsf{T}}$ 即Query中的第 $i$ 个 <code>token</code> 与Key中第 $j$ 个 <code>token</code> 的内积，可表示二者间的关系</p></li><li><p>权重矩阵第四行的每一列都对应 $[\mathbf{Q}]_4$ 向量与 $\mathbf{K}$ 中每个向量的内积</p><p>权重矩阵第二列对应 $\mathbf{Q}$ 中每个向量与 $[\mathbf{K}]_2$ 向量的内积</p><p><img src="/posts/3545351090/image-20240416205217708.png" alt="image-20240416205217708"></p></li></ul></li><li><p>然后，权重矩阵与 $\mathbf{V}$ 矩阵相乘，产生注意力分数向量 $\mathbf{Z}$ ——<em>Atten Score</em> 模块的输出</p><p><img src="/posts/3545351090/image-20240416205330516.png" alt="image-20240416205330516"></p></li></ol><p>一个词元的注意力得分 $[\mathbf{Z}]_i$ 可理解为注意力层对该词元编码的表示向量</p><p>编码值是对V矩阵中每个词元的值向量 $[\mathbf{V}]_j$ 加权求和得到，权重是权重矩阵中相应的注意力权重</p><p><img src="/posts/3545351090/Attn-7.png" alt="img"></p><p>某个词元值向量 $[\mathbf{V}]_j$ 的注意力权重由查询向量 $[\mathbf{Q}]_i$ 与该词元的键向量 $[\mathbf{K}]_j$ 内积得到，也就是 $[\mathbf{Q}]_i$ 与 $[\mathbf{K}]_j$ 的关系大小作为该词元的注意力权重</p><h4 id="QKV在注意力层中的作用"><a href="#QKV在注意力层中的作用" class="headerlink" title="QKV在注意力层中的作用"></a>QKV在注意力层中的作用</h4><p>$\mathbf{Q}$ 矩阵中每个查询行向量 $[\mathbf{Q}]_i$ 都是该注意力层待编码的 <code>token</code> ，为了编码当前的词元，需要去观察其他的词元（本序列与目标序列）。$\mathbf{K}$ 中的每个行向量表示匹配序列中候选词元被用于检索的信息，$\mathbf{V}$ 中相应的行向量是该候选词元真正的信息。</p><p>通过注意力得分层计算出的注意力分数矩阵，是 Query 矩阵表示的序列中每个 <code>token</code> 对 Key与Value 矩阵表示序列中每个 <code>token</code> 的注意力分数（即Q序列中token对K序列中token的关注度）</p><p><img src="/posts/3545351090/image-20240416220134327.png" alt="image-20240416220134327"></p><p>对于 “The ball is blue “这个句子，单词 “blue “这一行包含 “blue “与其他每个单词的注意力分数</p><h4 id="内积的作用"><a href="#内积的作用" class="headerlink" title="内积的作用"></a>内积的作用</h4><p>对于两个向量，内积可以表示二者的关系</p><ul><li>若内积为0，表示二者正交，表示二者无关</li><li>若内积大于0，表示二者正相关，且夹角越小，内积越大，表示二者越接近</li><li>若内积小于0，表示二者负相关，且夹角越大，内积越小，表示二者越远离</li></ul><h4 id="通过注意力机制学习词元间的相关性"><a href="#通过注意力机制学习词元间的相关性" class="headerlink" title="通过注意力机制学习词元间的相关性"></a>通过注意力机制学习词元间的相关性</h4><p>使用 embed 层对词元编码，若两个词元语义相近，则编码后的嵌入向量应该也是相似的，至少二者方向一致，即成锐角。对两个词元的嵌入向量求内积，会得到正值，且相关性越大，会得到更大的内积值。相应的注意力权重就越高，进而查询向量对该向量的注意力分数就越高。</p><p>而两个不相关的词元，注意力分数就会越低。</p><p>要做到这一点，需要相关词元向量一致，无关词向量有分歧。而词向量是根据嵌入层和线性层的权重生成。</p><p>因此，引入嵌入层与Query、Key、Value三个线性层，为注意力模块提供更多的参数，使得模型可以通过学习嵌入层与线性层的参数矩阵权重来调整嵌入词向量</p><ul><li>在训练过程中，每个epoch，模型会根据损失函数计算真实输出与预测输出之间的差异，通过梯度反传更新QKV线性层与嵌入层的权重，进而减小每一层的差异。最后传导至嵌入层，通过调整嵌入层权重调整输出的嵌入向量间的相关性，更好地将输入词元向量映射到稠密的向量空间</li></ul><p>所以，在一个注意力层，注意力模块实现对序列的编码，层内前馈接层学习词元间的非线性关系，残差连接学习词元间的线性关系</p><h2 id="超参数-1"><a href="#超参数-1" class="headerlink" title="超参数"></a>超参数</h2><h3 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h3><p>同一个任务产生的序列数据集中的序列数据通常具有一些共性特征，这些特征反映了该任务的特点和需求。以下是一些可能的共性特征：</p><ol><li><p><strong>任务特定的模式和结构</strong>：序列数据集可能会遵循特定任务的语法结构或数据格式。例如，在文本分类任务中，不同的文本可能包含不同的标签；时间序列数据可以是按时间顺序排列的数值序列等。</p><p>数据分布：序列数据集中的序列可能会展现出特定的分布特征，例如，某些类别的序列数据可能比其他类别更常见，或者某些模式在数据集中更普遍。</p></li><li><p><strong>语义相关性</strong>：同一个任务产生的序列数据可能在语义上有相似之处，即使它们的表达方式和细节不同，它们可能仍然表达相似的概念、观点或信息。</p></li><li><p><strong>上下文依赖性</strong>：在处理序列数据时，上下文信息对于理解序列的含义至关重要。例如，在对话系统中，对话的历史信息对于理解当前轮次的回复至关重要。</p></li><li><p><strong>词元共现性</strong>：同一个任务产生的序列数据中可能包含相似的词汇或短语，这些词汇或短语在语义上可能相关，并在不同序列数据中共同出现。</p></li><li><p><strong>噪声和错误</strong>：在实际的数据集中，可能会包含一些噪声或错误，这些也需要模型在训练过程中学习如何处理。</p></li></ol><h3 id="embedding层"><a href="#embedding层" class="headerlink" title="embedding层"></a>embedding层</h3><p>Transformer将某个词元的特征向量通过embedding层映射为连续向量空间中的embedding向量，这个映射通过可学习的参数实现。</p><p>在训练过程中，模型参数的更新通过对网络输出的误差进行反向传播完成，每个训练步骤中，模型会根据当前的参数生成预测，然后计算预测值与真实值的损失，这个误差会被逐层反向传播回去（包括嵌入层）。通过计算损失函数对模型参数的梯度，根据优化算法（如随机梯度下降、Adam 等） 更新模型参数。</p><p>因此，嵌入层的参数会随着训练过程适当地调整，以使模型更好的理解和处理数据。这也意味着模型在训练过程中会学习到更好的词表示，使得输入的嵌入向量更能有效地表达输入序列的语义和特征。</p><p>在推理过程中，不会再变</p><h4 id="embedding向量长度"><a href="#embedding向量长度" class="headerlink" title="embedding向量长度"></a>embedding向量长度</h4><p>embedding向量的长度决定了模型能够捕捉到的信息量及模型的泛化能力</p><p>学习 Embedding：</p><ul><li><strong>预训练方法</strong>：常见的预训练Embedding模型有Word2Vec（包括CBOW和Skip-gram）、GloVe和FastText等。这些模型通过无监督学习（基于上下文窗口的预测任务）从大量文本数据中学习词向量。</li><li><strong>端到端学习</strong>：在深度学习模型（如神经网络、Transformer等）中，Embedding层作为模型的一部分，其参数（即嵌入矩阵）与其他模型参数一起在有监督学习任务（如文本分类、机器翻译等）中进行端到端训练。</li><li><strong>微调（Fine-tuning）</strong>：对于预训练的通用Embedding，可以在特定任务上进行微调，使其更好地适应特定领域的语义特性。</li></ul><p>embedding向量长度的选择准则：</p><ul><li><p>训练集大小和复杂度</p><p>较大的训练集往往需要更高的维度来捕捉数据中的独特性和复杂性</p><p>如果训练集中包含了丰富的多样性，那么较长的向量长度也是必要的来表征这种多样性</p></li><li><p>任务性质</p><p>不同的任务，如分类、回归或生成任务，对embedding向量长度的需求也不同</p></li><li><p>模型架构</p><p>深层网络通常能够处理更复杂的表示，因此可能需要更长的向量长度来满足网络的学习能力。</p></li><li><p>预期性能</p><p>在追求高性能的情况下，可能会选择更长的向量长度来提高模型的表达能力</p></li><li><p>理论分析</p><p>向量维度与训练集中词表大小有关。</p><p>根据 <a target="_blank" rel="noopener" href="https://kexue.fm/archives/7695/comment-page-1">1</a> 中的理论，若词表大小为 $V$ ，则embedding向量的维度 $E&gt; 8.33\log V$ ，意味着对于包含10000个词的语料库，理论上需要的向量维度至少为96；如果要容纳500万个词，那么大概就是128</p></li><li><p>实践经验</p><p>常用的embedding向量维度有64、100、128、256、300等，实际上不同的维度之间效果差别其实也没多少</p></li><li><p>特征匹配</p><p>根据特定特征的规模来调整embedding向量长度。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/431398377">2</a> 不同的特征可能需要不同的向量长度来最佳地表示其数据</p></li><li><p>模型训练过程中的变化</p><p>在模型训练过程中，embedding向量的长度可能会变。初始阶段需要较短的向量长度避免过拟合，随着训练进行，可能会增加向量长度改善模型性能</p></li></ul><h3 id="注意力机制理解"><a href="#注意力机制理解" class="headerlink" title="注意力机制理解"></a>注意力机制理解</h3><h4 id="现象：词元的语义关联性"><a href="#现象：词元的语义关联性" class="headerlink" title="现象：词元的语义关联性"></a>现象：词元的语义关联性</h4><p>当一句话中有 “eat” 这个单词时，这个句子很可能与食物、餐饮或饮食习惯有关，因为 “eat” 对这句话的语义贡献很大，再多看几个关键单词，就能确定这句话整体要表达的意思。即 <strong>一句话的语义由句子中几个关键单词的依赖关系决定，句中其他单词会与句子的语义相近</strong>。不同句子虽然表达不同语义，但 <strong>表达同一语义的单词具有语义关联性和共现性</strong></p><ul><li><p>统计模型中：语义关联性通过单词共同出现的条件概率 $p(w_T\vert w_1,\cdots,w_{T-1})$ 来捕捉</p></li><li><p>RNNs中：每个 <code>token</code> 的语义会被记录到一个上下文隐变量中，<code>token</code> 的语义会随着隐变量逐渐向后传递，在传递过程中语义信息会不断累积并压缩在隐变量中，导致前面 <code>token</code> 的影响会逐渐减弱。但实际情况是，关键 <code>token</code> 对序列中其他 <code>token</code> 的语义起到决定性作用，虽然LSTM和GRU能改善这种情况，但未彻底解决</p><p><img src="/posts/3545351090/image-20240422002511792.png" alt="image-20240422002511792"></p></li><li><p>Transformer模型中：这种语义关联性是通过模型在学习过程中自动捕捉的，模型学习到单词之间的依赖关系，并将这些知识编码到模型的参数中。因此，当模型生成或处理文本时，它会倾向于生成或处理与当前上下文语义相关的单词。</p><ul><li><p>结合位置编码，使得Transformer对各输入词元的预测输出能够并行计算</p><p><img src="/posts/3545351090/image-20240426190244181.png" alt="image-20240426190244181"></p></li></ul></li></ul><h4 id="Transformer中的注意力机制理解"><a href="#Transformer中的注意力机制理解" class="headerlink" title="Transformer中的注意力机制理解"></a>Transformer中的注意力机制理解</h4><blockquote><p>人类在处理信息时，会关注于当前任务最相关的信息而忽略不相关信息</p><p>注意力机制模拟这种行为，使模型在处理序列数据时，更加关注对当前任务更为重要的部分，同时忽略不相关信息。</p></blockquote><p>注意力机制是一种语义表征方式，通过词向量之间的点积来表达单词之间的依赖关系</p><p>在Transformer中，用一个注意力头表示一个语义空间，语义空间的数量是可以通过超参数 $N_heads$ 设置的。</p><ul><li>为序列中每个词元计算 $\mathbf{Q},\mathbf{K},\mathbf{V}$ 三个矩阵，相当于获取每个词元在各个语义空间下的各个语义变体</li></ul><script type="math/tex;mode=display">\begin{bmatrix}
[\mathbf{V}]_{1,j}\\
[\mathbf{V}]_{2,j}\\
\vdots\\
[\mathbf{V}]_{T,j}
\end{bmatrix}</script><p>在第 $j$ 个注意力头中， $[\mathbf{V}]_{:,j:j+Q_size}=\begin{bmatrix}<br>[\mathbf{V}]_{1,j}\\<br>[\mathbf{V}]_{2,j}\\<br>\vdots\\<br>[\mathbf{V}]_{T,j}<br>\end{bmatrix}$ 表示序列中每个词元在第 $j$ 个语义空间中语义变体的信息，$[\mathbf{Q}]_{:,j:j+Q_size}=\begin{bmatrix}<br>[\mathbf{Q}]_{1,j}\\<br>[\mathbf{Q}]_{2,j}\\<br>\vdots\\<br>[\mathbf{Q}]_{T,j}<br>\end{bmatrix}$ 与 $[\mathbf{K}]_{:,j:j+Q_size}=\begin{bmatrix}<br>[\mathbf{K}]_{1,j}\\<br>[\mathbf{K}]_{2,j}\\<br>\vdots\\<br>[\mathbf{K}]_{T,j}<br>\end{bmatrix}$ 是该语义空间下查询词与被查询词的匹配信息</p><p>$[\mathbf{Q}]_{m,j}\triangleq\mathbf{q}_j$ 表示在查询词元向量的第 $j$ 个语义空间（ $[\mathbf{Q}]_m$ 的一个语义变体），$[\mathbf{K}]_{n,j}\triangleq\mathbf{k}_j$ 表示第 $n$ 个词元向量在第 $j$ 个语义空间下的被查询向量，二者用于匹配</p><ul><li>$\left(\mathbf{q}_j,\mathbf{k}_j\right)$ 代表了在第 $j$ 个语义空间下查询词 $\mathbf{q}_j$ 与 $\mathbf{k}_j$ 间的相关性。$\left(\mathbf{q}_j,\mathbf{k}_j\right)\times \mathbf{v}_j$ 代表了 $\mathbf{v}_j$ 这个语义在查询词 $\mathbf{q}_j$ 这个语义空间中的加权信息量，也就是 $\mathbf{v}_j$ 这个语义对 $\mathbf{q}_j$ 这个语义的重要性</li><li>将单注意力头拼接为多头注意力，就能得到第 $n$ 个词元向量 $[\mathbf{V}]_n$ 的多个语义变体在 $[\mathbf{Q}]_m$ 的多个语义空间下的加权语义信息</li></ul><p><img src="/posts/3545351090/image-20240422005628881.png" alt="image-20240422005628881"></p><p>因此，在一个多头注意力中，$(\mathbf{Q}\cdot \mathbf{K})*\mathbf{V}$ 能得到V序列中每个词元的多个语义变体在Q序列中每个词元的多个语义空间下的加权语义信息</p><p><img src="/posts/3545351090/image-20240422005706897.png" alt="image-20240422005706897"></p><p>进一步将多头注意力堆叠，能表达更复杂语境和语义变化</p><p><img src="/posts/3545351090/image-20240422005732358.png" alt="image-20240422005732358"></p><h4 id="Transformer的参数学习"><a href="#Transformer的参数学习" class="headerlink" title="Transformer的参数学习"></a>Transformer的参数学习</h4><p>在Transformer架构中，序列数据中的词元依赖性知识被编码为一组模型参数，这些参数通过训练过程得到优化。具体来说，编码器和解码器多层结构中的权重集合，这些权重通过自注意力机制和前馈神经网络捕获输入序列中的长距离依赖关系。因此，模型的参数化决定了其在处理序列数据时的能力和性能。</p><p>在传统序列模型RNNs中，每个时间步的权重都是相同的；注意力机制使得模型可以为每个时间步动态分配权重。</p><ul><li>上下文敏感性：在处理每个 <code>token</code> 时，考虑该 <code>token</code> 与当前序列的上下文关系，调整对序列中不同 <code>token</code> 的权重，进而捕获到序列中的局部和全局依赖关系</li><li>动态分配权重：注意力机制通过学习到的权重集合来编码序列中词元之间的依赖性，这些权重在处理每个序列位置时与当前的上下文信息相结合，以动态地确定每个输入词元对输出表示的贡献<ul><li>在计算注意力得分和加权求和时为每个词元动态分配权重，使与当前任务高度相关的 <code>token</code> 在计算最终的输出表示时更加突出</li></ul></li></ul><p>在Transformer模型的训练过程中，通过对模型参数的调整，模型会学习将语义上相似的词元（tokens）在嵌入空间中映射得更加接近，而将语义上不相似的词元映射得更加远离。这种学习过程是通过优化模型的损失函数来实现的，损失函数反映了模型输出与真实数据之间的差异。</p><p>模型的学习过程：</p><ol><li><strong>数据准备</strong>：首先，需要准备一个与任务相关的序列数据集。这个数据集应该包含大量的示例，每个示例都是一个输入序列和一个相应的输出序列（对于生成任务）或标签（对于分类任务）。</li><li><strong>模型初始化</strong>：Transformer模型由多个编码器（Encoder）和解码器（Decoder）层组成。每个层都包含自注意力机制和前馈网络。在训练开始之前，模型的参数是随机初始化的。</li><li><strong>前向传播</strong>：在训练过程中，输入序列通过编码器层进行处理，生成一系列上下文感知的表示。如果任务是序列生成（如机器翻译），解码器会接收编码器的输出和已经生成的部分输出序列，继续生成后续的token。</li><li><strong>注意力权重计算</strong>：在每个编码器和解码器层中，自注意力机制会计算序列中每个token与其他所有token的关联性，并为每个token动态分配权重。这些权重反映了序列中不同元素之间的相互关系。</li><li><strong>损失函数计算</strong>：模型的输出与真实输出之间的差异通过损失函数来衡量。对于生成任务，通常使用交叉熵损失函数来比较模型生成的序列和真实序列。对于分类任务，可能使用分类交叉熵或其他适合的损失函数。</li><li><strong>反向传播和参数更新</strong>：通过反向传播算法，计算损失函数关于模型参数的梯度。然后，使用优化算法（如Adam）根据这些梯度更新模型的参数。</li><li><strong>迭代训练</strong>：上述过程会重复多次，每次迭代都会使模型参数朝着减少损失的方向调整，逐渐学习到数据集中的动力学知识。随着训练的进行，模型在处理序列数据时的性能会逐渐提高。</li><li><strong>模型评估</strong>：在训练过程中，定期在验证集上评估模型的性能，以确保模型不仅在学习训练数据中的模式，而且能够泛化到未见过的数据。</li></ol></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/3545351090/" title="10. Transformer 原理">https://amostian.github.io/posts/3545351090/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/919959330/" rel="prev" title="3.Ceph数据处理"><i class="fa fa-chevron-left"></i> 3.Ceph数据处理</a></div><div class="post-nav-item"><a href="/posts/3938730880/" rel="next" title="Linux常用">Linux常用 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#10-1-%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%85%B6%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="nav-text">10.1 架构及其训练、推理过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-1-Transformer%E6%9E%B6%E6%9E%84"><span class="nav-text">10.1.1 Transformer架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%A0%86%E6%A0%88"><span class="nav-text">编码器&#x2F;解码器堆栈</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9D%97%E9%97%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="nav-text">块间的数据流</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="nav-text">残差连接与归一化层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-2-Self-Attention"><span class="nav-text">10.1.2 Self-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-text">为什么需要注意力机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="nav-text">注意力分数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-3-Transformer%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-text">10.1.3 Transformer训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%88%B6%E6%95%99%E5%AD%A6"><span class="nav-text">强制教学</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-4-Transformer%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="nav-text">10.1.4 Transformer推理过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-5-Transformer%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%98%E4%BA%8ERNN"><span class="nav-text">10.1.5 Transformer为什么优于RNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-2-%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="nav-text">10.2 数据流</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-1-%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%B1%82%E4%B8%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88%E8%BE%93%E5%85%A5%E5%B1%82%EF%BC%89"><span class="nav-text">10.2.1 词嵌入层与位置编码（输入层）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82-Embedding"><span class="nav-text">嵌入层(Embedding)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%B1%82-Position-Encoding"><span class="nav-text">位置编码层(Position Encoding)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE"><span class="nav-text">批量数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA"><span class="nav-text">输入层的输出</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-2-%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-text">10.2.2 编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-3-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">10.2.3 解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-4-%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-text">10.2.4 输出层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-5-%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">10.2.5 训练与损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-3-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E5%9D%97%E5%86%85%E8%AE%A1%E7%AE%97"><span class="nav-text">10.3 编码器&#x2F;解码器的块内计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-1-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">10.3.1 注意力层的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E7%9A%84%E8%BE%93%E5%85%A5"><span class="nav-text">注意力层的输入</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E5%B1%82"><span class="nav-text">编码器层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">解码器</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E7%9A%84%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="nav-text">注意力层的线性层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B-Multi-head-Attention"><span class="nav-text">多头注意力(Multi-head Attention)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E9%80%9A%E8%BF%87%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4%E6%8B%86%E5%88%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E6%95%B0%E6%8D%AE"><span class="nav-text">1. 通过注意力头拆分注意力层数据</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%B4%E5%BA%A6"><span class="nav-text">数据维度</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E4%B8%BA%E6%AF%8F%E4%B8%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%B4%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="nav-text">2. 为每个注意力头计算注意力分数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E8%9E%8D%E5%90%88%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="nav-text">3. 融合多头注意力分数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%95%B4%E4%BD%93%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B"><span class="nav-text">多头注意力整体计算流程</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A9%E7%A0%81-Attention-Masks"><span class="nav-text">注意力掩码(Attention Masks)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E5%88%86%E5%89%B2%E5%8F%AF%E4%BB%A5%E6%8D%95%E8%8E%B7%E6%9B%B4%E4%B8%B0%E5%AF%8C%E7%9A%84%E8%AF%AD%E4%B9%89%E7%89%B9%E5%BE%81"><span class="nav-text">多头分割可以捕获更丰富的语义特征</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E6%8D%95%E8%8E%B7%E4%BF%A1%E6%81%AF%E7%9A%84%E5%B7%AE%E5%BC%82"><span class="nav-text">编码器与解码器自注意力层捕获信息的差异</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-2-%E5%89%8D%E9%A6%88%E5%B1%82%E5%8E%8B%E7%BC%A9%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%BE%97%E5%88%86"><span class="nav-text">10.3.2 前馈层压缩注意力得分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-3-%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="nav-text">10.3.3 归一化层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-4-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%8E%9F%E7%90%86"><span class="nav-text">10.4 注意力原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-1-%E6%95%B0%E6%8D%AE%E7%9A%84%E8%BD%AC%E6%8D%A2%E8%BF%87%E7%A8%8B"><span class="nav-text">10.4.1 数据的转换过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%BE%97%E5%88%86"><span class="nav-text">10.4.2 注意力得分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#QKV%E5%9C%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-text">QKV在注意力层中的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E7%A7%AF%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-text">内积的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0%E8%AF%8D%E5%85%83%E9%97%B4%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="nav-text">通过注意力机制学习词元间的相关性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0-1"><span class="nav-text">超参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="nav-text">序列数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding%E5%B1%82"><span class="nav-text">embedding层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#embedding%E5%90%91%E9%87%8F%E9%95%BF%E5%BA%A6"><span class="nav-text">embedding向量长度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%90%86%E8%A7%A3"><span class="nav-text">注意力机制理解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E8%B1%A1%EF%BC%9A%E8%AF%8D%E5%85%83%E7%9A%84%E8%AF%AD%E4%B9%89%E5%85%B3%E8%81%94%E6%80%A7"><span class="nav-text">现象：词元的语义关联性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformer%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%90%86%E8%A7%A3"><span class="nav-text">Transformer中的注意力机制理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformer%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-text">Transformer的参数学习</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">216</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">82</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1150.8k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">46:51</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>