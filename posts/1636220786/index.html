<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]  转：https:&#x2F;&#x2F;blog.csdn.net&#x2F;totobey&#x2F;article&#x2F;details&#x2F;124994579  模型：多元线性回归 特征选择：LASSO特征选择 数据集：  X：输入特征 分类特征处理方式：one-hot编码，未出现的取值作为参照类别 连续特征处理方式：原始值，未进行标准化和归一化    y：自行车租赁量"><meta property="og:type" content="article"><meta property="og:title" content="LASSO"><meta property="og:url" content="https://amostian.github.io/posts/1636220786/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]  转：https:&#x2F;&#x2F;blog.csdn.net&#x2F;totobey&#x2F;article&#x2F;details&#x2F;124994579  模型：多元线性回归 特征选择：LASSO特征选择 数据集：  X：输入特征 分类特征处理方式：one-hot编码，未出现的取值作为参照类别 连续特征处理方式：原始值，未进行标准化和归一化    y：自行车租赁量"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/v2-f597f7aa9d3eff9fa54dcb10a434e40b_720w.webp"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/1cb78c3827b1407491063e9fa184ba92.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/5d90cb8f05c945d0b972b1edaf81c33a.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/e0fa58a2e9bd408c835e37dd767c56fe.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/08b54268c9ad4457850154f1efa0ab39.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/ac0045e81d3643c9a668f645b18a2bdc.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/1f71f5ce26154b559f5d53560d537bff.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/471be780371946938f5580cea7dab85f.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/31e6b35c8f9644eeb0cc10e5d98d1f31.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/3a97ed58df26454da61f3c12463acdc3.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/89622e6a579b4c9fb24268ae96c8657e.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/6f635a8ea3d046f599751f2764c425ec.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/f74c450dcdda4f1a886f61dbb7a1d9ae.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/fddbc98017dd44499fa7014bc93d95c7.png"><meta property="og:image" content="https://amostian.github.io/posts/1636220786/v2-bee228f16814c4a170a85646a272e642_720w.webp"><meta property="article:published_time" content="2024-07-19T02:21:57.000Z"><meta property="article:modified_time" content="2024-10-04T11:29:41.213Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="特征工程"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/1636220786/v2-f597f7aa9d3eff9fa54dcb10a434e40b_720w.webp"><link rel="canonical" href="https://amostian.github.io/posts/1636220786/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>LASSO | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">66</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">83</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">224</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/1636220786/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">LASSO</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-07-19 10:21:57" itemprop="dateCreated datePublished" datetime="2024-07-19T10:21:57+08:00">2024-07-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-10-04 19:29:41" itemprop="dateModified" datetime="2024-10-04T19:29:41+08:00">2024-10-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">10.7k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>29 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><blockquote><p>转：<a target="_blank" rel="noopener" href="https://blog.csdn.net/totobey/article/details/124994579">https://blog.csdn.net/totobey/article/details/124994579</a></p></blockquote><p>模型：多元线性回归</p><p>特征选择：LASSO特征选择</p><p>数据集：</p><ul><li>X：输入特征<ul><li>分类特征处理方式：one-hot编码，未出现的取值作为参照类别</li><li>连续特征处理方式：原始值，未进行标准化和归一化</li></ul></li><li></li><li>y：自行车租赁量</li></ul><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>特征选择是机器学习中很大的一个话题</p><p><img src="/posts/1636220786/v2-f597f7aa9d3eff9fa54dcb10a434e40b_720w.webp" alt="img"></p><p>why？当特征数量过多，传统的最小二乘法（Ordinary Least Squares, OLS）回归模型往往会得到过拟合的模型</p><ul><li>特征之间存在多重共线性，会导致模型参数估计不稳定甚至无法计算</li><li>并非所有特征都对预测结果有显著影响，一些无关或冗余的特征可能会干扰模型的预测性能。</li></ul><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p><strong>Ridge</strong></p><ul><li>ridge回归，无法直接剔除不重要特征，将不重要特征的回归系数趋于0，使其对预测值影响较小</li><li>L2正则能有效防止模型过拟合，解决非满秩情况下求逆困难问题</li></ul><p><strong>Lasso</strong></p><ul><li>lasso回归，将不重要特征的回归系数缩减为0，从而达到剔除的目的</li><li>L1正则，能够系数矩阵，在特征数庞大的情况下进行特征选择</li></ul><p><strong>lasso可用于特征选择，Ridge不可</strong></p><h3 id="LASSO优化目标是"><a href="#LASSO优化目标是" class="headerlink" title="LASSO优化目标是"></a>LASSO优化目标是</h3><p><img src="/posts/1636220786/1cb78c3827b1407491063e9fa184ba92.png" alt="img"></p><p>上式不是连续可导的，因此常规的解法如梯度下降法、牛顿法、就没法用了。常用的方法：<strong>坐标轴下降法与最小角回归法</strong>（Least-angle regression (LARS)）。</p><ul><li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6018889.html">刘建平老师的文章《Lasso回归算法： 坐标轴下降法与最小角回归法小结 》</a></li></ul><h4 id="坐标下降法"><a href="#坐标下降法" class="headerlink" title="坐标下降法"></a>坐标下降法</h4><p>迭代优化算法，求解具有可分离结构的凸优化问题。常用于训练特征数量非常大的大规模线性模型</p><ol><li><p>初始化权重向量w</p></li><li><p>迭代更新权重系数</p><p>在每次迭代过程中，依次更新每个权重系数 $w_m$ ，保持其他权重不变，对于第 $k$ 次迭代和第 $m$ 个权重系数，更新规则为</p><script type="math/tex;mode=display">w_m^{(k)}=\arg\min\limits_{w_m}Cost\left(w_1^{(k)},w_2^{(k-1)},\cdots,w_{m-1}^{(k-1)},w_m,w_{m+1}^{(k-1)},\cdots,w_n^{(k-1)}\right)</script></li><li><p>判断收敛性与迭代终止</p><p>如果所有权重系数变化非常小，或达到了预设的最大迭代次数，算法停止迭代</p></li></ol><p><strong>坐标下降法的优势在于其简单性和易于并行化</strong>，尤其适合于解决高维问题，因为每次更新一个变量时，不需要计算所有变量的信息。</p><p>然而，由于其更新规则的贪心性质，可能不保证全局最优解，特别是当损失函数非凸时。尽管如此，在实际应用中，坐标下降法通常能给出满意的结果。</p><ul><li>每次迭代固定其他的权重系数，只朝着其中一个坐标轴的方向更新，最后到达最优解</li></ul><p><img src="/posts/1636220786/5d90cb8f05c945d0b972b1edaf81c33a.png" alt="在这里插入图片描述"></p><p><strong>实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lassoUseCd</span>(<span class="params">X, y, lambdas=<span class="number">0.1</span>, max_iter=<span class="number">1000</span>, tol=<span class="number">1e-4</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Lasso回归，使用坐标下降法（coordinate descent）</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        X - 训练数据集</span></span><br><span class="line"><span class="string">        y - 目标标签值</span></span><br><span class="line"><span class="string">        lambdas - 惩罚项系数</span></span><br><span class="line"><span class="string">        max_iter - 最大迭代次数</span></span><br><span class="line"><span class="string">        tol - 变化量容忍值</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w - 权重系数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化 w 为零向量</span></span><br><span class="line">    w = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        done = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 遍历所有自变量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(w)):</span><br><span class="line">            <span class="comment"># 记录上一轮系数</span></span><br><span class="line">            weight = w[i]</span><br><span class="line">            <span class="comment"># 求出当前条件下的最佳系数</span></span><br><span class="line">            w[i] = down(X, y, w, i, lambdas)</span><br><span class="line">            <span class="comment"># 当其中一个系数变化量未到达其容忍值，继续循环</span></span><br><span class="line">            <span class="keyword">if</span> (np.<span class="built_in">abs</span>(weight - w[i]) &gt; tol):</span><br><span class="line">                done = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 所有系数都变化不大时，结束循环</span></span><br><span class="line">        <span class="keyword">if</span> (done):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down</span>(<span class="params">X, y, w, index, lambdas=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    cost(w) = (x1 * w1 + x2 * w2 + ... - y)^2 + ... + λ(|w1| + |w2| + ...)</span></span><br><span class="line"><span class="string">    假设 w1 是变量，这时其他的值均为常数，带入上式后，其代价函数是关于 w1 的一元二次函数，可以写成下式：</span></span><br><span class="line"><span class="string">    cost(w1) = (a * w1 + b)^2 + ... + λ|w1| + c (a,b,c,λ 均为常数)</span></span><br><span class="line"><span class="string">    =&gt; 展开后</span></span><br><span class="line"><span class="string">    cost(w1) = aa * w1^2 + 2ab * w1 + λ|w1| + c (aa,ab,c,λ 均为常数)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 展开后的二次项的系数之和</span></span><br><span class="line">    aa = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 展开后的一次项的系数之和</span></span><br><span class="line">    ab = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># 括号内一次项的系数</span></span><br><span class="line">        a = X[i][index]</span><br><span class="line">        <span class="comment"># 括号内常数项的系数</span></span><br><span class="line">        b = X[i][:].dot(w) - a * w[index] - y[i]</span><br><span class="line">        <span class="comment"># 可以很容易的得到展开后的二次项的系数为括号内一次项的系数平方的和</span></span><br><span class="line">        aa = aa + a * a</span><br><span class="line">        <span class="comment"># 可以很容易的得到展开后的一次项的系数为括号内一次项的系数乘以括号内常数项的和</span></span><br><span class="line">        ab = ab + a * b</span><br><span class="line">    <span class="comment"># 由于是一元二次函数，当导数为零时，函数值最小值，只需要关注二次项系数、一次项系数和 λ</span></span><br><span class="line">    <span class="keyword">return</span> det(aa, ab, lambdas)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">det</span>(<span class="params">aa, ab, lambdas=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过代价函数的导数求 w，当 w = 0 时，不可导</span></span><br><span class="line"><span class="string">    det(w) = 2aa * w + 2ab + λ = 0 (w &gt; 0)</span></span><br><span class="line"><span class="string">    =&gt; w = - (2 * ab + λ) / (2 * aa)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    det(w) = 2aa * w + 2ab - λ = 0 (w &lt; 0)</span></span><br><span class="line"><span class="string">    =&gt; w = - (2 * ab - λ) / (2 * aa)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    det(w) = NaN (w = 0)</span></span><br><span class="line"><span class="string">    =&gt; w = 0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w = - (<span class="number">2</span> * ab + lambdas) / (<span class="number">2</span> * aa)</span><br><span class="line">    <span class="keyword">if</span> w &lt; <span class="number">0</span>:</span><br><span class="line">        w = - (<span class="number">2</span> * ab - lambdas) / (<span class="number">2</span> * aa)</span><br><span class="line">        <span class="keyword">if</span> w &gt; <span class="number">0</span>:</span><br><span class="line">            w = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure><h4 id="最小角回归"><a href="#最小角回归" class="headerlink" title="最小角回归"></a>最小角回归</h4><p>Least Angle Regression, LARS 通过逐步考虑与当前残差最相关的特征来进行变量选择和系数估计。</p><ol><li><strong>初始化权重系数</strong> $w$。</li><li><strong>计算初始残差向量</strong> $residual$ 其等于目标标签向量 $y$ 减去设计矩阵 $X$ 与权重系数 $w$ 的乘积。在初始化时，$w$ 为零向量，所以残差向量等于目标标签向量 $y$</li><li><strong>寻找最相关的特征</strong>。选择一个与当前残差向量 $residual$ 相关性最大的特征向量 $x_i$</li><li><strong>更新权重系数</strong>。沿选定特征向量 $x_i$ 的方向更新权重系数 $w$，直至出现另一个特征向量 $x_j$ ，使得新的残差向量与 $x_i$ 和 $x_j$ 的相关性相等。此时，残差向量位于 $x_i$ 和 $x_j$ 的角平分线上。</li><li><strong>迭代过程</strong>。重复上述过程，每次迭代中添加一个新的特征向量，并调整权重系数 $w$，以使残差向量位于新加入的特征和其他所有已选特征的等角线上。</li><li><strong>终止条件</strong>。当残差向量的大小足够小或所有特征都已考虑过后，算法停止。足够小的残差意味着进一步添加特征不会显著改善模型。</li></ol><p>最小角回归法的优势在于其能够提供模型选择的整个过程，从一个简单的模型开始，逐步增加特征直到满足某个停止准则。这种方法易于理解，并且计算高效，特别适合于处理高维数据集。</p><p><strong>实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lassoUseLars</span>(<span class="params">X, y, lambdas=<span class="number">0.1</span>, max_iter=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Lasso回归，使用最小角回归法（Least Angle Regression）</span></span><br><span class="line"><span class="string">    论文：https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        X - 训练数据集</span></span><br><span class="line"><span class="string">        y - 目标标签值</span></span><br><span class="line"><span class="string">        lambdas - 惩罚项系数</span></span><br><span class="line"><span class="string">        max_iter - 最大迭代次数</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        w - 权重系数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n, m = X.shape</span><br><span class="line">    <span class="comment"># 已被选择的特征下标</span></span><br><span class="line">    active_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="comment"># 当前预测向量</span></span><br><span class="line">    cur_pred = np.zeros((n,), dtype=np.float32)</span><br><span class="line">    <span class="comment"># 残差向量</span></span><br><span class="line">    residual = y - cur_pred</span><br><span class="line">    <span class="comment"># 特征向量与残差向量的点积，即相关性</span></span><br><span class="line">    cur_corr = X.T.dot(residual)</span><br><span class="line">    <span class="comment"># 选取相关性最大的下标</span></span><br><span class="line">    j = np.argmax(np.<span class="built_in">abs</span>(cur_corr), <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 将下标添加至已被选择的特征下标集合</span></span><br><span class="line">    active_set.add(j)</span><br><span class="line">    <span class="comment"># 初始化权重系数</span></span><br><span class="line">    w = np.zeros((m,), dtype=np.float32)</span><br><span class="line">    <span class="comment"># 记录上一次的权重系数</span></span><br><span class="line">    prev_w = np.zeros((m,), dtype=np.float32)</span><br><span class="line">    <span class="comment"># 记录特征更新方向</span></span><br><span class="line">    sign = np.zeros((m,), dtype=np.int32)</span><br><span class="line">    sign[j] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 平均相关性</span></span><br><span class="line">    lambda_hat = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 记录上一次平均相关性</span></span><br><span class="line">    prev_lambda_hat = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        <span class="comment"># 计算残差向量</span></span><br><span class="line">        residual = y - cur_pred</span><br><span class="line">        <span class="comment"># 特征向量与残差向量的点积</span></span><br><span class="line">        cur_corr = X.T.dot(residual)</span><br><span class="line">        <span class="comment"># 当前相关性最大值</span></span><br><span class="line">        largest_abs_correlation = np.<span class="built_in">abs</span>(cur_corr).<span class="built_in">max</span>()</span><br><span class="line">        <span class="comment"># 计算当前平均相关性</span></span><br><span class="line">        lambda_hat = largest_abs_correlation / n</span><br><span class="line">        <span class="comment"># 当平均相关性小于λ时，提前结束迭代</span></span><br><span class="line">        <span class="comment"># https://github.com/scikit-learn/scikit-learn/blob/2beed55847ee70d363bdbfe14ee4401438fba057/sklearn/linear_model/_least_angle.py#L542</span></span><br><span class="line">        <span class="keyword">if</span> lambda_hat &lt;= lambdas:</span><br><span class="line">            <span class="keyword">if</span> (it &gt; <span class="number">0</span> <span class="keyword">and</span> lambda_hat != lambdas):</span><br><span class="line">                ss = ((prev_lambda_hat - lambdas) / (prev_lambda_hat - lambda_hat))</span><br><span class="line">                <span class="comment"># 重新计算权重系数</span></span><br><span class="line">                w[:] = prev_w + ss * (w - prev_w)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 更新上一次平均相关性</span></span><br><span class="line">        prev_lambda_hat = lambda_hat</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 当全部特征都被选择，结束迭代</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(active_set) &gt; m:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选中的特征向量</span></span><br><span class="line">        X_a = X[:, <span class="built_in">list</span>(active_set)]</span><br><span class="line">        <span class="comment"># 论文中 X_a 的计算公式 - (2.4)</span></span><br><span class="line">        X_a *= sign[<span class="built_in">list</span>(active_set)]</span><br><span class="line">        <span class="comment"># 论文中 G_a 的计算公式 - (2.5)</span></span><br><span class="line">        G_a = X_a.T.dot(X_a)</span><br><span class="line">        G_a_inv = np.linalg.inv(G_a)</span><br><span class="line">        G_a_inv_red_cols = np.<span class="built_in">sum</span>(G_a_inv, <span class="number">1</span>)     </span><br><span class="line">        <span class="comment"># 论文中 A_a 的计算公式 - (2.5)</span></span><br><span class="line">        A_a = <span class="number">1</span> / np.sqrt(np.<span class="built_in">sum</span>(G_a_inv_red_cols))</span><br><span class="line">        <span class="comment"># 论文中 ω 的计算公式 - (2.6)</span></span><br><span class="line">        omega = A_a * G_a_inv_red_cols</span><br><span class="line">        <span class="comment"># 论文中角平分向量的计算公式 - (2.6)</span></span><br><span class="line">        equiangular = X_a.dot(omega)</span><br><span class="line">        <span class="comment"># 论文中 a 的计算公式 - (2.11)</span></span><br><span class="line">        cos_angle = X.T.dot(equiangular)</span><br><span class="line">        <span class="comment"># 论文中的 γ</span></span><br><span class="line">        gamma = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 下一个选择的特征下标</span></span><br><span class="line">        next_j = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 下一个特征的方向</span></span><br><span class="line">        next_sign = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">if</span> j <span class="keyword">in</span> active_set:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 论文中 γ 的计算方法 - (2.13)</span></span><br><span class="line">            v0 = (largest_abs_correlation - cur_corr[j]) / (A_a - cos_angle[j]).item()</span><br><span class="line">            v1 = (largest_abs_correlation + cur_corr[j]) / (A_a + cos_angle[j]).item()</span><br><span class="line">            <span class="keyword">if</span> v0 &gt; <span class="number">0</span> <span class="keyword">and</span> (gamma <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> v0 &lt; gamma):</span><br><span class="line">                gamma = v0</span><br><span class="line">                next_j = j</span><br><span class="line">                next_sign = <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> v1 &gt; <span class="number">0</span> <span class="keyword">and</span> (gamma <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> v1 &lt; gamma):</span><br><span class="line">                gamma = v1</span><br><span class="line">                next_j = j</span><br><span class="line">                next_sign = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> gamma <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 论文中 γ 的计算方法 - (2.21)</span></span><br><span class="line">            gamma = largest_abs_correlation / A_a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选中的特征向量</span></span><br><span class="line">        sa = X_a</span><br><span class="line">        <span class="comment"># 角平分向量</span></span><br><span class="line">        sb = equiangular * gamma</span><br><span class="line">        <span class="comment"># 解线性方程（sa * sx = sb）</span></span><br><span class="line">        sx = np.linalg.lstsq(sa, sb)</span><br><span class="line">        <span class="comment"># 记录上一次的权重系数</span></span><br><span class="line">        prev_w = w.copy()</span><br><span class="line">        d_hat = np.zeros((m,), dtype=np.int32)</span><br><span class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(active_set):</span><br><span class="line">            <span class="comment"># 更新当前的权重系数</span></span><br><span class="line">            w[j] += sx[<span class="number">0</span>][i] * sign[j]</span><br><span class="line">            <span class="comment"># 论文中 d_hat 的计算方法 - (3.3)</span></span><br><span class="line">            d_hat[j] = omega[i] * sign[j]</span><br><span class="line">        <span class="comment"># 论文中 γ_j 的计算方法 - (3.4)</span></span><br><span class="line">        gamma_hat = -w / d_hat</span><br><span class="line">        <span class="comment"># 论文中 γ_hat 的计算方法 - (3.5)</span></span><br><span class="line">        gamma_hat_min = <span class="built_in">float</span>(<span class="string">&quot;+inf&quot;</span>)</span><br><span class="line">        <span class="comment"># 论文中 γ_hat 的下标</span></span><br><span class="line">        gamma_hat_min_idx = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(gamma_hat):</span><br><span class="line">            <span class="keyword">if</span> j &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> gamma_hat_min &gt; j:</span><br><span class="line">                gamma_hat_min = j</span><br><span class="line">                gamma_hat_min_idx = i</span><br><span class="line">        <span class="keyword">if</span> gamma_hat_min &lt; gamma:</span><br><span class="line">            <span class="comment"># 更新当前预测向量 - (3.6)</span></span><br><span class="line">            cur_pred = cur_pred + gamma_hat_min * equiangular</span><br><span class="line">            <span class="comment"># 将下标移除至已被选择的特征下标集合</span></span><br><span class="line">            active_set.remove(gamma_hat_min_idx)</span><br><span class="line">            <span class="comment"># 更新特征更新方向集合</span></span><br><span class="line">            sign[gamma_hat_min_idx] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 更新当前预测向量</span></span><br><span class="line">            cur_pred = X.dot(w)</span><br><span class="line">            <span class="comment"># 将下标添加至已被选择的特征下标集合</span></span><br><span class="line">            active_set.add(next_j)</span><br><span class="line">            <span class="comment"># 更新特征更新方向集合</span></span><br><span class="line">            sign[next_j] = next_sign</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>scikit-learn提供了这两种优化算法的Lasso是实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.Lasso(alpha=<span class="number">1.0</span>, *, fit_intercept=<span class="literal">True</span>, normalize=<span class="string">&#x27;deprecated&#x27;</span>, precompute=<span class="literal">False</span>, copy_X=<span class="literal">True</span>,max_iter=<span class="number">1000</span>, tol=<span class="number">0.0001</span>, warm_start=<span class="literal">False</span>, positive=<span class="literal">False</span>, random_state=<span class="literal">None</span>, selection=<span class="string">&#x27;cyclic&#x27;</span>)</span><br><span class="line"></span><br><span class="line">sklearn.linear_model.lars_path(X, y, Xy=<span class="literal">None</span>, *, Gram=<span class="literal">None</span>, max_iter=<span class="number">500</span>, alpha_min=<span class="number">0</span>, method=<span class="string">&#x27;lar&#x27;</span>, copy_X=<span class="literal">True</span>, eps=<span class="number">2.220446049250313e-16</span>, copy_Gram=<span class="literal">True</span>, verbose=<span class="number">0</span>, return_path=<span class="literal">True</span>, return_n_iter=<span class="literal">False</span>, positive=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ol><li>数据预处理</li><li>验证数据是否满足某些假设<ol><li>判断共线性</li><li>判断正态性</li></ol></li><li>建模与解释<ol><li>将权重表组合为一个DataFrame</li><li>获取实例真实值、预测值、置信区间、画图</li><li>观察模型拟合效果</li><li>用文本解释特征权重</li><li>解释截距</li><li>解释特征重要性</li><li>进一步可视化解释权重</li><li>可视化效应图</li><li>效应图解释单个实例</li></ol></li></ol><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ul><li>是否使用全部特征</li><li>分类变量（季节、天气情况）进行<strong>one-hot编码</strong>，其中<strong>未出现的取值（春、晴）作为参照类别。</strong></li><li>原始数据集中的温度、湿度、风速进行了标准化和归一化，<strong>本次处理时将取值恢复了原始值。</strong></li><li>链接中的原始数据集修正过季节，但是为了和书上的数据相对应，我使用了旧版的数据集，因此后续对季节的解释可能不符合实际情况。</li></ul><p>特征集统一进行z-score标准化</p><h4 id="分类特征处理"><a href="#分类特征处理" class="headerlink" title="分类特征处理"></a>分类特征处理</h4><p><code>OneHotEncoder</code>：将输入中的 <em>分类特征</em> 编码为一个one-hot数值数组，也称为 <em>one-of-K</em> 或 <em>dummy</em> ，适用于无序的类别特征。</p><ul><li>如果数据只有一个特征，使用 <code>array.reshape(-1, 1)</code> 将数据变形</li><li><p>如果仅一个样本，使用 <code>array.reshape(1, -1)</code> 将数据变形</p><p>如果对输出进行编码则使用 <code>LabelBinarizer</code></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">OneHotEncoder(dtype=np.int8,handle_unknown=<span class="string">&#x27;ignore&#x27;</span>,sparse=<span class="literal">False</span>,categories=keep_cate) </span><br><span class="line">sparse_out: <span class="built_in">bool</span>, default=<span class="literal">True</span>，默认为稀疏矩阵形式</span><br><span class="line">handle_unknown：规定fit中未出现但是transform中出现的值进行<span class="string">&#x27;ignore&#x27;</span></span><br><span class="line">categories：指定需要保留的取值（指定后可以通过ohe.categories_查看）</span><br><span class="line">drop：控制如何drop掉多余的列</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f_ohe</span>(<span class="params">df,col,new_names=<span class="literal">None</span>,keep_cate=<span class="string">&#x27;auto&#x27;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    【功能】对一个特征进行one-hot编码</span></span><br><span class="line"><span class="string">    【参数】df：dataframe,数据集</span></span><br><span class="line"><span class="string">           col：str,编码的特征名</span></span><br><span class="line"><span class="string">           new_names: dic,如果需要对取值重命名（使特征名更能表达真实意思）,则新建一个字典，默认None则特征名为col取值</span></span><br><span class="line"><span class="string">           keep_cate: list,需要保留的取值，如果取值是数值型则需要先排序,例如[[1,3,4]];默认&#x27;auto&#x27;表示保留所有值</span></span><br><span class="line"><span class="string">    【返回】dataframe</span></span><br><span class="line"><span class="string">    【举例】t_season=f_ohe(df=bike,col=&#x27;season&#x27;,new_names=&#123;1:&#x27;冬&#x27;,3:&#x27;夏&#x27;,4:&#x27;秋&#x27;&#125;,keep_cate=[[1,3,4]])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    ohe=OneHotEncoder(dtype=np.int8,handle_unknown=<span class="string">&#x27;ignore&#x27;</span>,sparse=<span class="literal">False</span>,categories=keep_cate) </span><br><span class="line">    ohe.fit(df[col].values.reshape(-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    tmp=pd.DataFrame(ohe.transform(df[col].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)))</span><br><span class="line">    org_names=ohe.get_feature_names_out([col]).tolist()  <span class="comment">#col作为新生成字段的前缀</span></span><br><span class="line">    <span class="keyword">if</span> new_names <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">        tmp.columns=org_names</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        new_names_keys=<span class="built_in">list</span>(new_names.keys()) <span class="comment">#获取输入的keys</span></span><br><span class="line">        new_names_keys=[col+<span class="string">&#x27;_&#x27;</span>+<span class="built_in">str</span>(item) <span class="keyword">for</span> item <span class="keyword">in</span> new_names_keys] <span class="comment">#输入的keys加上col前缀</span></span><br><span class="line">        <span class="comment"># print(new_names_keys) </span></span><br><span class="line">        new_names=<span class="built_in">dict</span>(<span class="built_in">zip</span>(new_names_keys,<span class="built_in">list</span>(new_names.values()))) <span class="comment">#加上前缀的keys和输入的values重新打包为字典</span></span><br><span class="line">        <span class="comment"># print(new_names)        </span></span><br><span class="line">        a=<span class="built_in">list</span>(pd.Series(data=org_names).<span class="built_in">map</span>(new_names).values) <span class="comment">#list不能直接map,把原特征名map转为Series后映射为新特征名       </span></span><br><span class="line">        tmp.columns=[col+<span class="string">&#x27;_&#x27;</span>+<span class="built_in">str</span>(item) <span class="keyword">for</span> item <span class="keyword">in</span> a]  <span class="comment">#加上col前缀</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> tmp</span><br></pre></td></tr></table></figure><p>使用上述 <code>f_ohe</code> 逐一对需要one-hot编码的列进行编码，并与其余特征拼接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t_season=f_ohe(df=bike,col=<span class="string">&#x27;season&#x27;</span>,new_names=&#123;<span class="number">1</span>:<span class="string">&#x27;冬&#x27;</span>,<span class="number">3</span>:<span class="string">&#x27;夏&#x27;</span>,<span class="number">4</span>:<span class="string">&#x27;秋&#x27;</span>&#125;,keep_cate=[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">t_weathersit=f_ohe(df=bike,col=<span class="string">&#x27;weathersit&#x27;</span>,new_names=&#123;<span class="number">2</span>:<span class="string">&#x27;雾&#x27;</span>,<span class="number">3</span>:<span class="string">&#x27;雨雪&#x27;</span>&#125;,keep_cate=[[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">bike_ohe=pd.concat((bike[[<span class="string">&#x27;cnt&#x27;</span>,<span class="string">&#x27;holiday&#x27;</span>, <span class="string">&#x27;workingday&#x27;</span>, <span class="string">&#x27;temp&#x27;</span>, <span class="string">&#x27;hum&#x27;</span>, <span class="string">&#x27;windspeed&#x27;</span>, <span class="string">&#x27;days_since_2011&#x27;</span>]], t_season,t_weathersit),axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">bike_ohe.to_csv(path+<span class="string">&#x27;处理完数据集/bike_ohe.csv&#x27;</span>,index=<span class="literal">False</span>,encoding=<span class="string">&#x27;utf_8_sig&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="验证数据是否满足一些假设"><a href="#验证数据是否满足一些假设" class="headerlink" title="验证数据是否满足一些假设"></a>验证数据是否满足一些假设</h3><p>1.线性：可先不用管。根据基础线性模型建立后的R方判断预测效果。后续如有需要可对基础线性模型进行改造（如添加交互项、使用回归样条等）。</p><p>2.正态性：本节对目标变量的正态性进行了检验，检验通过。</p><p>3.同方差性：必须建模后再判断，此时无法判断。</p><p>4.独立性：除了特殊情况（比如对同一人进行重复测量），一般可认为实例之间相互独立。</p><p>5.固定特征：测量无误差，一般都可认为是固定特征。</p><p>6.不存在多重共线性：本节进行了检验，检验通过。虽然同时计算了相关系数和VIF，但实践时可以直接以VIF的结论为准。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入one-hot编码后的数据</span></span><br><span class="line">df=bike_ohe</span><br><span class="line"><span class="comment"># 指定标签</span></span><br><span class="line">label=<span class="string">&#x27;cnt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出特征</span></span><br><span class="line">feas=df.columns.tolist()</span><br><span class="line">feas.remove(label)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;特征数量:&#x27;</span>,<span class="built_in">len</span>(feas))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过特征与标签划分</span></span><br><span class="line">X_train=df[feas]</span><br><span class="line">y_train=df[label]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X_train:&#x27;</span>,X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_train:&#x27;</span>,y_train.shape)</span><br></pre></td></tr></table></figure><h4 id="判断共线性"><a href="#判断共线性" class="headerlink" title="判断共线性"></a>判断共线性</h4><p>强相关特征会扰乱对权重的估计和解释（但这通常不会影响模型）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 计算相关系数（只能判断两两的线性关系）</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"> </span><br><span class="line">corr_res=X_train.corr()</span><br><span class="line">plt.subplots(figsize=(<span class="number">9</span>, <span class="number">9</span>))</span><br><span class="line"><span class="comment"># 为两两特征的相关性绘制热力图</span></span><br><span class="line">sns.heatmap(corr_res, annot=<span class="literal">True</span>, vmax=<span class="number">1</span>, square=<span class="literal">True</span>, cmap=<span class="string">&quot;coolwarm&quot;</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">15</span>) <span class="comment">#放大横纵坐标刻度线上的特征名字体</span></span><br><span class="line">plt.yticks(fontsize=<span class="number">15</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 计算方差膨胀因子（可以判断三个或更多变量之间的线性关系）</span></span><br><span class="line"><span class="comment"># 1.2.计算方差膨胀因子（可以判断三个或更多变量之间的线性关系）</span></span><br><span class="line"><span class="keyword">from</span> statsmodels.stats.outliers_influence <span class="keyword">import</span> variance_inflation_factor <span class="comment">#计算方差膨胀因子</span></span><br><span class="line"><span class="keyword">from</span> statsmodels.tools.tools <span class="keyword">import</span> add_constant  <span class="comment">#添加常量</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">checkVIF</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    【功能】计算方差膨胀因子</span></span><br><span class="line"><span class="string">    【参数】df:dataframe,特征集（不含target）</span></span><br><span class="line"><span class="string">    【返回】dataframe，展示各个特征的VIF</span></span><br><span class="line"><span class="string">    【参考】当0&lt;VIF&lt;10，不存在多重共线性；当10≤VIF&lt;100，存在较强的多重共线性；当VIF≥100，存在严重多重共线性。</span></span><br><span class="line"><span class="string">    【来源与介绍】https://blog.csdn.net/nixiang_888/article/details/122342338</span></span><br><span class="line"><span class="string">    【举例】VIF1=checkVIF(X_train)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    df = add_constant(df) <span class="comment">#添加一列常量const作为截距，全部赋值为1（不会改变原数据集）</span></span><br><span class="line">    name = df.columns</span><br><span class="line">    x = np.matrix(df)</span><br><span class="line">    VIF_list = [variance_inflation_factor(x,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>])]</span><br><span class="line">    VIF = pd.DataFrame(&#123;<span class="string">&#x27;feature&#x27;</span>:name,<span class="string">&quot;VIF&quot;</span>:VIF_list&#125;)</span><br><span class="line">    VIF = VIF.drop(index=<span class="number">0</span>) <span class="comment">#删除截距const行</span></span><br><span class="line">    VIF.sort_values([<span class="string">&#x27;VIF&#x27;</span>],ascending=<span class="literal">False</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    VIF[<span class="string">&#x27;remark&#x27;</span>]=np.where(VIF[<span class="string">&#x27;VIF&#x27;</span>]&gt;=<span class="number">100</span>,<span class="string">&#x27;严重多重共线性&#x27;</span>,np.where(VIF[<span class="string">&#x27;VIF&#x27;</span>]&gt;=<span class="number">10</span>,<span class="string">&#x27;较强多重共线性&#x27;</span>,<span class="string">&#x27;无多重共线性&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> VIF</span><br><span class="line"> </span><br><span class="line">VIF1=checkVIF(X_train)</span><br><span class="line">VIF1</span><br></pre></td></tr></table></figure><h4 id="判断正态性"><a href="#判断正态性" class="headerlink" title="判断正态性"></a>判断正态性</h4><p>假设目标结果服从正态分布，如违反，则特征权重的估计置信区间无效</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkNORM</span>(<span class="params">se,p=<span class="number">0.05</span>,alt=<span class="string">&#x27;two-sided&#x27;</span>,if_plot=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    【功能】检验一组数据是否符合正态分布</span></span><br><span class="line"><span class="string">    【参数】se:Series</span></span><br><span class="line"><span class="string">           p：float,p值，默认0.05</span></span><br><span class="line"><span class="string">           alt：str,默认双侧检验&#x27;two-sided&#x27;，可选&#x27;less&#x27;, &#x27;greater&#x27;</span></span><br><span class="line"><span class="string">           if_plot,是否画图，默认True</span></span><br><span class="line"><span class="string">    【返回】dataframe，展示各个特征的VIF</span></span><br><span class="line"><span class="string">    【参考】结果返回两个值：statistic → D值，pvalue → P值</span></span><br><span class="line"><span class="string">    【备注】import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="string">           %matplotlib inline </span></span><br><span class="line"><span class="string">    【举例】 res=checkNORM(y_train)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;数据量：&#x27;</span>,<span class="built_in">len</span>(se))</span><br><span class="line">    </span><br><span class="line">    u = se.mean()  <span class="comment"># 计算均值</span></span><br><span class="line">    std = se.std()  <span class="comment"># 计算标准差</span></span><br><span class="line">    res=stats.kstest(rvs=se, cdf=<span class="string">&#x27;norm&#x27;</span>,args= (u, std), alternative=alt)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;p值为:&#x27;</span>,res[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> res[<span class="number">1</span>]&gt;p:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;p值&gt;&#x27;</span>,p,<span class="string">&#x27;符合正态分布&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">         <span class="built_in">print</span>(<span class="string">&#x27;p值&lt;=&#x27;</span>,p,<span class="string">&#x27;不符合正态分布&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> if_plot==<span class="literal">True</span>:</span><br><span class="line">        fig = plt.figure(figsize = (<span class="number">10</span>,<span class="number">6</span>))         </span><br><span class="line">        se.hist(bins=<span class="number">30</span>,alpha = <span class="number">0.5</span>) <span class="comment">#直方图 alpha表示透明度</span></span><br><span class="line">        se.plot(kind = <span class="string">&#x27;kde&#x27;</span>, secondary_y=<span class="literal">True</span>) <span class="comment">#核密度估计KDE</span></span><br><span class="line">        plt.show()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验某一特征的取值是否符合正态分布</span></span><br><span class="line">res=checkNORM(y_train)</span><br></pre></td></tr></table></figure><h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><p>线性模型的库有：statsmodels库；sklearn库。其中statsmodels库的信息更多、更全。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.regression.linear_model <span class="keyword">import</span> OLS,GLS <span class="comment">#Ordinary least squares普通最小二乘法</span></span><br><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"> </span><br><span class="line"><span class="comment">#建模方式1：使用smf.ols，自己编写formula，会自动添加常数列</span></span><br><span class="line"><span class="comment">#cnt为目标变量，分类特征可使用C(season)进行编码，由于本数据集的分类特征都已事先编码，因此不需要添加c()</span></span><br><span class="line">model=smf.ols(formula=<span class="string">&#x27;cnt ~  season_夏 +  season_秋 + season_冬 + holiday + workingday +weathersit_雾 + weathersit_雨雪 + temp + hum + windspeed +days_since_2011 &#x27;</span>,data=df)</span><br><span class="line">results=model.fit()</span><br><span class="line">results.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">#建模方式2：使用sm.OLS，需要先sm.add_constant添加常数列以计算截距</span></span><br><span class="line">X = sm.add_constant(X_train)</span><br><span class="line">model=sm.OLS(endog=y_train,exog=X)</span><br><span class="line">results=model.fit()</span><br><span class="line">results.summary()</span><br></pre></td></tr></table></figure><h4 id="将权重表的值组合为一个DataFrame"><a href="#将权重表的值组合为一个DataFrame" class="headerlink" title="将权重表的值组合为一个DataFrame"></a>将权重表的值组合为一个DataFrame</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取模型的权重</span></span><br><span class="line">df_coef=pd.DataFrame(results.params)</span><br><span class="line">df_coef.reset_index(inplace=<span class="literal">True</span>)</span><br><span class="line">df_coef.columns=[<span class="string">&#x27;feature&#x27;</span>,<span class="string">&#x27;coef&#x27;</span>]</span><br><span class="line"><span class="comment">#获取权重的置信区间下限</span></span><br><span class="line">df_coef[<span class="string">&#x27;lw&#x27;</span>]=results.conf_int(alpha=<span class="number">0.05</span>)[<span class="number">0</span>].values </span><br><span class="line"><span class="comment">#获取权重的置信区间上限</span></span><br><span class="line">df_coef[<span class="string">&#x27;up&#x27;</span>]=results.conf_int(alpha=<span class="number">0.05</span>)[<span class="number">1</span>].values </span><br><span class="line"><span class="comment">#权重的标准误std err</span></span><br><span class="line">df_coef[<span class="string">&#x27;SE&#x27;</span>]=results.bse.values </span><br><span class="line"><span class="comment">#权重的t统计量，等于权重/标准误</span></span><br><span class="line">df_coef[<span class="string">&#x27;t&#x27;</span>]=results.tvalues.values </span><br><span class="line"><span class="comment">#参数的t统计的双尾 p 值</span></span><br><span class="line">df_coef[<span class="string">&#x27;p&#x27;</span>]=results.pvalues.values </span><br><span class="line"><span class="comment">#求绝对值</span></span><br><span class="line">df_coef[<span class="string">&#x27;t_abs&#x27;</span>]=<span class="built_in">abs</span>(df_coef[<span class="string">&#x27;t&#x27;</span>]) </span><br><span class="line"><span class="comment">#根据已有的权重和置信区间计算上下误差，计算完毕后发现上下误差相同</span></span><br><span class="line">df_coef[<span class="string">&#x27;lw_err&#x27;</span>]=df_coef[<span class="string">&#x27;coef&#x27;</span>]-df_coef[<span class="string">&#x27;lw&#x27;</span>] </span><br><span class="line">df_coef[<span class="string">&#x27;up_err&#x27;</span>]=df_coef[<span class="string">&#x27;up&#x27;</span>]-df_coef[<span class="string">&#x27;coef&#x27;</span>]</span><br><span class="line">df_coef</span><br></pre></td></tr></table></figure><h4 id="获取实例的真实值、预测值、置信区间，画图"><a href="#获取实例的真实值、预测值、置信区间，画图" class="headerlink" title="获取实例的真实值、预测值、置信区间，画图"></a>获取实例的真实值、预测值、置信区间，画图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取置信区间的上下限</span></span><br><span class="line">pred_ols = results.get_prediction()</span><br><span class="line">iv_l = pred_ols.summary_frame()[<span class="string">&quot;obs_ci_lower&quot;</span>]</span><br><span class="line">iv_u = pred_ols.summary_frame()[<span class="string">&quot;obs_ci_upper&quot;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#results.fittedvalues为模型预测值</span></span><br><span class="line">target_df=pd.concat((y_train,results.fittedvalues,iv_l,iv_u),axis=<span class="number">1</span>)</span><br><span class="line">target_df.columns=[<span class="string">&#x27;true&#x27;</span>,<span class="string">&#x27;predict&#x27;</span>,<span class="string">&#x27;ci_lower&#x27;</span>,<span class="string">&#x27;ci_upper&#x27;</span>]</span><br><span class="line">target_df[<span class="string">&#x27;resid&#x27;</span>]=results.resid <span class="comment">#残差</span></span><br><span class="line">target_df</span><br><span class="line"> </span><br><span class="line"><span class="comment">#按实际租赁量排序，reset_index是必须的</span></span><br><span class="line">plot_df=target_df.sort_values([<span class="string">&#x27;true&#x27;</span>]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">20</span>, <span class="number">9</span>))</span><br><span class="line"> </span><br><span class="line">ax.plot(plot_df[<span class="string">&#x27;true&#x27;</span>], <span class="string">&quot;b-&quot;</span>, label=<span class="string">&quot;True&quot;</span>)</span><br><span class="line">ax.plot(plot_df[<span class="string">&#x27;predict&#x27;</span>], <span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;Pred&quot;</span>)</span><br><span class="line"><span class="comment"># ax.plot(plot_df[&#x27;ci_lower&#x27;], &quot;r--&quot;,alpha=0.5) #置信区间虚线</span></span><br><span class="line"><span class="comment"># ax.plot(plot_df[&#x27;ci_upper&#x27;], &quot;r--&quot;,alpha=0.5) #置信区间虚线</span></span><br><span class="line">plt.fill_between(plot_df.index,plot_df[<span class="string">&#x27;ci_lower&#x27;</span>],plot_df[<span class="string">&#x27;ci_upper&#x27;</span>],color=<span class="string">&#x27;blue&#x27;</span>,alpha=<span class="number">0.15</span>)</span><br><span class="line">ax.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">plt.ylabel(<span class="string">&#x27;自行车租赁量&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;真实值与预测值对比&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/e0fa58a2e9bd408c835e37dd767c56fe.png" alt="img"></p><p>蓝线为真实值，红色实线为预测值，蓝紫色为置信区间。</p><p>左侧租赁量较小时，部分预测值远高于真实值且波动较大；右侧租赁量较大时，预测值整体偏低。</p><p>由此图也可以看出，前文中提到的线性回归模型的【同方差性】在现实中是很难满足的。</p><p>本案例数据量较小，如果数据量较大，可以随机抽样后再画图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#残差——同方差性</span></span><br><span class="line"><span class="comment">#1.应该为均值是0的正态分布</span></span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;whitegrid&quot;</span>,font_scale=<span class="number">1.2</span>)<span class="comment">#设置主题，文本大小</span></span><br><span class="line">plt.hist(target_df[<span class="string">&#x27;resid&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/08b54268c9ad4457850154f1efa0ab39.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#2.残差与predict之间应该不相关</span></span><br><span class="line"><span class="comment">#regplot默认参数线性回归图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;whitegrid&quot;</span>,font_scale=<span class="number">1.2</span>)<span class="comment">#设置主题，文本大小</span></span><br><span class="line">g=sns.regplot(x=<span class="string">&#x27;resid&#x27;</span>, y=<span class="string">&#x27;predict&#x27;</span>, data=target_df,</span><br><span class="line">             color=<span class="string">&#x27;#000000&#x27;</span>,<span class="comment">#设置marker及线的颜色</span></span><br><span class="line">             <span class="comment"># marker=&#x27;*&#x27;,#设置marker形状</span></span><br><span class="line">             )</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/ac0045e81d3643c9a668f645b18a2bdc.png" alt="img"></p><h3 id="模型解释"><a href="#模型解释" class="headerlink" title="模型解释"></a>模型解释</h3><h4 id="拟合优度"><a href="#拟合优度" class="headerlink" title="拟合优度"></a>拟合优度</h4><p>R方=0.79，表示该模型解释了目标结果79%的方差，拟合优度较高，可解释性较高</p><h4 id="特征重要性"><a href="#特征重要性" class="headerlink" title="特征重要性"></a>特征重要性</h4><p>使用t-统计量的绝对值解释特征重要性。</p><p>特征重要性随权重增加而增加，随方差增加而减少（方差越大表明对正确值的把握越小）</p><script type="math/tex;mode=display">t=\frac{coef}{\text{std err}}</script><p>进行数据处理（求绝对值、排序）后画图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">plot_df=df_coef.drop(index=<span class="string">&#x27;Intercept&#x27;</span>) <span class="comment">#删除截距行</span></span><br><span class="line">plot_df=plot_df.sort_values([<span class="string">&#x27;t_abs&#x27;</span>]) <span class="comment">#排序</span></span><br><span class="line"> </span><br><span class="line">fig = plt.figure(figsize = (<span class="number">9</span>,<span class="number">5</span>))</span><br><span class="line">plt.barh(plot_df.index,plot_df[<span class="string">&#x27;t_abs&#x27;</span>]) <span class="comment">#画水平条形图</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#设置x轴y轴</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;t-value绝对值&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;特征&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">12</span>) <span class="comment">#放大横纵坐标刻度线上的特征名字体</span></span><br><span class="line">plt.yticks(fontsize=<span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;特征重要性&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h5 id="不用权重代表特征重要性原因"><a href="#不用权重代表特征重要性原因" class="headerlink" title="不用权重代表特征重要性原因"></a>不用权重代表特征重要性原因</h5><p>改变特征量纲，权重就会发生变化</p><p>为进行验证，将风速的单位从km/h转换为km/分钟，即将风速的原始值除以60</p><p><img src="/posts/1636220786/1f71f5ce26154b559f5d53560d537bff.png" alt="img"></p><p><img src="/posts/1636220786/471be780371946938f5580cea7dab85f.png" alt="img"></p><blockquote><p>结论：</p><p>其他特征的估计均不变。风速的部分估计发生了变化</p><p>权重（coef）：扩大了60倍</p><p>权重标准误（std err）：扩大了60倍</p><p>t统计量：不变（t=权重/标准误）</p><p>p值：不变</p><p>置信区间：扩大了60倍</p></blockquote><p>所以在风速的本质并未发生改变的情况下，如果采用权重（coef）作为特征重要性的度量依据，就会发现其值会随着量纲的变化而变化，但t统计量却可以保持一致。</p><h4 id="根据权重和置信区间画权重图"><a href="#根据权重和置信区间画权重图" class="headerlink" title="根据权重和置信区间画权重图"></a>根据权重和置信区间画权重图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">plot_df=df_coef.drop(index=<span class="string">&#x27;Intercept&#x27;</span>) <span class="comment">#删除截距行</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line"><span class="comment">#由于上下误差相同，因此直接用 xerr=plot_df[&#x27;lw_err&#x27;]，否则可以使用xerr=plot_df[[&#x27;lw_err&#x27;,&#x27;up_err&#x27;]].T.values来分别规定上下限</span></span><br><span class="line">plt.errorbar(x=plot_df[<span class="string">&#x27;coef&#x27;</span>], y=plot_df.index,xerr=plot_df[<span class="string">&#x27;lw_err&#x27;</span>], color=<span class="string">&quot;black&quot;</span>, capsize=<span class="number">3</span>,</span><br><span class="line">             linestyle=<span class="string">&quot;None&quot;</span>,</span><br><span class="line">             marker=<span class="string">&quot;s&quot;</span>, markersize=<span class="number">7</span>, mfc=<span class="string">&quot;black&quot;</span>, mec=<span class="string">&quot;black&quot;</span>)</span><br><span class="line"> </span><br><span class="line">plt.grid(<span class="literal">True</span>) <span class="comment">#显示网格线</span></span><br><span class="line"> </span><br><span class="line">plt.xlabel(<span class="string">&#x27;权重估计&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;特征&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">12</span>) <span class="comment">#放大横纵坐标刻度线上的特征名字体</span></span><br><span class="line">plt.yticks(fontsize=<span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;权重估计图&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line"> </span><br><span class="line">plt.axvline(c=<span class="string">&quot;c&quot;</span>,ls=<span class="string">&quot;--&quot;</span>,lw=<span class="number">2</span>) <span class="comment">#原点竖线</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/31e6b35c8f9644eeb0cc10e5d98d1f31.png" alt="img"></p><blockquote><p>由上图可知：</p><p>1.雨雪天气对自行车租赁量有很大的负效应。</p><p>2.是否工作日的权重接近于0，并且95%的置信区间包含0，这表明该效应在统计上不显著。</p><p>3.温度的置信区间很短，估计值接近于0，但特征效应在统计上是显著的。</p><p>权重图的问题：</p><p>各个特征的量纲不一样，比如天气情况反映了晴天和雨雪天的差异，但是温度只反映了1℃的变化情况。</p><p>因此可以通过<strong>在建模前对特征进行标准化（均值为0，标准差为1），使估计的权重更具有可比性。</strong></p></blockquote><h4 id="效应图"><a href="#效应图" class="headerlink" title="效应图"></a>效应图</h4><p>效应图（effect plot）帮助了解权重和特征的组合对数据预测的贡献程度。<strong>特征效应为每个特征的权重乘以实例的特征值。</strong> 如改变特征的量纲，则权重会发生变化，但特征效应不会改变</p><p>通过画箱线图（注意，分类特征总结为一个箱线图），可以观察下面几个方面：</p><ol><li>特征效应的正负性</li><li>特征效应的绝对值大小</li><li>特征效应的方差（如果方差小，则意味着这个特征几乎在所有实例中都有类似的贡献）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#求特征效应——每个特征的权重乘以实例的特征值</span></span><br><span class="line">w=df_coef[<span class="string">&#x27;coef&#x27;</span>].values</span><br><span class="line">w_order=[] <span class="comment">#将特征权重与实例中的顺序一一对应</span></span><br><span class="line">my_dict=&#123;<span class="number">0</span>:<span class="number">4</span>,<span class="number">1</span>:<span class="number">5</span>,<span class="number">2</span>:<span class="number">8</span>,<span class="number">3</span>:<span class="number">9</span>,<span class="number">4</span>:<span class="number">10</span>,<span class="number">5</span>:<span class="number">11</span>,<span class="number">6</span>:<span class="number">3</span>,<span class="number">7</span>:<span class="number">1</span>,<span class="number">8</span>:<span class="number">2</span>,<span class="number">9</span>:<span class="number">6</span>,<span class="number">10</span>:<span class="number">7</span>&#125; <span class="comment">#权重表与数据集中特征的对应顺序</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    w_order.insert(i,w[my_dict[i]])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#计算特征效应    </span></span><br><span class="line">effect=X_train*w_order </span><br><span class="line"> </span><br><span class="line"><span class="comment">#分类特征合并</span></span><br><span class="line">effect[<span class="string">&#x27;season&#x27;</span>]=np.<span class="built_in">sum</span>(effect[[<span class="string">&#x27;season_冬&#x27;</span>,<span class="string">&#x27;season_夏&#x27;</span>,<span class="string">&#x27;season_秋&#x27;</span>]],axis=<span class="number">1</span>)</span><br><span class="line">effect[<span class="string">&#x27;weathersit&#x27;</span>]=np.<span class="built_in">sum</span>(effect[[<span class="string">&#x27;weathersit_雾&#x27;</span>,<span class="string">&#x27;weathersit_雨雪&#x27;</span>]],axis=<span class="number">1</span>)</span><br><span class="line">effect</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/3a97ed58df26454da61f3c12463acdc3.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">plt.subplots(figsize=(<span class="number">9</span>, <span class="number">9</span>))</span><br><span class="line"> </span><br><span class="line">cols=[<span class="string">&#x27;holiday&#x27;</span>, <span class="string">&#x27;workingday&#x27;</span>, <span class="string">&#x27;temp&#x27;</span>, <span class="string">&#x27;hum&#x27;</span>, <span class="string">&#x27;windspeed&#x27;</span>, <span class="string">&#x27;days_since_2011&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;season&#x27;</span>, <span class="string">&#x27;weathersit&#x27;</span>]</span><br><span class="line">sns.boxplot(data=effect[cols],orient=<span class="string">&quot;h&quot;</span>,width=<span class="number">0.5</span>,whis=<span class="number">0.5</span>, palette=<span class="string">&quot;Set2&quot;</span>)</span><br><span class="line"> </span><br><span class="line">plt.grid(<span class="literal">True</span>) <span class="comment">#显示网格线</span></span><br><span class="line"> </span><br><span class="line">plt.xlabel(<span class="string">&#x27;特征效应&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;特征&#x27;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">12</span>) <span class="comment">#放大横纵坐标刻度线上的特征名字体</span></span><br><span class="line">plt.yticks(fontsize=<span class="number">12</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;特征效应图&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line"> </span><br><span class="line">plt.axvline(c=<span class="string">&quot;c&quot;</span>,ls=<span class="string">&quot;--&quot;</span>,lw=<span class="number">2</span>) <span class="comment">#原点竖线</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/89622e6a579b4c9fb24268ae96c8657e.png" alt="img"></p><blockquote><p>由上图可知：</p><p>1.对预测自行车租赁数量正向贡献最大的来自温度和天数。</p><p>2.天气的情况参照类别为晴天，图中说明除了晴天外的天气（雾、雨雪）都会对自行车租赁量产生负向影响。</p></blockquote><h2 id="lasso特征选择"><a href="#lasso特征选择" class="headerlink" title="lasso特征选择"></a>lasso特征选择</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/totobey/article/details/124994579">https://blog.csdn.net/totobey/article/details/124994579</a></p><p>首先想到的是降维，争取用尽可能少的数据解决问题。Lasso使用L1正则化对权重加大惩罚，可以使很多权重估计值为0</p><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">#统计运行时间用</span></span><br><span class="line"><span class="keyword">import</span> copy  <span class="comment">#深拷贝的时候用</span></span><br><span class="line"><span class="keyword">import</span> _pickle <span class="keyword">as</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gc <span class="comment">#释放内存使用</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm,tqdm_notebook  <span class="comment">#Tqdm 是一个快速，可扩展的Python进度条</span></span><br><span class="line"><span class="keyword">import</span> datetime <span class="comment">#处理时间数据</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment">## 1. 标准化</span></span><br><span class="line">scaler = StandardScaler()  <span class="comment"># 标准化 z = (x - u) / s</span></span><br><span class="line">X_train_std = pd.DataFrame(scaler.fit_transform(X_train))</span><br><span class="line">X_train_std.columns=X_train.columns</span><br><span class="line">X_train_std</span><br><span class="line"></span><br><span class="line"><span class="comment">## 2. 对分类特征one-hot编码</span></span><br></pre></td></tr></table></figure><h3 id="运行lasso"><a href="#运行lasso" class="headerlink" title="运行lasso"></a>运行lasso</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_feas_lasso</span>(<span class="params">trainX,trainy,metric_name=<span class="string">&#x27;rmse&#x27;</span>,kfNum=<span class="number">2</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        【功能说明】 </span></span><br><span class="line"><span class="string">        【参数】trainX:DataFrame,训练集的特征部分，需要先进行标准化，one-hot编码需要保留参照类别</span></span><br><span class="line"><span class="string">               trainy:Series，训练集的标签列</span></span><br><span class="line"><span class="string">               metric_name:str，评估指标，默认&#x27;rmse&#x27;,可选&#x27;logloss&#x27;,&#x27;auc&#x27;</span></span><br><span class="line"><span class="string">               kfNum:int,&gt;=2,默认2，交叉验证轮数</span></span><br><span class="line"><span class="string">        【返回】字典，包含参数array，评估指标均值、标准差，保留特征数均值、标准差</span></span><br><span class="line"><span class="string">        【举例】scaler = StandardScaler()  # 标准化 z = (x - u) / s</span></span><br><span class="line"><span class="string">               X_train_std = pd.DataFrame(scaler.fit_transform(X_train))</span></span><br><span class="line"><span class="string">               res=select_feas_lasso(trainX=X_train_std,trainy=y_train,metric_name=&#x27;rmse&#x27;,kfNum=2)</span></span><br><span class="line"><span class="string">               lasso_alphas=res[&#x27;lasso_alphas&#x27;]</span></span><br><span class="line"><span class="string">               valid_scores=res[&#x27;valid_scores&#x27;]</span></span><br><span class="line"><span class="string">               keep_var_nums=res[&#x27;keep_var_nums&#x27;]</span></span><br><span class="line"><span class="string">        【版本】V1.0</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    s=time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n********lasso_select_feas...start&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;评估指标：&#x27;</span>,metric_name)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;交叉验证轮数：&#x27;</span>,kfNum)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练集形状：&#x27;</span>,trainX.shape,<span class="built_in">type</span>(trainX))</span><br><span class="line"> </span><br><span class="line">    <span class="comment">#对于#0.001-100，使用logspace</span></span><br><span class="line">    lasso_alphas1 = np.logspace(start=-<span class="number">3</span>, stop=<span class="number">2</span>, num=<span class="number">50</span>, base=<span class="number">10</span>) <span class="comment">#0.001-100</span></span><br><span class="line">    <span class="comment">#对于比较大的lambda，使用整数步长</span></span><br><span class="line">    lasso_alphas2 = np.arange(start=<span class="number">100</span>,stop=<span class="number">1000</span>,step=<span class="number">20</span>)</span><br><span class="line">    lasso_alphas= np.concatenate((lasso_alphas1, lasso_alphas2))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;待计算正则化参数数量：&#x27;</span>,<span class="built_in">len</span>(lasso_alphas))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;待计算正则化参数最小值：&#x27;</span>,np.<span class="built_in">min</span>(lasso_alphas))   </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;待计算正则化参数最大值：&#x27;</span>,np.<span class="built_in">max</span>(lasso_alphas))   </span><br><span class="line"> </span><br><span class="line">    valid_scores = [] <span class="comment">#存储每个正则化参数下的评估指标均值如rmse</span></span><br><span class="line">    keep_var_nums = [] <span class="comment">#存储每个正则化参数下保留的特征数量均值</span></span><br><span class="line">    valid_scores_std = [] <span class="comment">#标准差</span></span><br><span class="line">    keep_var_nums_std = []  <span class="comment">#标准差</span></span><br><span class="line">    <span class="keyword">for</span>  alpha <span class="keyword">in</span> tqdm(lasso_alphas):</span><br><span class="line">        clf = Lasso(max_iter=<span class="number">1000</span>,random_state=<span class="number">2020</span>,alpha=alpha)</span><br><span class="line">        kf=KFold(n_splits=kfNum, shuffle=<span class="literal">True</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">        valid_score=[]  <span class="comment">#存储每轮交叉验证的评估指标如rmse</span></span><br><span class="line">        keep_var_num=[] <span class="comment">#存储每轮交叉验证保留特征数量</span></span><br><span class="line">        <span class="keyword">for</span> i,(trn_index,val_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(trainX,trainy)):  <span class="comment">#i从0开始，可以显示第几轮了</span></span><br><span class="line">            trn_df=trainX.iloc[trn_index]</span><br><span class="line">            val_df=trainX.iloc[val_index]            </span><br><span class="line">            trn_y=trainy.iloc[trn_index]</span><br><span class="line">            val_y=trainy.iloc[val_index]</span><br><span class="line"> </span><br><span class="line">            clf.fit(X=trn_df, y=trn_y)</span><br><span class="line">            <span class="comment">#利用本轮模型预测本轮验证集  </span></span><br><span class="line">            valid_pred=clf.predict(val_df)</span><br><span class="line">            <span class="comment">#-------计算本轮评估指标--------#</span></span><br><span class="line">            <span class="keyword">if</span> metric_name == <span class="string">&#x27;rmse&#x27;</span>:</span><br><span class="line">                valid_score_this=mean_squared_error(val_y,valid_pred,squared=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">elif</span> metric_name == <span class="string">&#x27;logloss&#x27;</span>:</span><br><span class="line">                valid_score_this=log_loss(y_true=val_y,y_pred=valid_pred)</span><br><span class="line">            <span class="keyword">elif</span> metric_name == <span class="string">&#x27;auc&#x27;</span>:</span><br><span class="line">                valid_score_this=roc_auc_score(y_true=val_y,y_score=valid_pred)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;亲，没这评估指标&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">            valid_score.append(valid_score_this)  <span class="comment">#列表append后直接替换原对象，所以不用再赋值</span></span><br><span class="line">            <span class="comment"># print(valid_score)</span></span><br><span class="line">            keep_var_num=<span class="built_in">sum</span>(clf.coef_ != <span class="number">0</span>) <span class="comment">#统计系数不为0的特征数量（不含截距）</span></span><br><span class="line">            <span class="comment"># print(keep_var_num)</span></span><br><span class="line">        valid_scores.append(np.mean(valid_score)) <span class="comment">#metric取均值，存入</span></span><br><span class="line">        keep_var_nums.append(np.mean(keep_var_num)) <span class="comment">#保留特征数量取均值，存入</span></span><br><span class="line">        valid_scores_std.append(np.std(valid_score)) <span class="comment">#metric取标准差，存入</span></span><br><span class="line">        keep_var_nums_std.append(np.std(keep_var_num)) <span class="comment">#保留特征数量取均值取标准差，存入</span></span><br><span class="line">          </span><br><span class="line">    res=&#123;<span class="string">&#x27;lasso_alphas&#x27;</span>:lasso_alphas,</span><br><span class="line">         <span class="string">&#x27;valid_scores&#x27;</span>:valid_scores,<span class="string">&#x27;valid_scores_std&#x27;</span>:valid_scores_std,</span><br><span class="line">         <span class="string">&#x27;keep_var_nums&#x27;</span>:keep_var_nums,<span class="string">&#x27;keep_var_nums_std&#x27;</span>:keep_var_nums_std&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"> </span><br><span class="line">res=select_feas_lasso(trainX=X_train_std,trainy=y_train,metric_name=<span class="string">&#x27;rmse&#x27;</span>,kfNum=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/6f635a8ea3d046f599751f2764c425ec.png" alt="img"></p><h3 id="以正则化参数为x轴，特征数量、评估指标为双y轴画图"><a href="#以正则化参数为x轴，特征数量、评估指标为双y轴画图" class="headerlink" title="以正则化参数为x轴，特征数量、评估指标为双y轴画图"></a>以正则化参数为x轴，特征数量、评估指标为双y轴画图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">lasso_alphas=res[<span class="string">&#x27;lasso_alphas&#x27;</span>]</span><br><span class="line">valid_scores=res[<span class="string">&#x27;valid_scores&#x27;</span>]</span><br><span class="line">keep_var_nums=res[<span class="string">&#x27;keep_var_nums&#x27;</span>]</span><br><span class="line">mertic_name=<span class="string">&#x27;RMSE&#x27;</span></span><br><span class="line"> </span><br><span class="line">fig  = plt.figure(figsize=(<span class="number">18</span>, <span class="number">8</span>))</span><br><span class="line">ax1=fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.plot(lasso_alphas,keep_var_nums, <span class="string">&quot;b-o&quot;</span>,label=<span class="string">&#x27;特征数量&#x27;</span>) <span class="comment">#画出折线并且添加实心圆点</span></span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;特征数量&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">ax1.grid(<span class="literal">True</span>) <span class="comment">#显示网格线</span></span><br><span class="line">xmajorLocator  = MultipleLocator(<span class="number">100</span>)  <span class="comment"># x轴刻度间隔 100</span></span><br><span class="line">ymajorLocator  = MultipleLocator(<span class="number">1</span>)    <span class="comment"># y轴刻度间隔 1</span></span><br><span class="line">ax1.yaxis.set_major_locator(ymajorLocator) </span><br><span class="line">ax1.xaxis.set_major_locator(xmajorLocator) </span><br><span class="line">plt.xlabel(<span class="string">&#x27;正则化参数&#x27;</span>,fontsize=<span class="number">18</span>) <span class="comment">#添加x轴名称</span></span><br><span class="line"> </span><br><span class="line">ax2 = ax1.twinx()</span><br><span class="line">ax2.plot(lasso_alphas,valid_scores, <span class="string">&quot;r-D&quot;</span>,label=mertic_name)  <span class="comment">#画出折线并且添加实心菱形</span></span><br><span class="line">ax2.set_ylabel(mertic_name,fontsize=<span class="number">20</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">ax1.legend(loc=<span class="string">&#x27;center left&#x27;</span>,fontsize=<span class="number">15</span>) <span class="comment">#添加图例</span></span><br><span class="line">ax2.legend(loc=<span class="string">&#x27;center right&#x27;</span>,fontsize=<span class="number">15</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">plt.title(<span class="string">&#x27;lasso&#x27;</span>,fontsize=<span class="number">30</span>) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/f74c450dcdda4f1a886f61dbb7a1d9ae.png" alt="img"></p><p><strong>较大的 alpha 值会导致更强的正则化，从而选择更少的特征</strong>。</p><p><strong>较小的 alpha 值会选择更多的特征</strong>。</p><h3 id="参照图，找到模型预测效果-保留特征数量均合适的正则化参数值"><a href="#参照图，找到模型预测效果-保留特征数量均合适的正则化参数值" class="headerlink" title="参照图，找到模型预测效果+保留特征数量均合适的正则化参数值"></a>参照图，找到模型预测效果+保留特征数量均合适的正则化参数值</h3><p>如何选择正则化超参数：将其视为一个待优化参数，通过交叉验证找到模型预测效果+保留特征数量都合适的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 保留5个变量（示例）</span></span><br><span class="line"><span class="comment"># 由于画图使用的是交叉验证，后续用的是全量实例，因此正则化参数值可能会有微小区别。  </span></span><br><span class="line"><span class="comment"># 以上图的正则化参数值220为基础，调试后将其设定为250  </span></span><br><span class="line"><span class="comment"># 找到保留的5个变量</span></span><br><span class="line">best_clf = Lasso(max_iter=<span class="number">1000</span>,random_state=<span class="number">2020</span>,alpha=<span class="number">250</span>)</span><br><span class="line">best_clf.fit(X=X_train_std, y=y_train)</span><br><span class="line">coef=pd.Series(best_clf.coef_,index=best_clf.feature_names_in_)</span><br><span class="line">coef</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/fddbc98017dd44499fa7014bc93d95c7.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#连续型变量直接入选（temp、days_since_2011 ）</span></span><br><span class="line"><span class="comment">#上表中，季节中只有春季入选，因此其他三个季节（非春季）统一构成参照类别。这与基础线性回归时将春单独作为参照类别有很大不同，后续进行模型解释时要特别注意</span></span><br><span class="line"><span class="comment">#上表中，天气情况入选的为晴、雨雪，最终选择雾、雨雪。变更后本质一样，只是将参照类别从雾变更为了晴以提高可解释性</span></span><br><span class="line">keep_var5=[<span class="string">&#x27;temp&#x27;</span>,<span class="string">&#x27;days_since_2011&#x27;</span>,<span class="string">&#x27;season_春&#x27;</span>,<span class="string">&#x27;weathersit_雾&#x27;</span>,<span class="string">&#x27;weathersit_雨雪&#x27;</span>]</span><br></pre></td></tr></table></figure><p>scikit-learn 的Lasso实现中，更常用的是LassoCV(沿着正则化路径具有迭代拟合的线性模型)，对超参数 $\alpha$ 进行交叉验证，选择一个合适的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 参数</span></span><br><span class="line">eps：路径的长度。eps=<span class="number">1e-3</span>意味着alpha_min / alpha_max = <span class="number">1e-3</span>。</span><br><span class="line">n_alphas:沿正则化路径的Alpha个数，默认<span class="number">100</span>。</span><br><span class="line">alphas：用于计算模型的alpha列表。如果为<span class="literal">None</span>，自动设置Alpha。</span><br><span class="line">fit_intercept：是否估计截距，默认<span class="literal">True</span>。如果为<span class="literal">False</span>，则假定数据已经中心化。</span><br><span class="line">tol：优化的容忍度，默认<span class="number">1e-4</span>：如果更新小于tol，优化代码将检查对偶间隙的最优性，并一直持续到它小于tol为止</span><br><span class="line">cv：定交叉验证拆分策略</span><br><span class="line"></span><br><span class="line"><span class="comment">### 属性</span></span><br><span class="line"></span><br><span class="line">alpha_：交叉验证选择的惩罚量</span><br><span class="line">coef_：参数向量（目标函数公式中的w）。</span><br><span class="line">intercept_：目标函数中的截距。</span><br><span class="line">mse_path_：每次折叠不同alpha下测试集的均方误差。</span><br><span class="line">alphas_：对于每个l1_ratio，用于拟合的alpha网格。</span><br><span class="line">dual_gap_：最佳alpha（alpha_）优化结束时的双重间隔。</span><br><span class="line">n_iter_ <span class="built_in">int</span>：坐标下降求解器运行的迭代次数，以达到指定容忍度的最优alpha。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 方法</span></span><br><span class="line"></span><br><span class="line">fit(X, y[, sample_weight, check_input]) 用坐标下降法拟合模型。</span><br><span class="line">get_params([deep]) 获取此估计器的参数。</span><br><span class="line">path(X, y, *[, l1_ratio, eps, n_alphas, …]) 计算具有坐标下降的弹性网路径。</span><br><span class="line">predict(X) 使用线性模型进行预测。</span><br><span class="line">score(X, y[, sample_weight]) 返回预测的确定系数R ^ <span class="number">2</span>。</span><br><span class="line">set_params(**params) 设置此估算器的参数。</span><br></pre></td></tr></table></figure><h3 id="线性回归-建模解释"><a href="#线性回归-建模解释" class="headerlink" title="线性回归+建模解释"></a>线性回归+建模解释</h3><p>与线性回归区别在于训练特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df=bike_ohe_comp</span><br><span class="line">label=<span class="string">&#x27;cnt&#x27;</span></span><br><span class="line"> </span><br><span class="line">feas=df.columns.tolist()</span><br><span class="line">feas.remove(label)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;特征数量:&#x27;</span>,<span class="built_in">len</span>(feas))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 区别</span></span><br><span class="line">X_train=df[keep_var5]</span><br><span class="line">y_train=df[label]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;X_train:&#x27;</span>,X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_train:&#x27;</span>,y_train.shape)</span><br></pre></td></tr></table></figure><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="波士顿房价"><a href="#波士顿房价" class="headerlink" title="波士顿房价"></a>波士顿房价</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 导入库 </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="comment">##  读取数据</span></span><br><span class="line">url = <span class="string">r&#x27;F:\100-Days-Of-ML-Code\datasets\Regularization_Boston.csv&#x27;</span></span><br><span class="line">df = pd.read_csv(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">scaler=StandardScaler()</span><br><span class="line">df_sc= scaler.fit_transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集划分</span></span><br><span class="line">df_sc = pd.DataFrame(df_sc, columns=df.columns)</span><br><span class="line">y = df_sc[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">X = df_sc.drop(<span class="string">&#x27;price&#x27;</span>, axis=<span class="number">1</span>) <span class="comment"># becareful inplace= False</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>Lasso调参数，主要就是选择合适的alpha，上面提到LassoCV，GridSearchCV都可以实现</p><p>为绘图，手动实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">lasso = Lasso()</span><br><span class="line">coefs_lasso = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数步长</span></span><br><span class="line">alpha_lasso = <span class="number">10</span>**np.linspace(-<span class="number">3</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> alpha_lasso:</span><br><span class="line">    lasso.set_params(alpha = i)</span><br><span class="line">    lasso.fit(X_train, y_train)</span><br><span class="line">    coefs_lasso.append(lasso.coef_)</span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">10</span>))</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.plot(alpha_lasso, coefs_lasso)</span><br><span class="line">ax.set_xscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;alpha&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;weights: scaled coefficients&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Lasso regression coefficients Vs. alpha&#x27;</span>)</span><br><span class="line">plt.legend(df.drop(<span class="string">&#x27;price&#x27;</span>,axis=<span class="number">1</span>, inplace=<span class="literal">False</span>).columns)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/posts/1636220786/v2-bee228f16814c4a170a85646a272e642_720w.webp" alt="img"></p><p>图中展示的是不同的变量随着alpha惩罚后，其系数的变化，我们要保留的就是系数不为0的变量。</p><p><strong>alpha值不断增大时系数才变为0的变量在模型中越重要</strong></p><ul><li>正则化整体上是损害优化，使参数尽可能趋于0，而重要变量的系数比较大，只有正则作用比较大时，才会令其趋于0</li></ul><p>按系数绝对值大小倒序查看特征重要性，可以设置更大的超参数，可以看到更多的变量系数被压缩为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lasso = Lasso(alpha=<span class="number">10</span>**(-<span class="number">3</span>))</span><br><span class="line">model_lasso = lasso.fit(X_train, y_train)</span><br><span class="line">coef = pd.Series(model_lasso.coef_,index=X_train.columns)</span><br><span class="line"><span class="comment"># 输出排序</span></span><br><span class="line"><span class="built_in">print</span>(coef[coef != <span class="number">0</span>].<span class="built_in">abs</span>().sort_values(ascending = <span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line">LSTAT2 <span class="number">2.876424</span></span><br><span class="line">LSTAT <span class="number">2.766566</span></span><br><span class="line">LSTAT4 <span class="number">0.853773</span></span><br><span class="line">LSTAT5 <span class="number">0.178117</span></span><br><span class="line">LSTAT10 <span class="number">0.102558</span></span><br><span class="line">LSTAT9 <span class="number">0.088525</span></span><br><span class="line">LSTAT8 <span class="number">0.001112</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">lasso = Lasso(alpha=<span class="number">10</span>**(-<span class="number">2</span>))</span><br><span class="line">model_lasso = lasso.fit(X_train, y_train)</span><br><span class="line">coef = pd.Series(model_lasso.coef_,index=X_train.columns)</span><br><span class="line"><span class="built_in">print</span>(coef[coef != <span class="number">0</span>].<span class="built_in">abs</span>().sort_values(ascending = <span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line">LSTAT <span class="number">1.220552</span></span><br><span class="line">LSTAT3 <span class="number">0.625608</span></span><br><span class="line">LSTAT10 <span class="number">0.077125</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>或者直接绘制柱状图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fea = X_train.columns</span><br><span class="line">a = pd.DataFrame()</span><br><span class="line">a[<span class="string">&#x27;feature&#x27;</span>] = fea</span><br><span class="line">a[<span class="string">&#x27;importance&#x27;</span>] = coef.values</span><br><span class="line"></span><br><span class="line">a = a.sort_values(<span class="string">&#x27;importance&#x27;</span>,ascending = <span class="literal">False</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">plt.barh(a[<span class="string">&#x27;feature&#x27;</span>],a[<span class="string">&#x27;importance&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;the importance features&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>LASSO回归的优点是可以你不最小二乘法的不足，可以很好的进行特征选择</p><p>缺点是，LASSO 特征选择的结果可能会受到特征之间相关性的影响。如果您的数据集中存在高度相关的特征，LASSO 可能会随机选择其中之一</p><p>在这种情况下，您可能需要考虑使用其他方法，如 Elastic Net（结合了 LASSO 和 Ridge 回归）。</p><h3 id="模拟"><a href="#模拟" class="headerlink" title="模拟"></a>模拟</h3><p>基于scikit-learn的Lasso特征选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">claude35</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line"><span class="comment"># X为100个样本，20个变量维度，noise：噪声水平，random_state指定的随机数种子</span></span><br><span class="line"><span class="comment"># 回归模型，利用带噪声数据y&#x27;=X*w+b+noise，拟合\hat&#123;y&#125;=f(X),</span></span><br><span class="line">X, y, coef = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">20</span>, noise=<span class="number">0.1</span>, coef=<span class="literal">True</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用log1p对X和y进行归一化</span></span><br><span class="line">X_log = np.log1p(np.<span class="built_in">abs</span>(X))</span><br><span class="line">y_log = np.log1p(np.<span class="built_in">abs</span>(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化特征，使得不同特征具有相同的尺度，方便后续的模型训练和预测。</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X_log)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建LASSO模型，alpha为正则项权重 Y=W*X+b</span></span><br><span class="line"><span class="comment"># 损失函数为L1正则的最小二乘损失，alpha为正则超参数，alpha越小，模型越接近没有正则化的情况，即模型越接近原始模型</span></span><br><span class="line"><span class="comment"># 默认情况下，如在scikit-learn中，alpha=1.0，意味着有较强的正则化作用。</span></span><br><span class="line">lasso = Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">lasso.fit(X_scaled, y_log)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征重要性</span></span><br><span class="line">feature_importance = np.<span class="built_in">abs</span>(lasso.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择非零系数对应的特征</span></span><br><span class="line">selected_features = np.where(feature_importance &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选中的特征索引:&quot;</span>, selected_features)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选中的特征数量:&quot;</span>, <span class="built_in">len</span>(selected_features))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importance[selected_features])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取原始特征名称（假设特征有名称）</span></span><br><span class="line">original_feature_names = [<span class="string">f&quot;Feature_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>])]</span><br><span class="line">selected_feature_names = [original_feature_names[i] <span class="keyword">for</span> i <span class="keyword">in</span> selected_features]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选中的特征名称:&quot;</span>, selected_feature_names)</span><br></pre></td></tr></table></figure><p>使用 log1p 变换的好处包括：</p><ul><li>它可以帮助处理偏斜分布的数据，使其更接近正态分布。</li><li>它可以压缩大的值，同时保留小的值的差异。</li><li>对于零值，log1p (0) = 0，所以它可以安全地处理包含零的数据。</li></ul><h2 id="LASSO收敛"><a href="#LASSO收敛" class="headerlink" title="LASSO收敛"></a>LASSO收敛</h2><p>LASSO 通常使用坐标下降法（coordinate descent）等优化算法来求解，这些算法在一定条件下会停止迭代以保证收敛。</p><h3 id="收敛条件"><a href="#收敛条件" class="headerlink" title="收敛条件"></a>收敛条件</h3><ol><li><strong>迭代次数限制</strong>：大多数优化算法允许你设置最大迭代次数。如果达到这个次数，算法将停止，即使它还没有完全收敛。</li><li><strong>容忍度（Tolerance）</strong>：这通常指的是在连续两次迭代之间的目标函数值或系数变化的阈值。如果变化小于这个阈值，算法认为已经收敛。</li><li><strong>检查系数变化</strong>：在每次迭代后检查系数向量的变化量。如果变化量小于某个预设的容忍度，则停止迭代。</li></ol><h3 id="递增LASSO特征选择"><a href="#递增LASSO特征选择" class="headerlink" title="递增LASSO特征选择"></a>递增LASSO特征选择</h3><ol><li>初始化训练集：开始时64个样本</li><li>训练LASSO模型：使用当前训练集训练LASSO模型</li><li>检查收敛性：判断收敛条件是否满足</li><li>不收敛则增加数据：增加64个样本</li><li>重新训练</li><li>重复3-5，直至模型收敛</li><li>特征选择：模型收敛，则选择系数非0的特征</li></ol><p><strong>代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_convergence</span>(<span class="params">model, X, y, tol=<span class="number">1e-4</span></span>):</span><br><span class="line">    <span class="comment"># 简单的收敛检查,可以根据需要调整</span></span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    mse = np.mean((y - y_pred) ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> mse &lt; tol</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chech_coef_change</span>(<span class="params">lasso</span>):</span><br><span class="line">    <span class="comment"># 检查系数变化是否小于容忍度</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">all</span>(np.<span class="built_in">abs</span>(lasso.coef_ - np.zeros_like(lasso.coef_)) &lt;= tolerance)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lasso_feature_selection</span>():</span><br><span class="line">    <span class="comment"># 生成模拟数据</span></span><br><span class="line">    X, y = make_regression(n_samples=<span class="number">1000</span>, n_features=<span class="number">100</span>, noise=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始训练集</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=<span class="number">64</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化LASSO模型</span></span><br><span class="line">    lasso = Lasso(alpha=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    </span><br><span class="line">    converged = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> converged:</span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        lasso.fit(X_train, y_train)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检查收敛性</span></span><br><span class="line">        converged = check_convergence(lasso, X_train, y_train)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> converged:</span><br><span class="line">            <span class="comment"># 如果没有收敛,增加64个样本</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(X_train) + <span class="number">64</span> &lt;= <span class="built_in">len</span>(X):</span><br><span class="line">                X_new, _, y_new, _ = train_test_split(X_test, y_test, train_size=<span class="number">64</span>, random_state=<span class="number">42</span>)</span><br><span class="line">                X_train = np.vstack((X_train, X_new))</span><br><span class="line">                y_train = np.concatenate((y_train, y_new))</span><br><span class="line">                X_test = np.delete(X_test, <span class="built_in">range</span>(<span class="number">64</span>), axis=<span class="number">0</span>)</span><br><span class="line">                y_test = np.delete(y_test, <span class="built_in">range</span>(<span class="number">64</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;没有更多数据可以添加,模型未收敛&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 特征选择</span></span><br><span class="line">    selected_features = np.where(lasso.coef_ != <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> lasso, selected_features</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行算法</span></span><br><span class="line">model, selected_features = lasso_feature_selection()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选择的特征索引:&quot;</span>, selected_features)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选择的特征数量:&quot;</span>, <span class="built_in">len</span>(selected_features))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_convergence</span>(<span class="params">current_order, previous_order, overlap_length</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    检查两个排序是否在指定长度内收敛</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    current_order : array_like</span></span><br><span class="line"><span class="string">        当前迭代的特征排序</span></span><br><span class="line"><span class="string">    previous_order : array_like</span></span><br><span class="line"><span class="string">        上一次迭代的特征排序</span></span><br><span class="line"><span class="string">    overlap_length : int</span></span><br><span class="line"><span class="string">        需要保持一致的前 n 个元素的数量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    bool</span></span><br><span class="line"><span class="string">        如果前 overlap_length 个元素完全一致，返回 True；否则返回 False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 确保两个数组至少有 overlap_length 个元素</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(current_order) &lt; overlap_length <span class="keyword">or</span> <span class="built_in">len</span>(previous_order) &lt; overlap_length:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 比较前 overlap_length 个元素</span></span><br><span class="line">    <span class="keyword">return</span> np.array_equal(current_order[:overlap_length], previous_order[:overlap_length])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_nonzero_features_order</span>(<span class="params">coef</span>):</span><br><span class="line">    <span class="comment"># 获取非零特征的索引，并按照系数绝对值大小排序</span></span><br><span class="line">    nonzero_indices = np.nonzero(coef)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> nonzero_indices[np.argsort(np.<span class="built_in">abs</span>(coef[nonzero_indices]))[::-<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lasso_feature_selection</span>():</span><br><span class="line">    <span class="comment"># 生成模拟数据</span></span><br><span class="line">    X, y = make_regression(n_samples=<span class="number">10000</span>, n_features=<span class="number">100</span>, noise=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始训练集</span></span><br><span class="line">    X_train, X_pool, y_train, y_pool = train_test_split(X, y, train_size=<span class="number">64</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化LASSO模型，random_state为随机种子，</span></span><br><span class="line">    lasso = Lasso(alpha=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    converged = <span class="literal">False</span></span><br><span class="line">    previous_order = np.array([])</span><br><span class="line">    iteration = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> converged:</span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取当前非零特征的排序</span></span><br><span class="line">        current_order = get_nonzero_features_order(lasso.coef_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查收敛性</span></span><br><span class="line">        <span class="keyword">if</span> iteration &gt; <span class="number">0</span>:  <span class="comment"># 从第二次迭代开始检查</span></span><br><span class="line">            converged = check_convergence(current_order, previous_order)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> converged:</span><br><span class="line">            <span class="comment"># 如果没有收敛,增加样本</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(X_pool) &gt;= <span class="number">64</span>:</span><br><span class="line">                X_new, X_pool, y_new, y_pool = train_test_split(X_pool, y_pool, train_size=<span class="number">64</span>, random_state=<span class="number">42</span>)</span><br><span class="line">                X_train = np.vstack((X_train, X_new))</span><br><span class="line">                y_train = np.concatenate((y_train, y_new))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果剩余样本不足64个，添加所有剩余样本</span></span><br><span class="line">                X_train = np.vstack((X_train, X_pool))</span><br><span class="line">                y_train = np.concatenate((y_train, y_pool))</span><br><span class="line">                X_pool = np.array([])</span><br><span class="line">                y_pool = np.array([])</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;迭代 <span class="subst">&#123;iteration + <span class="number">1</span>&#125;</span>: 当前训练集大小: <span class="subst">&#123;<span class="built_in">len</span>(X_train)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(X_pool) == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;所有可用数据已用完，模型仍未收敛&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        previous_order = current_order</span><br><span class="line">        iteration += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">selected_features = np.nonzero(lasso.coef_)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> lasso, selected_features, iteration</span><br><span class="line"><span class="comment"># 运行算法</span></span><br><span class="line">model, selected_features, total_iterations = lasso_feature_selection()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选择的特征索引:&quot;</span>, selected_features)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;选择的特征数量:&quot;</span>, <span class="built_in">len</span>(selected_features))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;总迭代次数:&quot;</span>, total_iterations)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终非零特征排序:&quot;</span>, get_nonzero_features_order(model.coef_))</span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/1636220786/" title="LASSO">https://amostian.github.io/posts/1636220786/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag"><i class="fa fa-tags"></i> 特征工程</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3735389105/" rel="prev" title="特征缩放"><i class="fa fa-chevron-left"></i> 特征缩放</a></div><div class="post-nav-item"><a href="/posts/4083153812/" rel="next" title="并行强化学习">并行强化学习 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-text">背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LASSO%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E6%98%AF"><span class="nav-text">LASSO优化目标是</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-text">坐标下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E8%A7%92%E5%9B%9E%E5%BD%92"><span class="nav-text">最小角回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-text">实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-text">分类特征处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E6%98%AF%E5%90%A6%E6%BB%A1%E8%B6%B3%E4%B8%80%E4%BA%9B%E5%81%87%E8%AE%BE"><span class="nav-text">验证数据是否满足一些假设</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A4%E6%96%AD%E5%85%B1%E7%BA%BF%E6%80%A7"><span class="nav-text">判断共线性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A4%E6%96%AD%E6%AD%A3%E6%80%81%E6%80%A7"><span class="nav-text">判断正态性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1"><span class="nav-text">建模</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%86%E6%9D%83%E9%87%8D%E8%A1%A8%E7%9A%84%E5%80%BC%E7%BB%84%E5%90%88%E4%B8%BA%E4%B8%80%E4%B8%AADataFrame"><span class="nav-text">将权重表的值组合为一个DataFrame</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E5%AE%9E%E4%BE%8B%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%80%BC%E3%80%81%E9%A2%84%E6%B5%8B%E5%80%BC%E3%80%81%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4%EF%BC%8C%E7%94%BB%E5%9B%BE"><span class="nav-text">获取实例的真实值、预测值、置信区间，画图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A"><span class="nav-text">模型解释</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8B%9F%E5%90%88%E4%BC%98%E5%BA%A6"><span class="nav-text">拟合优度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-text">特征重要性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8D%E7%94%A8%E6%9D%83%E9%87%8D%E4%BB%A3%E8%A1%A8%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E5%8E%9F%E5%9B%A0"><span class="nav-text">不用权重代表特征重要性原因</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E6%9D%83%E9%87%8D%E5%92%8C%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4%E7%94%BB%E6%9D%83%E9%87%8D%E5%9B%BE"><span class="nav-text">根据权重和置信区间画权重图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%88%E5%BA%94%E5%9B%BE"><span class="nav-text">效应图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lasso%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-text">lasso特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%90%E8%A1%8Classo"><span class="nav-text">运行lasso</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A5%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%E4%B8%BAx%E8%BD%B4%EF%BC%8C%E7%89%B9%E5%BE%81%E6%95%B0%E9%87%8F%E3%80%81%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E4%B8%BA%E5%8F%8Cy%E8%BD%B4%E7%94%BB%E5%9B%BE"><span class="nav-text">以正则化参数为x轴，特征数量、评估指标为双y轴画图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E7%85%A7%E5%9B%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%95%88%E6%9E%9C-%E4%BF%9D%E7%95%99%E7%89%B9%E5%BE%81%E6%95%B0%E9%87%8F%E5%9D%87%E5%90%88%E9%80%82%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%E5%80%BC"><span class="nav-text">参照图，找到模型预测效果+保留特征数量均合适的正则化参数值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%BB%BA%E6%A8%A1%E8%A7%A3%E9%87%8A"><span class="nav-text">线性回归+建模解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7"><span class="nav-text">波士顿房价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F"><span class="nav-text">模拟</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LASSO%E6%94%B6%E6%95%9B"><span class="nav-text">LASSO收敛</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%9D%A1%E4%BB%B6"><span class="nav-text">收敛条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%92%E5%A2%9ELASSO%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-text">递增LASSO特征选择</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">224</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">66</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">83</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1201.1k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">48:40</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>