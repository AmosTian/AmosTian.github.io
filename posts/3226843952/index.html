<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="0.动手学深度学习"><meta property="og:url" content="https://amostian.github.io/posts/3226843952/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240314142145184.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/book-org.svg"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/fcb07912233b53860b2156241389eaec.jpg"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240314151817895.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240314151955987.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240314152625358.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230915223624368.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230922005559329.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230915224854802.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230915225207766.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918094051855.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918094143437.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918094618087.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918100347635.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918101221764.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918101354284.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20230918102040061.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240314205806310.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315000715558.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315001042618.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315092108161.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315092318002.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315212911205.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315220228753.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315220948418.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315221129124.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315221821281.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240315222831005.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316100833584.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316101117532.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316102930111.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316103935908.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316103847226.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316161448871.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316161740687.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20231007173446670.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316171423804.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316173835569.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316173918937.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240320223639702.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240320223715559.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20231007164548988.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20231007164626492.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20231007165242804.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20231007165350176.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240316204942597.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240317233728332.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240317233809540.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240317234518427.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240317095709565.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240317095959100.png"><meta property="og:image" content="https://amostian.github.io/posts/3226843952/image-20240317100942327.png"><meta property="article:published_time" content="2024-03-14T01:57:03.000Z"><meta property="article:modified_time" content="2024-05-05T02:36:19.825Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/3226843952/image-20240314142145184.png"><link rel="canonical" href="https://amostian.github.io/posts/3226843952/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>0.动手学深度学习 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">66</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">83</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">216</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/3226843952/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">0.动手学深度学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-03-14 09:57:03" itemprop="dateCreated datePublished" datetime="2024-03-14T09:57:03+08:00">2024-03-14</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-05-05 10:36:19" itemprop="dateModified" datetime="2024-05-05T10:36:19+08:00">2024-05-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">11.8k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>27 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-学习内容"><a href="#1-1-学习内容" class="headerlink" title="1.1 学习内容"></a>1.1 学习内容</h2><p>What：深度学习有哪些技术</p><p>How：如何实现这些技术和调参</p><p>Why：有效性原因，直觉、数学上解释</p><h2 id="1-2-深度学习概述"><a href="#1-2-深度学习概述" class="headerlink" title="1.2 深度学习概述"></a>1.2 深度学习概述</h2><h3 id="1-2-1-学科范畴"><a href="#1-2-1-学科范畴" class="headerlink" title="1.2.1 学科范畴"></a>1.2.1 学科范畴</h3><p><img src="/posts/3226843952/image-20240314142145184.png" alt="image-20240314142145184"></p><p>y轴为任务，x轴为解决任务可用的模型</p><ul><li>感知：所见即所得</li><li>推理：基于感知，推导未来可能发生的事情</li><li>知识：根据看到的东西、现象形成知识</li><li>规划：基于知识，进行长远规划</li></ul><p>自然语言处理：比较成熟的是机器翻译</p><ul><li>语言首先是符号，后续发展为用概率模型、ML模型去解释</li></ul><p>计算机视觉：基于图片内容可以做一些推理</p><ul><li>图像中都是像素点，很难用符号解释，所以CV大部分用概率模型或ML解释</li></ul><p>深度学习是机器学习中的一种</p><h3 id="1-2-2-课程设置"><a href="#1-2-2-课程设置" class="headerlink" title="1.2.2 课程设置"></a>1.2.2 课程设置</h3><p>深度学习基础</p><p>CNN是一个空间上的神经网络，RNN是一个时间上的神经网络</p><p>注意力机制：主要应用于NLP</p><p>优化算法：SGD、Momentum、Adam</p><p>高性能计算</p><ul><li>并行</li><li>多GPU</li><li>分布式</li></ul><p><img src="/posts/3226843952/book-org.svg" alt="结构"></p><p>2章：深度强化学习所需前置</p><ul><li>如何存储和处理数据</li><li>如何进行数值计算</li></ul><p>3、4章：深度学习基本概念和技术</p><p>5-10章：现代深度学习技术</p><ul><li>5：计算方式</li><li>6,7：CNN</li><li>8,9：RNN</li><li>10：注意力机制</li></ul><p>11章：用于深度学习模型的几种常用优化算法</p><p>12章：计算性能影响因素</p><p>14,15章：预训练语言表示模型并将其应用于NLP任务</p><h3 id="1-2-3-应用"><a href="#1-2-3-应用" class="headerlink" title="1.2.3 应用"></a>1.2.3 应用</h3><blockquote><p>符号学结合机器学习——结合图神经网络，可以做一些比较复杂的推理</p></blockquote><h4 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h4><h5 id="图片分类"><a href="#图片分类" class="headerlink" title="图片分类"></a>图片分类</h5><p><a target="_blank" rel="noopener" href="http://www.image-net.org">http://www.image-net.org</a> 是一个图片数据集，包括1000类物体的1000000张图片</p><p><img src="/posts/3226843952/fcb07912233b53860b2156241389eaec.jpg" alt="Image for article titled The data that transformed AI research—and possibly the world"></p><p>从2012年开始，深度学习开始应用于图像识别领域，到2017年，基本上所有的团队都能做到5%以内的错误率，精确率达到了人类在图片分类任务上的水平</p><h5 id="目标检测和分割"><a href="#目标检测和分割" class="headerlink" title="目标检测和分割"></a>目标检测和分割</h5><p>目标检测：图片中的物体是什么，出现在图片的什么位置</p><p>图像分割：图片中的每个像素属于那个目标</p><h5 id="样式迁移"><a href="#样式迁移" class="headerlink" title="样式迁移"></a>样式迁移</h5><p>AI作图，给定图片与模式，能够换成指定风格的图片</p><h5 id="人脸合成"><a href="#人脸合成" class="headerlink" title="人脸合成"></a>人脸合成</h5><h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><h5 id="文字生成图片"><a href="#文字生成图片" class="headerlink" title="文字生成图片"></a>文字生成图片</h5><h5 id="文本生成模型"><a href="#文本生成模型" class="headerlink" title="文本生成模型"></a>文本生成模型</h5><p>GPT</p><h4 id="计算机视觉应用于无人驾驶"><a href="#计算机视觉应用于无人驾驶" class="headerlink" title="计算机视觉应用于无人驾驶"></a>计算机视觉应用于无人驾驶</h4><h4 id="广告点击案例"><a href="#广告点击案例" class="headerlink" title="广告点击案例"></a>广告点击案例</h4><p>任务：根据搜索框输入的文本，推荐商品</p><p><img src="/posts/3226843952/image-20240314151817895.png" alt="image-20240314151817895"></p><ol><li>输入关键词，找到相关广告</li><li>机器学习模型，预测看到一个广告后，人点击各个广告的点击率</li><li>根据点击率与用户点击后广告主竞价对商品排序</li></ol><p>点击率的预测与预测模型的训练：</p><p><img src="/posts/3226843952/image-20240314151955987.png" alt="image-20240314151955987"></p><ul><li>预测：提取一个广告的特征，将这个广告输入给模型，模型会给出当前用户点击这个广告的概率</li><li>模型训练：当前用户对所有广告的点击数据，经过数据清洗后，广告特征与点击结果会作为训练数据，基于这个数据集，会训练出预测模型</li></ul><p><img src="/posts/3226843952/image-20240314152625358.png" alt="image-20240314152625358"></p><p>领域专家：关注机器模型会对产品产生什么的影响。模型控制广告展现，这些展现数据会被模型进一步学习，领域专家了解用户行为，模型需要对应用数据做哪些拟合。</p><ul><li>提出需求：相当于指标制定者，某个任务中，达成哪些指标就认为是好的结果</li></ul><p>数据科学家：将原始数据变为机器学习能理解的数据，然后训练模型</p><ul><li>产品：将领域专家的问题、需求翻译为机器学习可以实现的一些任务，得到不错的模型</li></ul><p>AI专家：关注某几个点，进行进一步提升</p><h3 id="1-2-4-模型可解释性"><a href="#1-2-4-模型可解释性" class="headerlink" title="1.2.4 模型可解释性"></a>1.2.4 模型可解释性</h3><p>模型的有效性可做一些解释：模型可应用于什么场景，什么样的模型会考虑空间信息，什么样的模型会考虑时间信息，什么样的模型泛化性好一些</p><p>模型的可解释性，如：人是否能理解模型，用数学解释模型什么时候工作，什么时候不工作，什么时候出现偏差这方面目前无法解决。</p><h2 id="1-3-表示学习-特征处理"><a href="#1-3-表示学习-特征处理" class="headerlink" title="1.3  表示学习(特征处理)"></a>1.3 表示学习(特征处理)</h2><blockquote><p>样本特征 $\iff$ 表示</p></blockquote><p>机器学习流程：</p><p><img src="/posts/3226843952/image-20230915223624368.png" alt="image-20230915223624368"></p><ul><li><p>特征工程(feature engineering)——人工处理</p><p>数据预处理：对于数据缺失多的特征，弃用</p><p>特征提取：提取有效特征，隐含特征也需提取</p><p>特征转换：某些冗余特征及有相关性特征，弃用或组合，形成更有效特征</p></li><li><p><strong>浅层学习</strong>：不涉及特征学习，其特征主要靠人工经验或特征转换的方法抽取</p></li></ul><h3 id="1-3-1-传统特征学习"><a href="#1-3-1-传统特征学习" class="headerlink" title="1.3.1 传统特征学习"></a>1.3.1 传统特征学习</h3><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><blockquote><p><strong>特征选择</strong> 是选取原始特征集合的一个有效子集，使得基于这个特征子集训练出来的模型准确率最高</p></blockquote><ul><li>保留有用特征，移除冗余或无关的特征</li></ul><p><strong>子集搜索</strong>：假设原始特征数为 $D$ ，则共有 $2^D$ 个候选子集。特征选择的目标就是选择一个最优的候选子集</p><p>常用贪心搜索策略：由空集合开始，每一轮添加该轮最优的特征，称为<strong>前向搜索</strong>（Forward Search）；</p><p>从原始特征集合开始，每次删除最无用的特征，称为<strong>反向搜索</strong>（Backward Search）．</p><h5 id="过滤式"><a href="#过滤式" class="headerlink" title="过滤式"></a>过滤式</h5><blockquote><p>不依赖具体机器学习模型的特征选择方法</p></blockquote><p>每次增加最有信息量的特征，或删除最没有信息量的特征</p><p>特征的信息量可以通过<strong>信息增益</strong>（Information Gain）来衡量，即引入特征后条件分布 $p_\theta(y\vert x)$ 的不确定性(熵)的减少程度</p><h5 id="包裹式"><a href="#包裹式" class="headerlink" title="包裹式"></a>包裹式</h5><blockquote><p>使用后续机器学习模型的准确率作为评价来选择一个特征子集的方法</p></blockquote><p>每次增加对后续机器学习模型最有用的特征，或删除对后续机器学习任务最无用的特征．</p><h5 id="L-1-正则化"><a href="#L-1-正则化" class="headerlink" title="$L_1$ 正则化"></a>$L_1$ 正则化</h5><p>$L_1$ 正则化会导致稀疏特征，间接实现了特征选择</p><h4 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h4><blockquote><p>构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示</p></blockquote><h5 id="监督的特征学习"><a href="#监督的特征学习" class="headerlink" title="监督的特征学习"></a>监督的特征学习</h5><blockquote><p>抽取对一个特定的预测任务最有用的特征</p></blockquote><ul><li>线性判别分析</li></ul><h5 id="无监督的特征学习"><a href="#无监督的特征学习" class="headerlink" title="无监督的特征学习"></a>无监督的特征学习</h5><blockquote><p>和具体任务无关，其目标通常是减少冗余信息和噪声</p></blockquote><ul><li>主成分分析PCA</li><li>自编码器</li></ul><p><img src="/posts/3226843952/image-20230922005559329.png" alt="image-20230922005559329"></p><h4 id="特征工程作用"><a href="#特征工程作用" class="headerlink" title="特征工程作用"></a>特征工程作用</h4><p>用较少的特征表示原始特征中的大部分信息，去掉噪声信息，并进而提高计算效率和减小维度灾难</p><ul><li>尤其是对于没有正则化的模型</li></ul><h3 id="1-3-2-语义鸿沟"><a href="#1-3-2-语义鸿沟" class="headerlink" title="1.3.2 语义鸿沟"></a>1.3.2 语义鸿沟</h3><p><strong>底层特征</strong> 与 <strong>高层语义</strong></p><blockquote><p>人们对文本、图像的理解（高层语义）无法从字符串或者图像的底层特征（底层特征）直接获得</p></blockquote><p><strong>好的表示</strong></p><ul><li>应该具有很强的表示能力——同样的一个空间应该可表示更多的语义</li><li>应该使后续的学习任务变得简单</li><li>应该具有一般性，是任务或领域独立的</li></ul><h3 id="1-3-3-表示方式"><a href="#1-3-3-表示方式" class="headerlink" title="1.3.3 表示方式"></a>1.3.3 表示方式</h3><p><strong>数据表示是机器学习的核心问题</strong></p><p>局部表示</p><blockquote><p>一个语义由一个量表示</p></blockquote><ul><li><p>离散表示</p><p>One-Hot向量</p></li><li><p>符号表示</p></li></ul><p>分布式表示</p><blockquote><p>一个语义由多个量共同表示</p></blockquote><ul><li>压缩、低维、稠密向量</li></ul><p><img src="/posts/3226843952/image-20230915224854802.png" alt="image-20230915224854802"></p><p>理解：</p><p>$k$ 个维度</p><ul><li><p>局部表示：单个维度代表语义</p><p>$k$ 个语义</p></li><li><p>分布式表示：所有维度加起来代表语义</p><p>$2^k$ 个语义</p></li></ul><p><img src="/posts/3226843952/image-20230915225207766.png" alt="image-20230915225207766"></p><h4 id="关联"><a href="#关联" class="headerlink" title="关联"></a>关联</h4><p>局部表示：配合知识库、规则使用</p><p>分布式表示：嵌入：压缩、低维、稠密</p><p>通常情况：将局部表示映射为分布式表示</p><ul><li><p>低维</p><p><img src="/posts/3226843952/image-20230918094051855.png" alt="image-20230918094051855"></p></li><li><p>稠密</p><p><img src="/posts/3226843952/image-20230918094143437.png" alt="image-20230918094143437"></p><p>相似语义靠的近，不同语义间隔大，便于后续量化</p></li></ul><h3 id="1-3-4-表示学习"><a href="#1-3-4-表示学习" class="headerlink" title="1.3.4 表示学习"></a>1.3.4 表示学习</h3><p>通过构建模型，让其自动学习好的特征（底层特征、中层特征、高层特征），从而最终提升预测或识别的准确性</p><p><img src="/posts/3226843952/image-20230918094618087.png" alt="image-20230918094618087"></p><h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p><strong>传统特征提取</strong></p><ul><li><p>线性投影（子空间）</p><p>PCA、LDA</p></li><li><p>非线性嵌入</p><p>LLE、Isomap、谱方法</p></li><li><p>自编码</p></li></ul><p>区别：</p><ul><li><p>特征提取：基于任务或 <strong>先验</strong> 去除无用特征</p><p>自设标准，对任务影响是好是坏并不确定，实质上也是一种特征工程</p></li><li><p>表示学习：通过深度模型学习高层语义特征</p><p>难点：没有明确目标</p></li></ul><h4 id="深度学习与表示学习"><a href="#深度学习与表示学习" class="headerlink" title="深度学习与表示学习"></a>深度学习与表示学习</h4><p><strong>一个好的表示学习策略必须具备一定深度</strong> ，指原始数据进行非线性特征转换的次数</p><ul><li><p>特征重用</p><p>指数级的表示能力，底层特征可以被高层特征重复利用</p></li><li><p>抽象表示与不变性</p><p>抽象表示需要多步构造</p><p><img src="/posts/3226843952/image-20230918100347635.png" alt="image-20230918100347635"></p></li></ul><h4 id="深度学习概念"><a href="#深度学习概念" class="headerlink" title="深度学习概念"></a>深度学习概念</h4><script type="math/tex;mode=display">深度学习=表示学习+\underbrace{决策(预测)学习}_{浅层学习}</script><p><img src="/posts/3226843952/image-20230918101221764.png" alt="image-20230918101221764"></p><p>核心问题是：<strong>贡献度分配问题</strong></p><p><img src="/posts/3226843952/image-20230918101354284.png" alt="image-20230918101354284"></p><p>即一个系统中不同的组件（component）或其参数对最终系统输出结果的贡献或影响</p><ul><li><strong>强化学习</strong> 可以通过反馈机制，获取当前步的决策对最终结果影响的概率大小，进而判断当前步的贡献度大小</li></ul><p>对于一般的深度学习，解决贡献度分配问题用 <strong>神经网络</strong></p><p><img src="/posts/3226843952/image-20230918102040061.png" alt="image-20230918102040061"></p><h5 id="端到端"><a href="#端到端" class="headerlink" title="端到端"></a>端到端</h5><p>整个学习过程中，没有人为干预</p><blockquote><p>指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标</p></blockquote><p><strong>传统机器学习方法</strong> 需要将一个任务的输入和输出之间人为地切割成很多子模块（或多个阶段），每个子模块分开学习</p><p>问题：</p><ul><li>每一个模块都需要单独优化，并且其优化目标和任务总体目标并不能保证一致</li><li>错误传播，即前一步的错误会对后续的模型造成很大的影响．这样就增加了机器学习方法在实际应用中的难度．</li></ul><h5 id="深度学习数学表示"><a href="#深度学习数学表示" class="headerlink" title="深度学习数学表示"></a>深度学习数学表示</h5><script type="math/tex;mode=display">\begin{array}{c|l}
y=f(x)&浅层学习\\
\Downarrow\\
y=f^2(f^1(x))\\
\Downarrow\\
\vdots\\
\Downarrow\\
y=f^{K}(f^{K-1}\cdots(f^1(x)))&深度学习\\
\end{array}</script><p>$f(x)$ 为非线性函数，不一定连续</p><ul><li>线性模型多层嵌套仍是线性，性能不会提升</li></ul><p>当 $f^1(x)$ 连续时，如 $f^l(x)=\sigma(W^{(l)}f^{l-1}(x))$ ，这个复合函数称为神经网络</p><h1 id="2-绪论"><a href="#2-绪论" class="headerlink" title="2. 绪论"></a>2. 绪论</h1><h2 id="2-0-资源"><a href="#2-0-资源" class="headerlink" title="2.0 资源"></a>2.0 资源</h2><p><a target="_blank" rel="noopener" href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497">视频</a></p><p><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_preface/index.html">第一版</a></p><p><a target="_blank" rel="noopener" href="https://discuss.d2l.ai/t/topic/2083">论坛</a></p><p><a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2/">课程主页</a> ：PPT、视频、PPT jupyter、<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_preface/index.html">教材</a></p><h2 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV18K411w7Vs/?spm_id_from=333.999.0.0">李沐-Windows安装</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ubuntu</span></span><br><span class="line">1. 升级安装工具</span><br><span class="line">sudo apt update</span><br><span class="line"></span><br><span class="line">2. 安装编译工具,解压工具</span><br><span class="line">sudo apt install build-essential zip</span><br><span class="line"></span><br><span class="line">3. 安装python</span><br><span class="line">sudo apt install python-3.8c </span><br><span class="line"></span><br><span class="line">4. 下载miniconda安装包</span><br><span class="line">wget [miniconda的安装包链接]</span><br><span class="line"></span><br><span class="line">5. 安装miniconda </span><br><span class="line">bash Miniconda-xx.sh</span><br><span class="line"></span><br><span class="line">6. bash 进入conda环境</span><br><span class="line"></span><br><span class="line">7. 创建环境</span><br><span class="line">conda create --name d2l python=3.9 -y</span><br><span class="line"></span><br><span class="line">8. 安装相关依赖 jupyter d2l torch torchvision</span><br><span class="line">pip install torch==1.12.0 torchvision==0.13.0 d2l==0.17.6</span><br><span class="line">conda install jupyter </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可能存在的问题，toolkit版本为11，改一下</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">CUDA 11.6</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">pip install torch==1.12.0+cu123 torchvision==0.13.0+cu123 torchaudio==0.12.0 --c</span></span><br><span class="line"></span><br><span class="line">9. 下载jupyter记事本，项目文件，解压pytorch文件夹</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装jupyter notebook</span></span><br><span class="line">conda install -y jupyter</span><br><span class="line">conda install ipykernel</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看有哪些环境</span></span><br><span class="line">jupyter kernelspec list</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将conda环境添加到jupyter的内核中</span></span><br><span class="line">python -m ipykernel install --name conda_env_name</span><br><span class="line">python -m ipykernel install --user --name pytorch171 --display-name pytorch171</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除指定kernel环境</span></span><br><span class="line">jupyter kernelspec uninstall pytorch1.8_python3.6</span><br><span class="line"></span><br><span class="line">10. 将远端地址映射到本地</span><br><span class="line">ssh -L8888:localhost:8888 [远端 用户名@ip]</span><br><span class="line"></span><br><span class="line">11. 启动jupyter</span><br><span class="line">jupyter notebook</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">###记得看内核正不正确</span></span></span><br></pre></td></tr></table></figure><p><img src="/posts/3226843952/image-20240314205806310.png" alt="image-20240314205806310"></p><p>将pytorch安装到jupyter</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install ipykernel</span><br><span class="line">python -m ipykernel install --user --name=[conda_name]</span><br></pre></td></tr></table></figure><h3 id="2-1-1-查阅文档"><a href="#2-1-1-查阅文档" class="headerlink" title="2.1.1 查阅文档"></a>2.1.1 查阅文档</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span>()<span class="comment"># 查找该模块可以调用的函数和类</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch.distributions))</span><br></pre></td></tr></table></figure><p>以 <code>__</code> 开始和结束的函数为Python中的特殊对象</p><p>以 <code>_</code> 开始和结束的函数为内部函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(torch.ones)</span><br></pre></td></tr></table></figure><p>给出函数的详细声明</p><ul><li>参数列表</li><li>函数描述</li><li>普通参数 Args</li><li>关键字参数 Keyword arguments</li><li>示例：Example</li></ul><h2 id="2-2-数据操作"><a href="#2-2-数据操作" class="headerlink" title="2.2 数据操作"></a>2.2 数据操作</h2><h3 id="2-2-1-数据类型"><a href="#2-2-1-数据类型" class="headerlink" title="2.2.1 数据类型"></a>2.2.1 数据类型</h3><p><img src="/posts/3226843952/image-20240315000715558.png" alt="image-20240315000715558"></p><p><img src="/posts/3226843952/image-20240315001042618.png" alt="image-20240315001042618"></p><p>标量：一个特征值</p><p>一维向量：一个样本的多维特征值</p><p>二维向量：多个样本组成的特征矩阵、一张图片的某个通道</p><p>三维向量：一张RGB图片（$宽\times 高\times 通道$）</p><p>四维向量：一个RGB图片批量，一次卷积运算会计算很多张图片，称为一个批量</p><p>五维向量：$时间\times batch$</p><h3 id="2-2-2-tensor操作与运算"><a href="#2-2-2-tensor操作与运算" class="headerlink" title="2.2.2 tensor操作与运算"></a>2.2.2 tensor操作与运算</h3><blockquote><p>在pytorch中，<code>tensor</code> 表示多维数组，与 <code>numpy</code> 的用法基本相同</p></blockquote><h4 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h4><ul><li><p><code>arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor</code></p><p>创建从 $[start，end)$ 的数组，每隔 step-1 取一个数字</p><p>dtype为数据类型：<code>torch.float32</code> ,…</p><p>layout为内存布局，可选<code>torch.stried</code>或<code>torch.sparse_coo</code>。分别对应顺序储存、离散储存。</p><p>device为CPU或CUDA</p></li><li><p><code>torch.zero((a,b,c))</code> ：$(a,b,c)$ 为一个tuple，在torch中封装为 Size 类型的对象，表示张量的形状；全0的张量</p></li><li><p><code>torch.ones((a,b,c))</code> ：全1的张量</p></li><li><p>随机数：</p><ul><li><p><code>torch.rand(sizes,out=None)</code> ：构造均匀分布张量的方法</p><ul><li><code>torch.rand(2,3,4)</code></li></ul></li><li><p><code>torch.randn(sizes,out=None)</code> ：构造标准正态分布张量的方法</p></li><li><p><code>torch.randint(low=0, high, sizes, out=None)</code> ：构造区间分布张量</p><ul><li><code>torch.randint(1, 10, (4, 3))</code></li></ul></li><li><p><code>torch.randperm(n, out=None, dtype=torch.int64)</code> ：对张量序号进行随机排序的函数，并根据生成的随机序列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor_4 = torch.Tensor(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor_4</span><br><span class="line">tensor([[-<span class="number">6.4468e+37</span>,  <span class="number">1.7825e-42</span>,  <span class="number">0.0000e+00</span>],</span><br><span class="line">        [ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>],</span><br><span class="line">        [ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>],</span><br><span class="line">        [ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个0~3的随机行序号</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>idx = torch.randperm(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="string">&quot;\n生成的随机序号\n&quot;</span>, idx)</span><br><span class="line"></span><br><span class="line">生成的随机序号</span><br><span class="line"> tensor([<span class="number">3</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 在行的方向上，对tensor_4进行随机排序，并输出结果</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="string">&quot;\n随机排序后的张量\n&quot;</span>, tensor_4[idx])</span><br><span class="line"></span><br><span class="line">随机排序后的张量</span><br><span class="line"> tensor([[ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>],</span><br><span class="line">        [ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>],</span><br><span class="line">        [-<span class="number">6.4468e+37</span>,  <span class="number">1.7825e-42</span>,  <span class="number">0.0000e+00</span>],</span><br><span class="line">        [ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>]])</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>torch.tensor([list])</code> ：通过python的列表类型，转换为张量</p><p>如：<code>torch.tensor([[1,2],[3,4],[5,6]])</code> 用列表创建 $3\times 2$ 的二维张量</p></li></ul><p><code>.numel()</code> ：number of element 张量中元素的个数</p><p>通过 <code>.shape</code> 访问张量对象的尺寸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">1</span>,<span class="number">12</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor([ 1,  3,  5,  7,  9, 11])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.numel())</span><br><span class="line"><span class="comment"># 6</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([6])</span></span><br></pre></td></tr></table></figure><h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><ul><li><code>.reshape(a,b,c)</code> ：改变张量的形状而不改变元素数量和元素值，将张量变为 $a\times b\times c$ 的张量；维度为c的一维向量，有b个一维向量，$b\times c$ 的矩阵有 $a$ 个</li><li>张量连接：<code>torch.cat((x,y),dim=0)</code> ，<code>torch.cat((x,y),dim=1)</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensor([[ 1,  3],</span></span><br><span class="line"><span class="comment">#        [ 5,  7],</span></span><br><span class="line"><span class="comment">#        [ 9, 11]])</span></span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">12</span>,dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>)) </span><br><span class="line">y = torch.tensor([[<span class="number">2.0</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.cat((x,y),dim=<span class="number">0</span>)) <span class="comment">#行拼接</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat((x,y),dim=<span class="number">1</span>)) <span class="comment">#列拼接</span></span><br><span class="line"><span class="comment"># tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class="line"><span class="comment">#         [ 4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#         [ 8.,  9., 10., 11.],</span></span><br><span class="line"><span class="comment">#         [ 2.,  1.,  4.,  3.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="comment">#         [ 4.,  3.,  2.,  1.]])</span></span><br><span class="line"><span class="comment"># tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span></span><br><span class="line"><span class="comment">#         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="comment">#         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])</span></span><br></pre></td></tr></table></figure><h4 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h4><ul><li><p>算术运算：<code>+ - * / **</code> 都是逐元素运算</p></li><li><p>关系运算：<code>==</code> <code>&gt;</code> <code>&lt;</code> <code>&gt;=</code> <code>&lt;=</code> 逐元素比较</p></li><li>所有元素求和：<code>.sum()</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>]) <span class="comment"># 1.0创建浮点数组</span></span><br><span class="line">y = torch.tensor([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]) </span><br><span class="line"><span class="built_in">print</span>(x*y)</span><br><span class="line"><span class="built_in">print</span>(x**y)</span><br><span class="line"><span class="comment"># tensor([ 5.,  8., 12., 16.])</span></span><br><span class="line"><span class="comment"># tensor([ 1., 16., 64., 64.])</span></span><br><span class="line"></span><br><span class="line">x == y</span><br><span class="line"><span class="comment"># tensor([[False,  True, False,  True],</span></span><br><span class="line"><span class="comment">#         [False, False, False, False],</span></span><br><span class="line"><span class="comment">#         [False, False, False, False]])</span></span><br><span class="line"></span><br><span class="line">x.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># tensor(66.) # 输出一个标量值</span></span><br></pre></td></tr></table></figure><p><strong>广播机制</strong> ：维度(秩)相同，尺寸不同的张量间算术运算，会进行广播</p><ul><li><p>同秩同尺寸最好</p></li><li><p>同秩不同尺寸，一定会广播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">同秩异尺寸</span><br><span class="line">a = torch.arange(<span class="number">2</span>).reshape((<span class="number">3</span>,<span class="number">1</span>)) <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line">b = torch.arange(<span class="number">6</span>).reshape((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># tensor([[0, 1],</span></span><br><span class="line"><span class="comment">#         [2, 3],</span></span><br><span class="line"><span class="comment">#         [4, 5]])</span></span><br><span class="line">a+b</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(a,b)</span><br><span class="line"><span class="built_in">print</span>(a+b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0],</span></span><br><span class="line"><span class="comment">#         [1],</span></span><br><span class="line"><span class="comment">#         [2]]) tensor([[0, 1]])</span></span><br><span class="line"><span class="comment"># tensor([[0, 1],</span></span><br><span class="line"><span class="comment">#         [1, 2],</span></span><br><span class="line"><span class="comment">#         [2, 3]])</span></span><br></pre></td></tr></table></figure></li><li><p>异秩异尺寸：高秩张量降一维可对齐则可广播运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">b降一维是尺寸为<span class="number">2</span>的向量，a是尺寸为<span class="number">3</span>的向量。此时，由于无法对齐，无法运算 </span><br><span class="line">a = torch.arange(<span class="number">3</span>)<span class="comment">#tensor([0, 1, 2]) torch.Size([3])</span></span><br><span class="line">b = torch.arange(<span class="number">6</span>).reshape((<span class="number">3</span>,<span class="number">2</span>)) <span class="comment"># torch.Size([3, 2])</span></span><br><span class="line"><span class="comment"># tensor([[0, 1],</span></span><br><span class="line"><span class="comment">#         [2, 3],</span></span><br><span class="line"><span class="comment">#         [4, 5]])</span></span><br><span class="line"></span><br><span class="line">b降<span class="number">1</span>维是尺寸为<span class="number">2</span>的向量，a是尺寸为<span class="number">2</span>的向量，可对齐，所以可运算</span><br><span class="line">a = torch.arange(<span class="number">2</span>)	</span><br><span class="line"><span class="comment">#tensor([0, 1])</span></span><br><span class="line">b = torch.arange(<span class="number">6</span>).reshape((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># tensor([[0, 2],</span></span><br><span class="line"><span class="comment">#         [2, 4],</span></span><br><span class="line"><span class="comment">#         [4, 6]])</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h4><p><img src="/posts/3226843952/image-20240315092108161.png" alt="image-20240315092108161"></p><p>一个元素 <code>x[1,2]</code> ：行索引为1，列索引为2，第二行第一列</p><p>一行 <code>x[1,:]</code> <code>x[1,]</code> ：行索引为1，列索引为全部</p><p>一列 <code>x[:,2]</code>：行索引为全部，列索引为2</p><p><img src="/posts/3226843952/image-20240315092318002.png" alt="image-20240315092318002"></p><p>访问子区域 <code>x[1:3,1:]</code> ：行索引为1-2，列索引为1到全部，步长默认为1</p><p>访问子区域 <code>x[::3,::2]</code> ：行步长为3，表示取每3行的第一行；列步长为2，表示取每2列的第一列</p><hr><p><code>[1,2]</code> 表示访问行索引为1，列索引为2的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b[<span class="number">1</span>,<span class="number">2</span>]=<span class="number">111</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[  0,   1,   2,   3],</span></span><br><span class="line"><span class="comment">#         [  4,   5, 111,   7],</span></span><br><span class="line"><span class="comment">#         [  8,   9,  10,  11]])</span></span><br></pre></td></tr></table></figure><p>用 <code>[-1]</code> 选择最后一个元素，用 <code>[1:3]</code> 选择索引为1和2的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">b = torch.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b[-<span class="number">1</span>])<span class="comment">#3*4矩阵的最后一个元素为 b[2]</span></span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">1</span>:<span class="number">3</span>,]) <span class="comment">#第2行和第3行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([ 8,  9, 10, 11])</span></span><br><span class="line"><span class="comment">#tensor([[ 4,  5,  6,  7],</span></span><br><span class="line"><span class="comment">#        [ 8,  9, 10, 11]])</span></span><br></pre></td></tr></table></figure><p>同样，可批量读则可批量写</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b[<span class="number">1</span>:<span class="number">3</span>,] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[ 0,  1,  2,  3],</span></span><br><span class="line"><span class="comment">#         [12, 12, 12, 12],</span></span><br><span class="line"><span class="comment">#         [12, 12, 12, 12]])</span></span><br></pre></td></tr></table></figure><h4 id="避免内存重分配"><a href="#避免内存重分配" class="headerlink" title="避免内存重分配"></a>避免内存重分配</h4><p>执行一些操作会为结果分配新的内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y = X+Y</span><br><span class="line"><span class="built_in">id</span>(Y) == before <span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>执行原地操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">b = torch.arange(<span class="number">6</span>,<span class="number">12</span>,<span class="number">0.5</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">c = torch.zeros_like(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(c)&#x27;</span>,<span class="built_in">id</span>(c))</span><br><span class="line">c[:] = a + b <span class="comment"># 此处变为 c = a + b 则不是原地操作</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(c)&#x27;</span>,<span class="built_in">id</span>(c))</span><br></pre></td></tr></table></figure><p>实际上将c中的元素进行改写，并没有为c分配新的内存</p><p><code>X+=Y</code> 也是原地操作，将 <code>X</code> 的内容改写</p><h4 id="变量类型转换"><a href="#变量类型转换" class="headerlink" title="变量类型转换"></a>变量类型转换</h4><p>转换为numpy张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">X = a.numpy()</span><br><span class="line">Y = torch.tensor(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(X),<span class="built_in">type</span>(Y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt; &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>将大小为1的张量变为Python标量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a,a.item(),<span class="built_in">float</span>(a),<span class="built_in">int</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([3.5000]) 3.5 3.5 3</span></span><br></pre></td></tr></table></figure><h3 id="2-2-3-数据预处理"><a href="#2-2-3-数据预处理" class="headerlink" title="2.2.3 数据预处理"></a>2.2.3 数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在上层目录下创建目录</span></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;./&#x27;</span>,<span class="string">&#x27;data&#x27;</span>),exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;./&#x27;</span>,<span class="string">&#x27;data&#x27;</span>,<span class="string">&#x27;house_tiny.csv&#x27;</span>) <span class="comment">#在指定目录下创建文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向打开的文件中写入数据</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#从csv中读取文件</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从指定路径中读取为csv文件</span></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将0,1列作为输入向量，将2列作为输出向量</span></span><br><span class="line">inputs, outputs = data.iloc[:,<span class="number">0</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())<span class="comment"># 用均值填充NA元素</span></span><br><span class="line">inputs = pd.concat([inputs,data.iloc[:,<span class="number">1</span>]],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#对于inputs中的类别值或离散值，将NaN视为一个类别</span></span><br><span class="line">inputs = pd.get_dummies(inputs,dummy_na=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ndarray中的值默认为numpy对象，需要将其转换为数值才能转换为tensor</span></span><br><span class="line">inputs = inputs.astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x,y = torch.tensor(inputs.values),torch.tensor(outputs.values)</span><br><span class="line"><span class="built_in">print</span>(x, y)</span><br></pre></td></tr></table></figure><h2 id="2-3-线性代数"><a href="#2-3-线性代数" class="headerlink" title="2.3 线性代数"></a>2.3 线性代数</h2><h3 id="约定"><a href="#约定" class="headerlink" title="约定"></a>约定</h3><p>$\mathbf{x}$ 为向量</p><p>$\mathbf{X}$ 为矩阵</p><h3 id="2-3-1-标量"><a href="#2-3-1-标量" class="headerlink" title="2.3.1 标量"></a>2.3.1 标量</h3><p><strong>简单操作</strong> $c=a+b$ ，$c=a\cdot b$ ，$c=\sin a$</p><p><strong>模</strong></p><script type="math/tex;mode=display">\vert a\vert=\begin{cases}
a&,a>0\\
-a&,其他
\end{cases}\\
\vert a+b\vert\le \vert a\vert +\vert b\vert\\
\vert a\cdot b\vert =\vert a\vert \cdot \vert b\vert</script><h3 id="2-3-2-向量"><a href="#2-3-2-向量" class="headerlink" title="2.3.2 向量"></a>2.3.2 向量</h3><p><strong>简单操作</strong></p><p><img src="/posts/3226843952/image-20240315212911205.png" alt="image-20240315212911205"></p><script type="math/tex;mode=display">\begin{array}{ll}
\mathbf{c}=\mathbf{a}+\mathbf{b}&c_i=a_i+b_i\\
\mathbf{c}=a\mathbf{b}&c_i=a\cdot b_i\\
\mathbf{c}=\sin\mathbf{a}&c_i=\sin a_i
\end{array}</script><p><strong>模</strong></p><script type="math/tex;mode=display">\Vert \mathbf{a}\Vert_2=\left[\sum\limits_{i=1}^ma_i^2\right]^{\frac{1}{2}}\\
\Vert \mathbf{a}\Vert\ge 0,\forall \mathbf{a}\\
\Vert \mathbf{a}+\mathbf{b}\Vert\le \Vert \mathbf{a}\Vert+\Vert \mathbf{b}\Vert\\
\Vert \mathbf{a}\cdot \mathbf{b}\Vert=\Vert \mathbf{a}\Vert\cdot \Vert \mathbf{b}\Vert</script><p><strong>内积</strong></p><script type="math/tex;mode=display">\mathbf{a}\cdot \mathbf{b}=\mathbf{a}^T\mathbf{b}=\sum\limits_{i}a_i\cdot b_i=\Vert \mathbf{a}\Vert\Vert\mathbf{b}\Vert\cos\theta</script><ul><li>向量正交，点积为0，$\mathbf{a}\cdot \mathbf{b}=0$</li></ul><h3 id="2-3-3-矩阵"><a href="#2-3-3-矩阵" class="headerlink" title="2.3.3 矩阵"></a>2.3.3 矩阵</h3><p><strong>简单操作</strong></p><script type="math/tex;mode=display">\begin{array}{ll}
\mathbf{C}=\mathbf{A}+\mathbf{B}&C_{ij}=A_{ij}+B_{ij}\\
\mathbf{C}=\alpha\cdot \mathbf{B}&C_{ij}=\alpha B_{ij}\\
\mathbf{C}=\sin \mathbf{A}&C_{ij}=\sin A_{ij}
\end{array}</script><p><strong>乘法</strong></p><script type="math/tex;mode=display">\begin{array}{rll}
矩阵\times 向量&\mathbf{c}=\mathbf{A}\cdot \mathbf{b}&c_i=\sum\limits_{j}A_{ij}b_j\\
矩阵\times 矩阵&\mathbf{C}=\mathbf{A}\mathbf{B}&C_{ik}=\sum\limits_{j}A_{ij}B_{jk}
\end{array}</script><ul><li><p>矩阵左乘向量，相当于对向量进行放缩与旋转操作</p><p><img src="/posts/3226843952/image-20240315220228753.png" alt="image-20240315220228753"></p><p><img src="/posts/3226843952/image-20240315220948418.png" alt="image-20240315220948418"></p></li><li><p>矩阵乘矩阵</p><p><img src="/posts/3226843952/image-20240315221129124.png" alt="image-20240315221129124"></p></li></ul><p><strong>模</strong>——矩阵的模为范数</p><script type="math/tex;mode=display">\mathbf{c}=\mathbf{A}\cdot \mathbf{b}\Rightarrow \Vert \mathbf{c}\Vert=\Vert \mathbf{A}\cdot \mathbf{b}\Vert\le \Vert \mathbf{A}\Vert\cdot \Vert \mathbf{b}\Vert</script><p>矩阵范数：最小的满足上述公式的值</p><ul><li>F范数：$\Vert \mathbf{A}\Vert=\left[\sum\limits_{ij}\mathbf{A}_{ij}^2\right]^{\frac{1}{2}}$</li></ul><h4 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h4><h5 id="对称与反对称"><a href="#对称与反对称" class="headerlink" title="对称与反对称"></a>对称与反对称</h5><p><img src="/posts/3226843952/image-20240315221821281.png" alt="image-20240315221821281"></p><h5 id="正定阵"><a href="#正定阵" class="headerlink" title="正定阵"></a>正定阵</h5><script type="math/tex;mode=display">\Vert \mathbf{x}\Vert^2=\mathbf{x}^T\mathbf{x}\Rightarrow \mathbf{x}^T\mathbf{A}\mathbf{x}\ge 0</script><p>若 $\mathbf{A}$ 是正定阵，则与任意行列向量相乘都大于等于0</p><h5 id="正交"><a href="#正交" class="headerlink" title="正交"></a>正交</h5><p>所有行都相互正交</p><p>所有行都是单位长度 $\mathbf{U},\delta_{ik}=\sum\limits_{j}U_{ij}U_{jk}$</p><p>可写成 $\mathbf{U}\mathbf{U}^T=\mathbf{1}$ 对角线为1的单位矩阵</p><h5 id="置换阵"><a href="#置换阵" class="headerlink" title="置换阵"></a>置换阵</h5><p>置换阵是正交阵</p><script type="math/tex;mode=display">P_{ij}=1且 j=\pi(i)</script><h4 id="特征向量-amp-特征值"><a href="#特征向量-amp-特征值" class="headerlink" title="特征向量&amp;特征值"></a>特征向量&amp;特征值</h4><p>矩阵乘的本质是对空间进行放缩与旋转，不被矩阵改变方向的向量称为特征向量，对特征向量放缩倍数称为特征值</p><p><img src="/posts/3226843952/image-20240315222831005.png" alt="image-20240315222831005"></p><h4 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h4><h5 id="标量导数"><a href="#标量导数" class="headerlink" title="标量导数"></a>标量导数</h5><p><img src="/posts/3226843952/image-20240316100833584.png" alt="image-20240316100833584"></p><h5 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h5><blockquote><p>导数不一定存在的位置</p></blockquote><p><img src="/posts/3226843952/image-20240316101117532.png" alt="image-20240316101117532"></p><h5 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h5><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th>$x$</th><th>$\mathbf{x}_{1\times n}$</th></tr></thead><tbody><tr><td style="text-align:left">$y$</td><td>标量： $\frac{\partial y}{\partial x}$</td><td>向量：$\frac{\partial y}{\partial \mathbf{x}}_{1\times n}$</td></tr><tr><td style="text-align:left">$\mathbf{y}_{1\times m}$</td><td>向量：$\frac{\partial \mathbf{y}}{\partial x}_{1\times m}$</td><td>矩阵：$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}_{m\times n}$</td></tr></tbody></table></div><script type="math/tex;mode=display">\mathbf{x}=[x_{1},x_2,\cdots,x_n]\Rightarrow \frac{\partial y}{\partial \mathbf{x}}=\left[\frac{\partial y}{\partial x_1},\frac{\partial y}{\partial x_2},\cdots,\frac{\partial y}{\partial x_n}\right]</script><p>如：$y=x_1^2+2x_2^2$ ，$\mathbf{x}=[x_1,x_2]$ ，若此时 $(x_1,x_2)=(1,1)$ ，则梯度为 $\frac{\partial y}{\partial \mathbf{x}}_{(1,1)}=[2x_1,4x_2]=(2,4)$</p><ul><li>梯度大小为逐元素求偏导，方向垂直与等值线；梯度方向与等值线垂直</li></ul><p><strong>一些梯度运算法则</strong></p><p><img src="/posts/3226843952/image-20240316102930111.png" alt="image-20240316102930111"></p><p>分子布局：偏导结果的行与分子的行数相等</p><p>分母布局：分子布局的转置</p><p><strong>分子布局</strong></p><p><img src="/posts/3226843952/image-20240316103935908.png" alt="image-20240316103935908"></p><p>矩阵求导</p><div class="table-container"><table><thead><tr><th>列向量</th><th>$x$</th><th>$\mathbf{x}_{p\times 1}$</th><th>$\mathbf{X}_{p\times q}$</th></tr></thead><tbody><tr><td>$y$</td><td>标量： $\frac{\partial y}{\partial x}$</td><td>向量：$\frac{y}{\partial x}_{1\times p}$</td><td>矩阵：$\frac{\partial y}{\partial X}_{ q\times p}$</td></tr><tr><td>$\mathbf{y}_{m\times 1}$</td><td>向量：$\frac{\partial \mathbf{y}}{\partial x}_{m\times 1}$</td><td>矩阵：$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}_{m\times1\times1\times p=m\times p}$</td><td>矩阵：$\frac{\partial \mathbf{y}}{\partial \mathbf{X}}_{m\times 1\times q\times p=m\times q\times p}$</td></tr><tr><td>$\mathbf{Y}_{m\times n}$</td><td>矩阵：$\frac{\partial \mathbf{Y}}{\partial x}_{m\times n}$</td><td>矩阵：$\frac{\partial \mathbf{Y}}{\partial \mathbf{x}}_{m\times n\times 1\times p=m\times n\times p}$</td><td>batch：$\frac{\partial \mathbf{Y}}{\partial \mathbf{X}}_{m\times n\times q\times p}$</td></tr></tbody></table></div><p><img src="/posts/3226843952/image-20240316103847226.png" alt="image-20240316103847226"></p><div class="table-container"><table><thead><tr><th>行向量</th><th>$x$</th><th>$\mathbf{x}_{1\times q}$</th><th>$\mathbf{X}_{p\times q}$</th></tr></thead><tbody><tr><td>$y$</td><td>标量： $\frac{\partial y}{\partial x}$</td><td>向量：$\frac{\partial y}{\partial \mathbf{x}}_{q\times 1}$</td><td>矩阵：$\frac{\partial y}{\partial X}_{p\times q}$</td></tr><tr><td>$\mathbf{y}_{1\times m}$</td><td>向量：$\frac{\partial \mathbf{y}}{\partial x}_{m\times 1}$</td><td>矩阵：$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}_{m\times1\times1\times q=m\times q}$</td><td>矩阵：$\frac{\partial \mathbf{y}}{\partial \mathbf{X}}_{m\times 1\times p\times q=m\times p\times q}$</td></tr><tr><td>$\mathbf{Y}_{n\times m}$</td><td>矩阵：$\frac{\partial \mathbf{Y}}{\partial x}_{m\times n}$</td><td>矩阵：$\frac{\partial \mathbf{Y}}{\partial \mathbf{x}}_{m\times n\times 1\times q=m\times n\times q}$</td><td>batch：$\frac{\partial \mathbf{Y}}{\partial \mathbf{X}}_{m\times n\times p\times q}$</td></tr></tbody></table></div><h4 id="链式求导法则"><a href="#链式求导法则" class="headerlink" title="链式求导法则"></a>链式求导法则</h4><script type="math/tex;mode=display">y=f_5(f_4(f_3(f_2(f_1(x)))))\rightarrow \frac{\partial y}{\partial x}=\frac{\partial f_5}{\partial f_4}\frac{\partial f_4}{\partial f_3}\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial f_1}\frac{\partial f_1}{\partial x}</script><hr><p>若 $x\in \mathbb{R}$，</p><script type="math/tex;mode=display">\begin{cases}
z_i=f_i(y)&z=\left[f_1(y),f_2(y),\cdots,f_N(y)\right]\in \mathbb{R}^{N}\\
y_i=g_i(x)&y=[g_1(x),g_2(x),\cdots,g_M(x)]\in \mathbb{R}^{M}
\end{cases}</script><p>则有</p><script type="math/tex;mode=display">\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}_{N\times M}\frac{\partial y}{\partial x}_{M\times 1}\in\R^{N\times 1}</script><hr><p>若 $x\in \mathbb{R}^{M}$</p><script type="math/tex;mode=display">\begin{cases}
y_i=g_i(x)&y=[g_1(x),g_2(x),\cdots,g_K(x)]\in \mathbb{R}^{K}\\
z_i=f_i(y)&z=\left[f_1(y),f_2(y),\cdots,f_N(y)\right]\in \mathbb{R}^{N}
\end{cases}</script><p>则有</p><script type="math/tex;mode=display">\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}_{N\times K}\frac{\partial y}{\partial x}_{K\times M}\in \mathbb{R}^{N\times M}</script><hr><p>若 $X\in R^{M\times N}$ ，</p><script type="math/tex;mode=display">\begin{cases}
y_i=g_i(X)&y=[g_1(X),g_2(X),\cdots,g_K(X)]\in \mathbb{R}^{K}\\
z_i=f(y)\in \mathbb{R}
\end{cases}</script><p>则有</p><script type="math/tex;mode=display">\frac{\partial z}{\partial x_{ij}}=\frac{\partial z}{\partial y}_{1\times K}\frac{\partial y}{\partial x_{ij}}_{K\times 1}\in \mathbb{R}</script><h5 id="标量链式求导"><a href="#标量链式求导" class="headerlink" title="标量链式求导"></a>标量链式求导</h5><script type="math/tex;mode=display">y=f(u),u=g(x),\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u}\frac{\partial u}{\partial x}</script><h5 id="向量的链式求导"><a href="#向量的链式求导" class="headerlink" title="向量的链式求导"></a>向量的链式求导</h5><script type="math/tex;mode=display">\begin{array}{lll}
y_{1},\mathbf{x}_{p\times 1}&y=f(u),u=g(\mathbf{x})&\frac{\partial y}{\partial \mathbf{x}}_{1\times p}=\frac{\partial y}{\partial u}_1\frac{\partial u}{\partial \mathbf{x}}_{1\times p}\\
&y=f(\mathbf{u}),\mathbf{u}_{k\times 1}=g(\mathbf{x})&\frac{\partial y}{\partial \mathbf{x}}_{1\times p}=\frac{\partial y}{\partial \mathbf{u}}_{1\times k}\frac{\partial \mathbf{u}}{\partial \mathbf{x}}_{k\times p}\\
\mathbf{y}_{m\times 1},\mathbf{x}_{p\times 1}&\mathbf{y}=f(\mathbf{u}),\mathbf{u}_{k\times 1}=g(\mathbf{x})&\frac{\partial \mathbf{y}}{\partial \mathbf{x}}_{m\times p}=\frac{\partial \mathbf{y}}{\partial \mathbf{u}}_{m\times k}\frac{\partial \mathbf{u}}{\partial \mathbf{x}}_{k\times p}
\end{array}</script><p><img src="/posts/3226843952/image-20240316161448871.png" alt="image-20240316161448871"></p><p><img src="/posts/3226843952/image-20240316161740687.png" alt="image-20240316161740687"></p><h5 id="更为高效的参数学习"><a href="#更为高效的参数学习" class="headerlink" title="更为高效的参数学习"></a>更为高效的参数学习</h5><p>梯度下降法需要计算损失函数对每个参数的偏导数，如果通过链式法则逐一对每个参数求偏导，会很低效</p><ul><li><p>反向传播算法</p></li><li><p>自动梯度计算</p><p>计算一个函数在指定值上的导数</p></li></ul><h4 id="自动梯度计算"><a href="#自动梯度计算" class="headerlink" title="自动梯度计算"></a>自动梯度计算</h4><p>神经网络的参数主要通过梯度下降来优化，需要手动用链式求导来计算风险函数对每个参数的梯度，并转换为计算机程序。</p><ul><li>手动计算并转换为计算机程序的过程容易出错</li></ul><p>目前，主流的深度学习框架都包含了自动梯度计算功能，只需要考虑网络结构并用代码实现，大大提高了开发效率</p><h5 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h5><p>用数值方法计算 $f(x)$ 的导数</p><script type="math/tex;mode=display">f'(x)=\lim\limits_{\Delta x\rightarrow 0}\frac{f(x+\Delta x)-f(x)}{\Delta x}</script><ol><li><p>找到一个合适的 $\Delta x$ 十分困难</p><ul><li>$\Delta x$ 过小，会引起数值计算问题，舍入误差</li><li>$\Delta x$ 过大，会增加截断误差（受模型影响的理论值与数值解之间的误差）</li></ul><p>在实际应用中，经常使用以下公式计算梯度，减少截断误差</p><script type="math/tex;mode=display">f'(x)=\lim\limits_{\Delta x\rightarrow 0} \frac{f(x+\Delta x)-f(x+\Delta x)}{2\Delta x}</script></li><li><p>数值微分另一个问题是计算复杂度</p><p>假设参数数量为 $N$ ，则每个参数都需要单独施加扰动，并计算梯度。假设每次正向传播的计算复杂度为 $O(N)$ ，则计算数值微分的总体时间复杂度为 $O(N^2)$</p></li></ol><h5 id="符号微分"><a href="#符号微分" class="headerlink" title="符号微分"></a>符号微分</h5><p>符号计算一般来讲是对 <strong>输入的表达式</strong>，用计算机来通过迭代或递归使用一些事先定义的规则进行转换．当转换结果不能再继续使用变换规则时，便停止计算，<strong>输出是目标函数 / 表达式</strong>。如：mathematica</p><ul><li>一般包括对数学表达式的化简、因式分解、微分、积分、解代数方程、求解常微分方程等运算</li></ul><p>符号微分可以在编译时就计算梯度的数学表示，并进一步利用符号计算方法进行优化</p><p>且符号计算与平台无关，可在CPU或GPU上运行</p><p>缺点：</p><ul><li>编译时间较长，特别是对于循环，需要很长时间进行编译</li><li>为了进行符号微分，一般需要设计一种专门的语言来表示数学表达式，并且要对变量（符号）进行预先声明</li><li>很难对程序进行调试</li></ul><h4 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h4><h5 id="自动微分与符号微分区别"><a href="#自动微分与符号微分区别" class="headerlink" title="自动微分与符号微分区别"></a>自动微分与符号微分区别</h5><p>符号微分：处理数学表达式</p><p>自动微分：处理一个函数或一段程序</p><p><img src="/posts/3226843952/image-20231007173446670.png" alt="image-20231007173446670"></p><p>符号微分和自动微分都用计算图和链式法则自动求导</p><p>符号微分：</p><ol><li>符号微分在编译阶段先构造一个符合函数的计算图，通过符号计算得到导数表达式，并对表达式进行优化</li><li>在程序运行运行阶段才代入变量数值计算导数</li></ol><p>自动微分：</p><ol><li>无需事先编译，程序运行阶段，边计算边记录计算图</li><li>计算图上的局部梯度都直接代入数值进行计算，然后用前向或反向模式计算最终梯度</li></ol><h5 id="自动微分过程"><a href="#自动微分过程" class="headerlink" title="自动微分过程"></a>自动微分过程</h5><blockquote><p>基本原理：所有的数值计算可以分解为一些基本操作，包含+, −, ×, / 和一些初等函数exp, log, sin, cos 等，然后利用链式法则来自动计算一个复合函数的梯度</p></blockquote><p>将表达式分解为操作子，将计算表示成无环图</p><p><img src="/posts/3226843952/image-20240316171423804.png" alt="image-20240316171423804"></p><p>正向求导（从简单到复杂）：</p><ol><li>输入 $\mathbf{w},\mathbf{x}$ ，对 $a=(\mathbf{w},\mathbf{x})$ 求偏导 $\frac{\partial a}{\partial \mathbf{w}}=\mathbf{x}$</li><li>$\mathbf{b}=a-y$ 对 $a$ 求偏导 $\frac{\partial b}{\partial a}=1$</li><li>$z=\mathbf{b}^2$ 对 $b$ 求偏导，$\frac{\partial z}{\partial \mathbf{b}}=2\mathbf{b}$</li></ol><h5 id="自动求导模式"><a href="#自动求导模式" class="headerlink" title="自动求导模式"></a>自动求导模式</h5><p>链式法则</p><script type="math/tex;mode=display">\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u_n}\frac{\partial u_{n}}{\partial u_{n-1}}\frac{\partial u_{n-1}}{\partial u_{n-2}}\cdots \frac{\partial u_{2}}{\partial u_{1}}\frac{\partial u_{1}}{\partial x}</script><ul><li><p>正向累积</p><script type="math/tex;mode=display">\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u_n}\left(\frac{\partial u_{n}}{\partial u_{n-1}}\frac{\partial u_{n-1}}{\partial u_{n-2}}\left(\cdots \left(\frac{\partial u_{2}}{\partial u_{1}}\frac{\partial u_{1}}{\partial x}\right)\right)\right)</script><p>从简单到复杂</p></li><li><p>反向累积、反向传递</p><script type="math/tex;mode=display">\frac{\partial y}{\partial x}=\left(\left(\left(\left(\frac{\partial y}{\partial u_n}\frac{\partial u_{n}}{\partial u_{n-1}}\right)\frac{\partial u_{n-1}}{\partial u_{n-2}}\right)\cdots\right)\frac{\partial u_{2}}{\partial u_{1}}\right) \frac{\partial u_{1}}{\partial x}</script><p>从复杂到简单</p></li></ul><h5 id="反向传递计算过程"><a href="#反向传递计算过程" class="headerlink" title="反向传递计算过程"></a>反向传递计算过程</h5><ol><li><p>正向计算函数值，存储中间值</p><p><img src="/posts/3226843952/image-20240316173835569.png" alt="image-20240316173835569"></p></li><li><p>反向计算导数值，去除无关的枝</p><p><img src="/posts/3226843952/image-20240316173918937.png" alt="image-20240316173918937"></p></li></ol><h5 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h5><p>$n$ 为中间变量个数</p><p>正向计算代价和反向计算代价都是 $O(n)$</p><p>反向计算内存复杂度 $O(n)$ ：需要存储正向计算的所有中间结果</p><p>正向累积：</p><ul><li>计算一个变量的梯度时间复杂度为 $O(n)$</li><li>内存复杂度 $O(1)$ ：不需要存任何结果</li><li>计算几个变量扫几遍遍计算图，计算效率低</li></ul><h5 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h5><p>计算图可以显式构造，也可以隐式构造</p><ul><li><p>mxnet ， Tensorflow / Theano都可显式构造</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> sym<span class="comment">#(symbol)</span></span><br><span class="line"></span><br><span class="line">a = sym.var() <span class="comment">#将a定义为中间变量，后续可指定表达式</span></span><br><span class="line">b = sym.var() <span class="comment">#将b定义为中间变量，后续可指定表达式</span></span><br><span class="line">c = <span class="number">2</span>*a+b</span><br></pre></td></tr></table></figure></li><li><p>隐式构造：Pytorch、Mxnet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,nd<span class="comment">#(symbol)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    a = nd.ones((<span class="number">2</span>,<span class="number">1</span>)) <span class="comment">#构造2维全1列向量</span></span><br><span class="line">    b = nd.ones((<span class="number">2</span>,<span class="number">1</span>)) <span class="comment">#构造2维全1列向量</span></span><br><span class="line">    c = <span class="number">2</span>*a+b</span><br></pre></td></tr></table></figure></li></ul><hr><p>以 $f(x;w,b)=\frac{1}{exp(-(wx+b))+1}$ 为例，其中 $x$ 为输入标量， $w,b$ 为权重和偏置</p><p><img src="/posts/3226843952/image-20240320223639702.png" alt="image-20240320223639702"></p><p><img src="/posts/3226843952/image-20240320223715559.png" alt="image-20240320223715559"></p><p>复合函数关于参数的导数可以通过计算图路径上节点的所有导数连乘法得到</p><p><img src="/posts/3226843952/image-20231007164548988.png" alt="image-20231007164548988"></p><ul><li><img src="/posts/3226843952/image-20231007164626492.png" alt="image-20231007164626492"></li></ul><p>如果函数与参数之间有多条路径，则将不同路径上的导数相加，可以得到最终的梯度</p><hr><p>根据计算导数的顺序，自动微分可以分为：正向模式和反向模式</p><ul><li><p>正向模式（从简单到复合）：按照计算图中与参数计算方向相同的方向来递归计算梯度</p><p><img src="/posts/3226843952/image-20231007165242804.png" alt="image-20231007165242804"></p></li><li><p>反向模式（从复合到简单）：按照计算图中与参数计算方向相反的方向来计算梯度</p><p><img src="/posts/3226843952/image-20231007165350176.png" alt="image-20231007165350176"></p><p>反向模式与反向传播的梯度计算方式相同</p></li></ul><p>准则：</p><p>当输入变量的数量大于输出变量的数量，用反向模式</p><ul><li><p>前向模式需要对每一个输入都进行遍历</p></li><li><p>反向模式需要对每一个输出都进行遍历</p></li></ul><p>在前馈神经网络中，风险函数为 $f:\R^{N}\rightarrow \mathbb{R}$ 输出为标量，采用反向模式，内存占用小，只需要计算一遍</p><hr><p><strong>静态计算图和动态计算图</strong></p><p>静态计算图：在编译时构建计算图，运行过程中不可修改</p><ul><li>在构建时可以进行优化，并行能力强</li><li>灵活性差</li></ul><p>动态计算图：在程序运行时构建计算图</p><ul><li>不容易优化，输入不同结构的网络，难以并行计算</li><li>灵活性高</li></ul><h3 id="2-3-4-用tensor实现线代"><a href="#2-3-4-用tensor实现线代" class="headerlink" title="2.3.4 用tensor实现线代"></a>2.3.4 用tensor实现线代</h3><h4 id="标量"><a href="#标量" class="headerlink" title="标量"></a>标量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">3.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([<span class="number">2.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x+y,x*y,x/y,x**y)</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">5.</span>]) tensor([<span class="number">6.</span>]) tensor([<span class="number">1.5000</span>]) tensor([<span class="number">9.</span>])</span><br></pre></td></tr></table></figure><h4 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h4><p><strong>torch中创建的向量默认是行向量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(x)</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">torch.Size([<span class="number">4</span>])</span><br></pre></td></tr></table></figure><h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.T</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">4</span>,  <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">5</span>,  <span class="number">9</span>, <span class="number">13</span>, <span class="number">17</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">14</span>, <span class="number">18</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">19</span>]])</span><br></pre></td></tr></table></figure><p>对称阵，$\mathbf{A}=\mathbf{A}^T$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B == B.T</span><br><span class="line">tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p>向量是标量的推广，矩阵是向量的推广，张量是矩阵的推广，可以构造具有任意多轴的数据结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure><h5 id="逐元素运算"><a href="#逐元素运算" class="headerlink" title="逐元素运算"></a>逐元素运算</h5><p>相同形状的任意两个张量，按运算的二元运算结果都是相同形状的张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.arange(<span class="number">20</span>,dtype=torch.float32).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = A.clone() <span class="comment">#分配新的内存，将A的副本分配给B</span></span><br><span class="line">B=A<span class="comment">#二者均指向同一块内存，改变B，A也会变</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A,A+B</span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">        [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]), tensor([[ <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">12.</span>, <span class="number">14.</span>],</span><br><span class="line">        [<span class="number">16.</span>, <span class="number">18.</span>, <span class="number">20.</span>, <span class="number">22.</span>],</span><br><span class="line">        [<span class="number">24.</span>, <span class="number">26.</span>, <span class="number">28.</span>, <span class="number">30.</span>],</span><br><span class="line">        [<span class="number">32.</span>, <span class="number">34.</span>, <span class="number">36.</span>, <span class="number">38.</span>]]))</span><br></pre></td></tr></table></figure><p>矩阵按元素乘法称为 哈达玛积 $\odot$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">        [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">        [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * B</span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">1.</span>,   <span class="number">4.</span>,   <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">16.</span>,  <span class="number">25.</span>,  <span class="number">36.</span>,  <span class="number">49.</span>],</span><br><span class="line">        [ <span class="number">64.</span>,  <span class="number">81.</span>, <span class="number">100.</span>, <span class="number">121.</span>],</span><br><span class="line">        [<span class="number">144.</span>, <span class="number">169.</span>, <span class="number">196.</span>, <span class="number">225.</span>],</span><br><span class="line">        [<span class="number">256.</span>, <span class="number">289.</span>, <span class="number">324.</span>, <span class="number">361.</span>]])</span><br></pre></td></tr></table></figure><p>标量乘矩阵也是逐元素运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a+X,(a*X).shape</span><br><span class="line">(tensor([[[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">         [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>],</span><br><span class="line">         [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>]]]), torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br></pre></td></tr></table></figure><h5 id="求和"><a href="#求和" class="headerlink" title="求和"></a>求和</h5><p><strong>所有元素求和</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p><strong>指定轴求和</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">        [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) <span class="comment">#0轴为行，结果呈现在1轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis0 , A_sum_axis0.shape</span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)<span class="comment">#0轴为列，结果呈现在0轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis1 , A_sum_axis1.shape</span><br><span class="line">(tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]), torch.Size([<span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">tensor(<span class="number">190.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A,A.shape</span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">        [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]), torch.Size([<span class="number">5</span>, <span class="number">4</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) <span class="comment">#0轴为通道，结果放在1,2轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis0 , A_sum_axis0.shape</span><br><span class="line">(tensor([[<span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">22</span>, <span class="number">24</span>, <span class="number">26</span>],</span><br><span class="line">        [<span class="number">28</span>, <span class="number">30</span>, <span class="number">32</span>, <span class="number">34</span>]]), torch.Size([<span class="number">3</span>, <span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)<span class="comment">#1轴为高，结果放在0,2轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis1 , A_sum_axis1.shape</span><br><span class="line">(tensor([[<span class="number">12</span>, <span class="number">15</span>, <span class="number">18</span>, <span class="number">21</span>],</span><br><span class="line">        [<span class="number">48</span>, <span class="number">51</span>, <span class="number">54</span>, <span class="number">57</span>]]), torch.Size([<span class="number">2</span>, <span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis2 = A.<span class="built_in">sum</span>(axis=<span class="number">2</span>) <span class="comment">#2轴为宽，结果放在0,1轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis2 , A_sum_axis2.shape</span><br><span class="line">(tensor([[ <span class="number">6</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        [<span class="number">54</span>, <span class="number">70</span>, <span class="number">86</span>]]), torch.Size([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis12 = A.<span class="built_in">sum</span>(axis=[<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 1,2轴求和，结果放在0轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A_sum_axis12, A_sum_axis12.shape</span><br><span class="line">(tensor([ <span class="number">66</span>, <span class="number">210</span>]), torch.Size([<span class="number">2</span>]))</span><br></pre></td></tr></table></figure><h5 id="求均值"><a href="#求均值" class="headerlink" title="求均值"></a>求均值</h5><p>实质上是求和除轴上元素个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.arange(<span class="number">0.</span>,<span class="number">20</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.mean()</span><br><span class="line">tensor(<span class="number">9.5000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.<span class="built_in">sum</span>() / A.numel()</span><br><span class="line">tensor(<span class="number">9.5000</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.mean(axis=<span class="number">0</span>),A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)/A.shape[<span class="number">0</span>]</span><br><span class="line">(tensor([ <span class="number">7.5000</span>,  <span class="number">8.5000</span>,  <span class="number">9.5000</span>, <span class="number">10.5000</span>, <span class="number">11.5000</span>]), tensor([ <span class="number">7.5000</span>,  <span class="number">8.5000</span>,  <span class="number">9.5000</span>, <span class="number">10.5000</span>, <span class="number">11.5000</span>]))</span><br></pre></td></tr></table></figure><h5 id="求和或均值时保持轴数-秩-不变"><a href="#求和或均值时保持轴数-秩-不变" class="headerlink" title="求和或均值时保持轴数(秩)不变"></a>求和或均值时保持轴数(秩)不变</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sum_A</span><br><span class="line">tensor([[<span class="number">10.</span>],</span><br><span class="line">        [<span class="number">35.</span>],</span><br><span class="line">        [<span class="number">60.</span>],</span><br><span class="line">        [<span class="number">85.</span>]])</span><br></pre></td></tr></table></figure><p>按轴求和，$4\times 5$ 变为 $4\times 1$</p><p>可通过广播机制对每列各元素使用不同数做相同操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A / sum_A</span><br><span class="line">tensor([[<span class="number">0.0000</span>, <span class="number">0.1000</span>, <span class="number">0.2000</span>, <span class="number">0.3000</span>, <span class="number">0.4000</span>],</span><br><span class="line">        [<span class="number">0.1429</span>, <span class="number">0.1714</span>, <span class="number">0.2000</span>, <span class="number">0.2286</span>, <span class="number">0.2571</span>],</span><br><span class="line">        [<span class="number">0.1667</span>, <span class="number">0.1833</span>, <span class="number">0.2000</span>, <span class="number">0.2167</span>, <span class="number">0.2333</span>],</span><br><span class="line">        [<span class="number">0.1765</span>, <span class="number">0.1882</span>, <span class="number">0.2000</span>, <span class="number">0.2118</span>, <span class="number">0.2235</span>]])</span><br></pre></td></tr></table></figure><h5 id="某个轴上累加求和"><a href="#某个轴上累加求和" class="headerlink" title="某个轴上累加求和"></a>某个轴上累加求和</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>],</span><br><span class="line">        [<span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.cumsum(axis=<span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">11.</span>, <span class="number">13.</span>],</span><br><span class="line">        [<span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>, <span class="number">24.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">30.</span>, <span class="number">34.</span>, <span class="number">38.</span>, <span class="number">42.</span>, <span class="number">46.</span>]])</span><br></pre></td></tr></table></figure><h5 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">4</span>,dtype=torch.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="number">4</span>,dtype=torch.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x,y,torch.dot(x,y)</span><br><span class="line">(tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]), tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]), tensor(<span class="number">6.</span>))</span><br></pre></td></tr></table></figure><p>等价于逐元素乘再求和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">sum</span>(x*y)</span><br><span class="line">tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure><h5 id="矩阵乘向量"><a href="#矩阵乘向量" class="headerlink" title="矩阵乘向量"></a>矩阵乘向量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.arange(<span class="number">20</span>,dtype=torch.float32).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">5</span>,dtype=torch.float32)</span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>],</span><br><span class="line">        [<span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]])</span><br><span class="line"> tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mv(A,x), torch.mv(A,x).shape</span><br><span class="line">(tensor([ <span class="number">30.</span>,  <span class="number">80.</span>, <span class="number">130.</span>, <span class="number">180.</span>]), torch.Size([<span class="number">4</span>]))</span><br></pre></td></tr></table></figure><p>$\mathbf{A}\cdot \mathbf{x}$ 的第 $i$ 个元素 $i_{th}=\mathbf{a}_{i}^T\mathbf{x}$ ，$\mathbf{a}_i$ 为矩阵 $\mathbf{A}$ 的第 $i$ 个行向量</p><h5 id="矩阵乘矩阵"><a href="#矩阵乘矩阵" class="headerlink" title="矩阵乘矩阵"></a>矩阵乘矩阵</h5><ul><li><code>torch.mm(A,B)</code></li><li><code>torch.dot(A,B)</code></li><li><code>torch.matmul(A,B)</code></li><li><code>A@B</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.arange(<span class="number">20</span>,dtype=torch.float32).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = torch.arange(<span class="number">20</span>,dtype=torch.float32).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mm(A,B)</span><br><span class="line">torch.matmul(A,B)</span><br><span class="line">tensor([[<span class="number">120.</span>, <span class="number">130.</span>, <span class="number">140.</span>, <span class="number">150.</span>],</span><br><span class="line">        [<span class="number">320.</span>, <span class="number">355.</span>, <span class="number">390.</span>, <span class="number">425.</span>],</span><br><span class="line">        [<span class="number">520.</span>, <span class="number">580.</span>, <span class="number">640.</span>, <span class="number">700.</span>],</span><br><span class="line">        [<span class="number">720.</span>, <span class="number">805.</span>, <span class="number">890.</span>, <span class="number">975.</span>]])</span><br></pre></td></tr></table></figure><p>将矩阵乘法 $\mathbf{A}_{m\times j}\cdot\mathbf{B}_{j\times n}$ 看做 $n$ 次矩阵乘向量 $\mathbf{A}\cdot \mathbf{b}_t$ ，$\mathbf{b}_t$ 为矩阵 $\mathbf{B}$ 的第 $t$ 个列向量，$t\in [0,n-1]$</p><h5 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h5><p>2范数是向量元素平方和的平方根</p><script type="math/tex;mode=display">\Vert \mathbf{x}\Vert_2=\sqrt{\sum\limits_{i=1}^nx_i^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">3</span>,dtype=torch.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(x)</span><br><span class="line">tensor(<span class="number">2.2361</span>)</span><br></pre></td></tr></table></figure><p>1范数是向量元素的绝对值之和</p><script type="math/tex;mode=display">\Vert \mathbf{x}\Vert_1=\sum\limits_{i=1}^n\vert x_i\vert</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">abs</span>(x).<span class="built_in">sum</span>()</span><br><span class="line">tensor(<span class="number">3.</span>)</span><br></pre></td></tr></table></figure><h5 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h5><p>F范数为矩阵元素的平方和的根</p><script type="math/tex;mode=display">\Vert \mathbf{A}\Vert_F=\sqrt{\sum\limits_{i=1}^m\sum\limits_{i=1}^na_{ij}^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(torch.ones(<span class="number">4</span>,<span class="number">9</span>))</span><br><span class="line">tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure><h4 id="自动求导-1"><a href="#自动求导-1" class="headerlink" title="自动求导"></a>自动求导</h4><p>$\mathbf{y}=2\mathbf{x}^2=2\cdot \mathbf{x}^T\cdot \mathbf{x}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad_(<span class="literal">True</span>) <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>在计算关于 $\mathbf{x}$ 的梯度前，若想要存储 $\mathbf{x}$ 的每个分量的梯度值，需要通过参数 <code>requires_grad=True</code> 开启存储在内存选项，且这个内存空间会被重复利用，更新 $\mathbf{x}$ 的值后，再次计算其梯度，仍存储在第一次分配的内存空间中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor(<span class="number">28.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><ul><li>梯度函数 <code>grad_fn=&lt;MulBackward0&gt;</code> ：隐式构造了计算图，表明 $y$ 是从 $\mathbf{x}$ 计算得出的</li></ul><p>调用反向传播函数来自动计算 $y$ 关于 $\mathbf{x}$ 的每个分量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.backward() <span class="comment">#.backward()为反向求导</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br></pre></td></tr></table></figure><p>在计算关于 $\mathbf{x}$ 的另一个函数前，需要清零初始化</p><ul><li><p>默认情况下，pytorch会累加梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x.<span class="built_in">sum</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([ <span class="number">1.</span>,  <span class="number">5.</span>,  <span class="number">9.</span>, <span class="number">13.</span>])</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad.zero_()</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x.<span class="built_in">sum</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><h5 id="非标量的反向传播"><a href="#非标量的反向传播" class="headerlink" title="非标量的反向传播"></a>非标量的反向传播</h5><p>向量对向量的导数是微分矩阵，高维的 $\mathbf{X}$ 对 $\mathbf{Y}$ 的导数是高阶张量</p><p>但是深度学习中，调用反向传播计算梯度的目的并不是得到微分矩阵(高阶微分张量)，而是单独计算一个batch中每个样本对某个参数的偏导数之和</p><ul><li>因此这也是 pytorch 会默认累加梯度的原因</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad.zero_()</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure><h5 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h5><p>有时，我们希望将某些计算移动到计算图外。</p><script type="math/tex;mode=display">y=f(x)\\
z=g(x,y)\\</script><p>求 $\frac{dz}{dx}$ ，若将 $y$ 视为常数，且只考虑 $x$ 在 $y$ 被计算后产生的作用。即先计算部分梯度</p><p>则分离 $y$ 将其作为一个新变量 $u$ ，但将其视为与 $x$ 无关的变量，该变量与 $y$ 有相同的值。</p><p>如：</p><script type="math/tex;mode=display">\begin{aligned}
&令 \mathbf{x}=[x_1,x_2,x_3]\in \mathbb{R}^3，\\
&\left.
\begin{aligned}
\mathbf{y}=\mathbf{x}*\mathbf{x}\\
\mathbf{z}=\mathbf{y}* \mathbf{x}
\end{aligned}
\right\}\Rightarrow \mathbf{z}=y(\mathbf{x})*\mathbf{x}=[x_1^3,x_2^3,x_3^3]
\end{aligned}</script><p>不分离计算的情况，</p><script type="math/tex;mode=display">\begin{aligned}
\frac{\partial \mathbf{z}}{\partial \mathbf{x}}&=\left[\frac{\partial z_1}{\partial x_1},\frac{\partial z_2}{\partial x_2},\frac{\partial z_3}{\partial x_3}\right]=\left[\frac{\partial y(x_1)x_1}{\partial x_1},\frac{\partial y(x_2)x_2}{\partial x_2},\frac{\partial y(x_3)x_3}{\partial x_3}\right]\\
&=\left[y'(x_1)x_1+y(x_1),y'(x_2)x_2+y(x_2),y'(x_3)x_3+y(x_3)\right]\\
&=\left[2x_1^2+x_1^2,2x_2^2+x_2^2,2x_3^2+x_3^2\right]\\
&=[3x_1^2,3x_2^2,3x_3^2]
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">1.0</span>,<span class="number">10</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = y * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.backward(torch.ones(<span class="built_in">len</span>(x)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([  <span class="number">3.</span>,  <span class="number">27.</span>,  <span class="number">75.</span>, <span class="number">147.</span>, <span class="number">243.</span>])</span><br></pre></td></tr></table></figure><p><img src="/posts/3226843952/image-20240316204942597.png" alt="image-20240316204942597"></p><p>分离情况，仅计算到 $\mathbf{u}$ ，梯度并不会向后传到到 $\mathbf{x}$</p><script type="math/tex;mode=display">\left.
\begin{aligned}
\mathbf{y}=\mathbf{x}*\mathbf{x}\\
\mathbf{z}=\mathbf{y}* \mathbf{x}
\end{aligned}
\right\}\xRightarrow{\mathbf{u}=\mathbf{x}*\mathbf{x}} \mathbf{z}=\mathbf{u}* \mathbf{x}=\left[x_1\mathbf{u}_1,x_2\mathbf{u}_2,x_3\mathbf{u}_3\right]</script><script type="math/tex;mode=display">\frac{\partial \mathbf{z}}{\partial \mathbf{x}}=\left[\frac{\partial z_1}{\partial x_1},\frac{\partial z_2}{\partial x_2},\frac{\partial z_3}{\partial x_3}\right]=\left[\frac{\partial x_1\mathbf{u}_1}{\partial x_1},\frac{\partial x_2\mathbf{u}_2}{\partial x_2},\frac{\partial x_3\mathbf{u}_3}{\partial x_3}\right]

=[\mathbf{u}_1,\mathbf{u}_2,\mathbf{u}_3]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>u = y.detach()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = u * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.backward(torch.ones(<span class="built_in">len</span>(x)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([ <span class="number">1.</span>,  <span class="number">9.</span>, <span class="number">25.</span>, <span class="number">49.</span>, <span class="number">81.</span>])</span><br></pre></td></tr></table></figure><h5 id="控制流梯度计算"><a href="#控制流梯度计算" class="headerlink" title="控制流梯度计算"></a>控制流梯度计算</h5><p>即使构建函数的计算图需要通过控制流，仍可计算得到变量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure><p>相当于 $d$ 是 $b$ 的函数，而 $b$ 的取值由 $a$ 各元素的和与其二次幂的二范数有关</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.grad == d / a</span><br><span class="line">tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="2-4-常用损失函数"><a href="#2-4-常用损失函数" class="headerlink" title="2.4 常用损失函数"></a>2.4 常用损失函数</h2><h3 id="平方损失"><a href="#平方损失" class="headerlink" title="平方损失"></a>平方损失</h3><script type="math/tex;mode=display">\ell(y,y')=\frac{1}{2}\left(y-y'\right)^2</script><p><img src="/posts/3226843952/image-20240317233728332.png" alt="image-20240317233728332"></p><ul><li>蓝线：损失函数曲线</li><li>绿线：似然函数</li><li>橙线：导函数</li></ul><p>特点：随着损失函数减小，梯度会越来越小，对参数的更新幅度会越来越小</p><h3 id="L1损失"><a href="#L1损失" class="headerlink" title="L1损失"></a>L1损失</h3><script type="math/tex;mode=display">\ell(y,y')=\vert y-y'\vert</script><p><img src="/posts/3226843952/image-20240317233809540.png" alt="image-20240317233809540"></p><p>特点：预测值与真实值损失的梯度都是常数，对参数更新影响不会随着损失函数的变化而变化，稳定性比较好</p><p>0点处不可导，且在0附近导数有正负变化，当优化到末期（预测值接近真实值），稳定性变差</p><h3 id="Huber’s-Robust-损失"><a href="#Huber’s-Robust-损失" class="headerlink" title="Huber’s Robust 损失"></a>Huber’s Robust 损失</h3><script type="math/tex;mode=display">\ell(y,y')=\begin{cases}
\vert y-y'\vert-\frac{1}{2}&,\vert y-y'\vert>1\\
\frac{1}{2}\left(y-y'\right)^2&,其他
\end{cases}</script><ul><li><p>当预测值与真实值相差大时，绝对误差：梯度均匀变化</p></li><li><p>当预测值与真实值相差小时，平方误差：梯度取值会越来越小</p><p>保证参数优化过程是平滑的</p></li></ul><p><img src="/posts/3226843952/image-20240317234518427.png" alt="image-20240317234518427"></p><h2 id="2-5-基础优化方法"><a href="#2-5-基础优化方法" class="headerlink" title="2.5 基础优化方法"></a>2.5 基础优化方法</h2><p><img src="/posts/3226843952/image-20240317095709565.png" alt="image-20240317095709565"></p><ul><li>负梯度是值下降最快的方向，梯度方向垂直与函数等值线</li></ul><p>每次参数更新，需要对损失函数求梯度，损失函数是所有样本的平均损失，求一次梯度需要把所有样本重新计算一次。在一个神经网络模型中可能需要数分钟甚至数小时</p><p>由于梯度的计算是整个学习过程中最耗费计算资源的部分，所以梯度的计算次数应该尽量小，</p><ul><li><p>学习率的选择：学习率会决定梯度下降的步数</p><p><img src="/posts/3226843952/image-20240317095959100.png" alt="image-20240317095959100"></p><p>学习率太小：梯度下降慢，一次更新优化效果不明显</p><p>学习率太大：出现除零或除无穷的情况，NaN</p></li><li><p>小批量随机梯度下降是深度学习默认的求解算法——稳定、简单</p><p>随机采样 $b\ll n$ 个小批量样本 $\mathcal{B}$ 来近似损失，</p><script type="math/tex;mode=display">\overline{\ell}_t=\frac{1}{\vert \mathcal{B}\vert}\sum\limits_{i\in \mathbf{I}_\mathcal{B}}\ell(\mathbf{x}_i,\mathbf{y}_i,\mathbf{w}_t)</script><p>用随机批量损失的梯度代替整体样本损失的梯度</p><script type="math/tex;mode=display">\mathbf{w}_t\leftarrow\mathbf{w}_{t-1}-\frac{\eta}{\vert \mathcal{B}\vert}\sum\limits_{i\in\mathbf{I}_{\mathcal{B}}}\frac{\partial}{\partial \mathbf{w}}\ell(\mathbf{x}_i,\mathbf{y}_i,\mathbf{w}_t)</script><p><img src="/posts/3226843952/image-20240317100942327.png" alt="image-20240317100942327"></p></li></ul><p>对损失函数求平均，是为了将梯度控制在一定的范围内，便于对比调整学习率的效果</p><ul><li>对学习率不太敏感的优化算法：Adam</li></ul></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/3226843952/" title="0.动手学深度学习">https://amostian.github.io/posts/3226843952/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/4136854086/" rel="prev" title="5-基于策略梯度的RL"><i class="fa fa-chevron-left"></i> 5-基于策略梯度的RL</a></div><div class="post-nav-item"><a href="/posts/3309717669/" rel="next" title="2+3.线性模型与全连接前馈神经网络">2+3.线性模型与全连接前馈神经网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="nav-text">1. 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9"><span class="nav-text">1.1 学习内容</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-text">1.2 深度学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-%E5%AD%A6%E7%A7%91%E8%8C%83%E7%95%B4"><span class="nav-text">1.2.1 学科范畴</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-%E8%AF%BE%E7%A8%8B%E8%AE%BE%E7%BD%AE"><span class="nav-text">1.2.2 课程设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-%E5%BA%94%E7%94%A8"><span class="nav-text">1.2.3 应用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F"><span class="nav-text">图像</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB"><span class="nav-text">图片分类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2"><span class="nav-text">目标检测和分割</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB"><span class="nav-text">样式迁移</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%BA%E8%84%B8%E5%90%88%E6%88%90"><span class="nav-text">人脸合成</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-text">生成模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87"><span class="nav-text">文字生成图片</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-text">文本生成模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%94%E7%94%A8%E4%BA%8E%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6"><span class="nav-text">计算机视觉应用于无人驾驶</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BB%E6%A1%88%E4%BE%8B"><span class="nav-text">广告点击案例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-4-%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7"><span class="nav-text">1.2.4 模型可解释性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-text">1.3 表示学习(特征处理)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-1-%E4%BC%A0%E7%BB%9F%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.3.1 传统特征学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%87%E6%BB%A4%E5%BC%8F"><span class="nav-text">过滤式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8C%85%E8%A3%B9%E5%BC%8F"><span class="nav-text">包裹式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#L-1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">$L_1$ 正则化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96"><span class="nav-text">特征抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E7%9A%84%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="nav-text">监督的特征学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="nav-text">无监督的特征学习</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BD%9C%E7%94%A8"><span class="nav-text">特征工程作用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-2-%E8%AF%AD%E4%B9%89%E9%B8%BF%E6%B2%9F"><span class="nav-text">1.3.2 语义鸿沟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-%E8%A1%A8%E7%A4%BA%E6%96%B9%E5%BC%8F"><span class="nav-text">1.3.3 表示方式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E8%81%94"><span class="nav-text">关联</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-4-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.3.4 表示学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94"><span class="nav-text">对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-text">深度学习与表示学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5"><span class="nav-text">深度学习概念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF"><span class="nav-text">端到端</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E8%A1%A8%E7%A4%BA"><span class="nav-text">深度学习数学表示</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E7%BB%AA%E8%AE%BA"><span class="nav-text">2. 绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-0-%E8%B5%84%E6%BA%90"><span class="nav-text">2.0 资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E5%AE%89%E8%A3%85"><span class="nav-text">2.1 安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-%E6%9F%A5%E9%98%85%E6%96%87%E6%A1%A3"><span class="nav-text">2.1.1 查阅文档</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="nav-text">2.2 数据操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-text">2.2.1 数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-tensor%E6%93%8D%E4%BD%9C%E4%B8%8E%E8%BF%90%E7%AE%97"><span class="nav-text">2.2.2 tensor操作与运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA"><span class="nav-text">创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%93%8D%E4%BD%9C"><span class="nav-text">操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%90%E7%AE%97"><span class="nav-text">运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE"><span class="nav-text">访问</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E5%86%85%E5%AD%98%E9%87%8D%E5%88%86%E9%85%8D"><span class="nav-text">避免内存重分配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="nav-text">变量类型转换</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">2.2.3 数据预处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-text">2.3 线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%A6%E5%AE%9A"><span class="nav-text">约定</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-%E6%A0%87%E9%87%8F"><span class="nav-text">2.3.1 标量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-%E5%90%91%E9%87%8F"><span class="nav-text">2.3.2 向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-%E7%9F%A9%E9%98%B5"><span class="nav-text">2.3.3 矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5"><span class="nav-text">特殊矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AF%B9%E7%A7%B0%E4%B8%8E%E5%8F%8D%E5%AF%B9%E7%A7%B0"><span class="nav-text">对称与反对称</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A3%E5%AE%9A%E9%98%B5"><span class="nav-text">正定阵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A3%E4%BA%A4"><span class="nav-text">正交</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%AE%E6%8D%A2%E9%98%B5"><span class="nav-text">置换阵</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F-amp-%E7%89%B9%E5%BE%81%E5%80%BC"><span class="nav-text">特征向量&amp;特征值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC"><span class="nav-text">矩阵求导</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%87%E9%87%8F%E5%AF%BC%E6%95%B0"><span class="nav-text">标量导数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="nav-text">偏导数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6"><span class="nav-text">梯度</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99"><span class="nav-text">链式求导法则</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%87%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC"><span class="nav-text">标量链式求导</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC"><span class="nav-text">向量的链式求导</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9B%B4%E4%B8%BA%E9%AB%98%E6%95%88%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-text">更为高效的参数学习</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-text">自动梯度计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86"><span class="nav-text">数值微分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%BE%AE%E5%88%86"><span class="nav-text">符号微分</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-text">自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E4%B8%8E%E7%AC%A6%E5%8F%B7%E5%BE%AE%E5%88%86%E5%8C%BA%E5%88%AB"><span class="nav-text">自动微分与符号微分区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E8%BF%87%E7%A8%8B"><span class="nav-text">自动微分过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E6%A8%A1%E5%BC%8F"><span class="nav-text">自动求导模式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E9%80%92%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-text">反向传递计算过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="nav-text">复杂度</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-text">计算图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-4-%E7%94%A8tensor%E5%AE%9E%E7%8E%B0%E7%BA%BF%E4%BB%A3"><span class="nav-text">2.3.4 用tensor实现线代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E9%87%8F"><span class="nav-text">标量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F"><span class="nav-text">向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5"><span class="nav-text">矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-text">张量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%90%E5%85%83%E7%B4%A0%E8%BF%90%E7%AE%97"><span class="nav-text">逐元素运算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B1%82%E5%92%8C"><span class="nav-text">求和</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B1%82%E5%9D%87%E5%80%BC"><span class="nav-text">求均值</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B1%82%E5%92%8C%E6%88%96%E5%9D%87%E5%80%BC%E6%97%B6%E4%BF%9D%E6%8C%81%E8%BD%B4%E6%95%B0-%E7%A7%A9-%E4%B8%8D%E5%8F%98"><span class="nav-text">求和或均值时保持轴数(秩)不变</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9F%90%E4%B8%AA%E8%BD%B4%E4%B8%8A%E7%B4%AF%E5%8A%A0%E6%B1%82%E5%92%8C"><span class="nav-text">某个轴上累加求和</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%82%B9%E7%A7%AF"><span class="nav-text">点积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E5%90%91%E9%87%8F"><span class="nav-text">矩阵乘向量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E7%9F%A9%E9%98%B5"><span class="nav-text">矩阵乘矩阵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0"><span class="nav-text">向量范数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0"><span class="nav-text">矩阵范数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC-1"><span class="nav-text">自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E6%A0%87%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">非标量的反向传播</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="nav-text">分离计算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6%E6%B5%81%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-text">控制流梯度计算</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">2.4 常用损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="nav-text">平方损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1%E6%8D%9F%E5%A4%B1"><span class="nav-text">L1损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Huber%E2%80%99s-Robust-%E6%8D%9F%E5%A4%B1"><span class="nav-text">Huber’s Robust 损失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-text">2.5 基础优化方法</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">216</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">66</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">83</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1150.8k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">46:51</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>