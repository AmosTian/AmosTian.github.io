<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="5.动手学深度学习-pytorch深度计算"><meta property="og:url" content="https://amostian.github.io/posts/958416017/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/958416017/image-20240322093857097.png"><meta property="og:image" content="https://amostian.github.io/posts/958416017/image-20240322175345411.png"><meta property="og:image" content="https://amostian.github.io/posts/958416017/image-20240323202329871.png"><meta property="og:image" content="https://amostian.github.io/posts/958416017/image-20240323202247119.png"><meta property="og:image" content="https://amostian.github.io/posts/958416017/image-20240324003641277.png"><meta property="article:published_time" content="2024-03-22T01:23:08.000Z"><meta property="article:modified_time" content="2024-03-23T16:43:06.000Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/958416017/image-20240322093857097.png"><link rel="canonical" href="https://amostian.github.io/posts/958416017/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>5.动手学深度学习-pytorch深度计算 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">58</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">363</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/958416017/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">5.动手学深度学习-pytorch深度计算</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-03-22 09:23:08" itemprop="dateCreated datePublished" datetime="2024-03-22T09:23:08+08:00">2024-03-22</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-03-24 00:43:06" itemprop="dateModified" datetime="2024-03-24T00:43:06+08:00">2024-03-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">5.1k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>13 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h1 id="5-深度计算"><a href="#5-深度计算" class="headerlink" title="5. 深度计算"></a>5. 深度计算</h1><h2 id="5-1-层和块"><a href="#5-1-层和块" class="headerlink" title="5.1 层和块"></a>5.1 层和块</h2><h3 id="5-1-1-块的概念"><a href="#5-1-1-块的概念" class="headerlink" title="5.1.1 块的概念"></a>5.1.1 块的概念</h3><p>整个深度模型包含一些参数（所有组成层的参数集合），接受原始输入，生成输出。同样，每个层包含该层的参数，接受上一层提供的输出作为输入，并生成该层输出，这些参数根据下一层反向传播的信息被更新（误差反向传播）</p><ul><li>视觉领域，ResNet-152有数百层，这些层由 层组(groups of layers) 堆叠组成</li><li>其他领域，也有 层组 堆叠排列的类似架构</li></ul><p>因此将神经网络组成模块划分为 <em>块</em>（Module）， 块可描述单个层、多个层组成的 <em>层组</em> 或深度模型本身。</p><p><img src="/posts/958416017/image-20240322093857097.png" alt="image-20240322093857097"></p><h3 id="5-1-2-pytorc中的块"><a href="#5-1-2-pytorc中的块" class="headerlink" title="5.1.2 pytorc中的块"></a>5.1.2 pytorc中的块</h3><p>从编程实现角度，<em>块</em> 的概念就是 <em>类</em> 。块 <code>nn.Module</code> 的任何子类都必须定义 $输入\mapsto 输出$ 的前向传播函数 <code>forward(self,X)</code> ；对各层输入、输出参数的初始化（该层参数形状）</p><ul><li>反向传播函数用于计算梯度，但由于自动微分的引入，不需要考虑</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment"># functional 库封装常用函数库</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重点是这部分的理解</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment">#	tensor([[ 0.0343,  0.0264,  0.2505, -0.0243,  0.0945,  </span></span><br><span class="line"><span class="comment">#	         0.0012, -0.0141,  0.0666,-0.0547, -0.0667],</span></span><br><span class="line"><span class="comment">#	        [ 0.0772, -0.0274,  0.2638, -0.0191,  0.0394, </span></span><br><span class="line"><span class="comment">#	         -0.0324,  0.0102,  0.0707,-0.1481, -0.1031]], </span></span><br><span class="line"><span class="comment">#	       grad_fn=&lt;AddmmBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p><code>nn.Sequential</code> 构建了一个深度模型，层的顺序是参数传入的顺序</p><ul><li><p><code>nn.Sequential</code> ：是 <code>nn.Module</code> 的一个子类，维护 <em>块</em> 实例的有序列表（也是 <code>nn.Module</code> 的实例 / 子类实例 ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(nn.Sequential.__mro__)</span><br><span class="line">(&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.modules.container.Sequential&#x27;</span>&gt;, &lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.modules.module.Module&#x27;</span>&gt;, &lt;<span class="keyword">class</span> <span class="string">&#x27;object&#x27;</span>&gt;)</span><br></pre></td></tr></table></figure></li></ul><p><code>net(参数)</code> 调用深度模型：相当于 <code>net.__call__(X)</code> 的简写</p><h4 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h4><p>块提供的功能</p><ol><li>存储和访问前向传播计算所需的参数</li><li>根据需要初始化模型参数</li><li>接收输入数据：作为前向传播函数的参数</li><li>通过前向传播函数生成输出</li><li>计算输出关于输入的梯度：反向传播函数访问；通常自动微分完成</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 用模型参数声明层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 	子类实例化时也可以指定其他函数参数，例如模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 实例化两个全连接层</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="comment">#	以X作为输入， 计算带有激活函数的隐藏表示，并输出其未规范化的输出值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line">    </span><br><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment">#	tensor([[ 0.0669,  0.2202, -0.0912, -0.0064,  0.1474, </span></span><br><span class="line"><span class="comment">#	         -0.0577, -0.3006,  0.1256,-0.0280,  0.4040],</span></span><br><span class="line"><span class="comment">#	        [ 0.0545,  0.2591, -0.0297,  0.1141,  0.1887,</span></span><br><span class="line"><span class="comment">#	         0.0094, -0.2686,  0.0732,-0.0135,  0.3865]], </span></span><br><span class="line"><span class="comment">#	       grad_fn=&lt;AddmmBackward0&gt;)</span></span><br></pre></td></tr></table></figure><h4 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h4><blockquote><p><code>Sequential</code>的设计是为了把其他模块串起来</p></blockquote><ol><li><code>__init(self, *args)__</code> ：将块逐个追加到列表中</li><li><code>forward</code> ：前向传播：将输入按追加块的顺序前向传播</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。</span></span><br><span class="line">            <span class="comment"># _modules：保存module实例的有序字典实例；属于 OrderedDict 类</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            <span class="comment"># X是经过运算块后的返回值，相当于各层的输出</span></span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="comment"># 这里返回的前向输出</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>自定义有序字典类 <code>OrderedDict</code> 作用：在模型初始化过程中，pytorch从其实例 <code>_modules</code> 中查找需要初始化参数的子块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment">#	tensor([[ 2.2759e-01, -4.7003e-02,  4.2846e-01, -1.2546e-01,  1.5296e-01,</span></span><br><span class="line"><span class="comment">#			1.8972e-01,  9.7048e-02,  4.5479e-04, 	-3.7986e-02,  6.4842e-02],</span></span><br><span class="line"><span class="comment">#	        [ 2.7825e-01, -9.7517e-02,  4.8541e-01, -2.4519e-01, -8.4580e-02,</span></span><br><span class="line"><span class="comment">#			2.8538e-01,  3.6861e-02,  2.9411e-02, 	-1.0612e-01,  1.2620e-01]], grad_fn=&lt;AddmmBackward0&gt;)</span></span><br></pre></td></tr></table></figure><ul><li><code>nn.ReLU()</code> 内部调用了 <code>F.rele()</code> ，具有状态和可学习参数</li></ul><h5 id="块访问"><a href="#块访问" class="headerlink" title="块访问"></a>块访问</h5><p><code>nn.Sequential</code> 实例可以理解为一个 <code>nn.module</code> 类实例的有序列表，可以通过索引来访问模型的任一层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问顺序块，返回顺序块实例信息</span></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># MySequential(</span></span><br><span class="line"><span class="comment">#   (0): Linear(in_features=20, out_features=256, bias=True)</span></span><br><span class="line"><span class="comment">#   (1): ReLU()</span></span><br><span class="line"><span class="comment">#   (2): Linear(in_features=256, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回OrderedDict类实例，封装了该顺序块关联的块字典列表</span></span><br><span class="line"><span class="built_in">print</span>(net._modules)</span><br><span class="line"><span class="comment"># OrderedDict([(&#x27;0&#x27;, Linear(in_features=20, out_features=256, bias=True)), </span></span><br><span class="line"><span class="comment">#				(&#x27;1&#x27;, ReLU()), </span></span><br><span class="line"><span class="comment">#				(&#x27;2&#x27;, Linear(in_features=256, out_features=10, bias=True))])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据索引获取字典中的块实例</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>])<span class="comment">#nn.Sequential中实现了通过索引访问，与下列两种方式等价</span></span><br><span class="line"><span class="built_in">print</span>(net._modules[<span class="string">&#x27;2&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(net._modules.get(<span class="string">&#x27;2&#x27;</span>))</span><br><span class="line"><span class="comment">#	Linear类继承Module类</span></span><br><span class="line"><span class="comment">#			Linear(in_features=256, out_features=10, bias=True)</span></span><br></pre></td></tr></table></figure><h4 id="自定义块的特殊用法"><a href="#自定义块的特殊用法" class="headerlink" title="自定义块的特殊用法"></a>自定义块的特殊用法</h4><p><code>Sequential</code> 类允许我们定制深度网络架构，并提供很大灵活性。如：可以在前向传播中引入控制流或自定义处理</p><h5 id="层引入常数参数"><a href="#层引入常数参数" class="headerlink" title="层引入常数参数"></a>层引入常数参数</h5><p>若在某一层需要合并一些其他参数 <em>常数参数(constant param)</em> ：既不是上一层结果也不是可更新参数，如 $f(\mathbf{x};\mathbf{W})=c\mathbf{x}\mathbf{W}^T$ ，$c$ 就是常数参数，整个优化过程没有被更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># 1.计算线性层输出</span></span><br><span class="line">    <span class="comment"># 2.对第一次输出乘常量参数+1，再relu</span></span><br><span class="line">    <span class="comment"># 3.计算第二次输出的线性层输出</span></span><br><span class="line">    <span class="comment"># 4.控制流：若X每个元素绝对值的和&gt;1，则除2直至小于1</span></span><br><span class="line">    <span class="comment"># 5.返回标量，X.sum()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><h5 id="块组合"><a href="#块组合" class="headerlink" title="块组合"></a>块组合</h5><p><code>Sequential</code> 类允许我们定制深度网络架构，并提供很大灵活性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure><h3 id="5-1-3-操作效率"><a href="#5-1-3-操作效率" class="headerlink" title="5.1.3 操作效率"></a>5.1.3 操作效率</h3><p>在一个高性能的深度学习库中进行了大量的字典查找、 代码执行和许多其他的Python代码。 Python的问题<a target="_blank" rel="noopener" href="https://wiki.python.org/moin/GlobalInterpreterLock">全局解释器锁</a> 是众所周知的。 在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。</p><h2 id="5-2-参数管理"><a href="#5-2-参数管理" class="headerlink" title="5.2 参数管理"></a>5.2 参数管理</h2><p><code>Paramater</code> 类的实例是可优化的参数实例</p><p>在定义好模型架构并设置好超参数后，完成对 <strong>各层参数设置</strong> 后，就可以进行模型训练</p><ul><li>访问参数</li><li>参数初始化</li><li>不同 <em>块(module)</em> 间共享参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 2个数据4维特征</span></span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><h3 id="5-2-1-参数访问"><a href="#5-2-1-参数访问" class="headerlink" title="5.2.1 参数访问"></a>5.2.1 参数访问</h3><p><code>net.state_dict()</code> 返回块的所有参数信息组成的字典 <code>OrderedDict</code></p><ul><li>权重就是神经网络的状态，权重的一轮迭代就是网络状态的一次变化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用nn.Sequential，该类封装了一些按索引访问方法，支持列表式访问各层</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># net[2] 返回Linear类的块实例，即本层信息</span></span><br><span class="line"><span class="comment">#	Linear(in_features=8, out_features=1, bias=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Module有state_dict()：返回 a dictionary containing a whole state of the module</span></span><br><span class="line"><span class="built_in">print</span>(net.state_dict())</span><br><span class="line"><span class="comment">#OrderedDict([</span></span><br><span class="line"><span class="comment">#    (&#x27;0.weight&#x27;, tensor([[-0.3076, -0.4758,  0.1081, -0.2304],</span></span><br><span class="line"><span class="comment">#        [ 0.4211,  0.4465, -0.3960, -0.1722],</span></span><br><span class="line"><span class="comment">#        [ 0.2387,  0.3205,  0.2718,  0.2265],</span></span><br><span class="line"><span class="comment">#        [ 0.2729,  0.1801, -0.1720, -0.0440],</span></span><br><span class="line"><span class="comment">#        [-0.1461, -0.1271,  0.1468,  0.3713],</span></span><br><span class="line"><span class="comment">#        [-0.4266,  0.4993,  0.1876, -0.3648],</span></span><br><span class="line"><span class="comment">#        [-0.3004,  0.0743, -0.3056,  0.0593],</span></span><br><span class="line"><span class="comment">#        [ 0.0245, -0.1389, -0.1488, -0.4814]])), </span></span><br><span class="line"><span class="comment">#    (&#x27;0.bias&#x27;, tensor([-0.2036, -0.0883,  0.4036, -0.3335, -0.4430,  0.2864, -0.1805, -0.1634])), </span></span><br><span class="line"><span class="comment">#    (&#x27;2.weight&#x27;, tensor([[ 0.2721,  0.2518,  0.3448,  0.2209,  0.1964, -0.1971,  0.0200,  0.2595]])), </span></span><br><span class="line"><span class="comment">#    (&#x27;2.bias&#x27;, tensor([0.1401]))])</span></span><br><span class="line"></span><br><span class="line">net.state_dict()[<span class="string">&#x27;0.bias&#x27;</span>]</span><br><span class="line"><span class="comment"># tensor([-0.0771, -0.1678, -0.3164, -0.3792,  0.1848, -0.2182,  0.3182, -0.1454])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Linear类继承了Module类</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="comment"># 	OrderedDict([(&#x27;weight&#x27;, tensor([[-0.2372, -0.2587, -0.2270, -0.1828, -0.1542, -0.2847,  0.1730, -0.1300]])), (&#x27;bias&#x27;, tensor([-0.1647]))])</span></span><br></pre></td></tr></table></figure><p>Linear类实例的输出是一个字典，键包括：权重，偏置</p><ul><li>键唯一标识每个参数</li></ul><h4 id="参数类"><a href="#参数类" class="headerlink" title="参数类"></a>参数类</h4><p>每个参数都被表示为一个参数类实例（<code>&lt;class &#39;torch.nn.parameter.Parameter&#39;&gt;</code>），属性有：</p><ul><li>值：<code>data</code></li><li>梯度：<code>grad</code></li><li>其他额外信息</li></ul><p>这些参数信息可以被统一管理和更新，所以需要 <code>Parameter</code> 类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="comment"># &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回的是参数类实例 参数名为bias的实例</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="comment"># 	Parameter containing:</span></span><br><span class="line"><span class="comment"># 	tensor([-0.1647], requires_grad=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只返回bias属性的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="comment">#	tensor([-0.1647])</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.grad)</span><br></pre></td></tr></table></figure><h4 id="一次性访问所有参数net-named-parameters"><a href="#一次性访问所有参数net-named-parameters" class="headerlink" title="一次性访问所有参数net.named_parameters()"></a>一次性访问所有参数<code>net.named_parameters()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># *[...]参数解包</span></span><br><span class="line"><span class="comment">#	接收元组的list，并将其作为两个单独的参数分别输出</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="comment">#(&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class="line"><span class="comment"># param是Parameter类的实例</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"><span class="comment">#(&#x27;0.weight&#x27;, torch.Size([8, 4])) (&#x27;0.bias&#x27;, torch.Size([8])) (&#x27;2.weight&#x27;,torch.Size([1, 8])) (&#x27;2.bias&#x27;, torch.Size([1]))</span></span><br></pre></td></tr></table></figure><h4 id="从嵌套块收集参数"><a href="#从嵌套块收集参数" class="headerlink" title="从嵌套块收集参数"></a>从嵌套块收集参数</h4><p>深度模型中，层之间存在嵌套关系，也可以通过嵌套列表索引分层访问块参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># f&#x27;&#123;i&#125;&#x27; 格式化字符串，将&#123;i&#125;中的变量值替换</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rgnet)</span><br></pre></td></tr></table></figure><p><img src="/posts/958416017/image-20240322175345411.png" alt="image-20240322175345411"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rgnet[0]=block2()</span></span><br><span class="line"><span class="comment"># rgnet[0][1]=block2()[1]=(block 1)</span></span><br><span class="line"><span class="comment"># rgnet[0][1][0]=block2()[1][0]=(block 1)[0]=Linear(in_features=4,out_feartures=8,bias=True)</span></span><br><span class="line"><span class="comment"># rgnet[0][1][0].bias=tensor(8)</span></span><br><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br></pre></td></tr></table></figure><h3 id="5-2-2-参数初始化"><a href="#5-2-2-参数初始化" class="headerlink" title="5.2.2 参数初始化"></a>5.2.2 参数初始化</h3><p>pytorch提供默认随机初始化，也允许通过重写自定义初始化方法</p><ul><li>默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的——Xavier初始化</li></ul><h4 id="预置初始化"><a href="#预置初始化" class="headerlink" title="预置初始化"></a>预置初始化</h4><p><code>nn.init</code> 模块提供类很多预置的初始化方法</p><ul><li>正态分布</li><li>全0，全1</li><li>常量</li><li>Xavier初始化</li></ul><p><strong>正态分布、全0/全1</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">        </span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><code>net.apply(fn)</code> 递归地将一个函数应用到神经网络中的每个子模块上，包括其本身</p><ul><li>需要定义一个函数，该函数接收一个模块作为参数，并对该模块执行自定义操作</li></ul><p><strong>常量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># (tensor([1., 1., 1., 1.]), tensor(0.))</span></span><br></pre></td></tr></table></figure><p><strong>Xavier初始化</strong></p><ul><li><code>xavier_normal</code></li><li><code>xavier_normal_</code> ：这个是更新后的版本，在计算时会考虑到ReLU激活函数的特性，以确保网络在初始化时的激活值分布更加合理</li><li><code>xavier_uniform</code></li><li><code>xavier_uniform_</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(init_xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure><h4 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="自定义初始化"></a>自定义初始化</h4><script type="math/tex;mode=display">\begin{aligned}
    w \sim \begin{cases}
        U(5, 10) & \text{ 可能性 } \frac{1}{4} \\
            0    & \text{ 可能性 } \frac{1}{2} \\
        U(-10, -5) & \text{ 可能性 } \frac{1}{4}
    \end{cases}
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>也可以在任意位置直接修改参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># tensor([42.0000, -5.2469,  9.0045,  1.0000])</span></span><br></pre></td></tr></table></figure><h3 id="5-2-3-参数共享"><a href="#5-2-3-参数共享" class="headerlink" title="5.2.3 参数共享"></a>5.2.3 参数共享</h3><p>在层之间共享权重，实质上共享层引用是同一个参数矩阵的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># tensor([True, True, True, True, True, True, True, True])</span></span><br><span class="line"><span class="comment"># tensor([True, True, True, True, True, True, True, True])</span></span><br></pre></td></tr></table></figure><p>net有4个隐藏层，第二个隐藏层和第三个隐藏层都由shared定义</p><ul><li>作用：由于模型参数包含梯度，因此在反向传播期间参数共享层的梯度会加在一起</li></ul><h2 id="5-3-自定义层"><a href="#5-3-自定义层" class="headerlink" title="5.3 自定义层"></a>5.3 自定义层</h2><p>与自定义块类似，只需要继承 <code>nn.module</code> ，并实现 <code>__init__(self)</code> 和 <code>forward(self,X)</code> 即可</p><h3 id="5-3-1-不带参数的层"><a href="#5-3-1-不带参数的层" class="headerlink" title="5.3.1 不带参数的层"></a>5.3.1 不带参数的层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br><span class="line">    </span><br><span class="line">layer = CenteredLayer()</span><br><span class="line"><span class="comment"># 将数据 X=[1,2,3,4,5] 传入该层，会输出前向传播的值</span></span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br><span class="line"><span class="comment">#	tensor([-2., -1.,  0.,  1.,  2.])</span></span><br></pre></td></tr></table></figure><p>只要继承了 <code>nn.Module</code> 类的块都可以应用到更复杂的模型中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br><span class="line"></span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">Y.mean()</span><br><span class="line"><span class="comment">#	tensor(7.4506e-09, grad_fn=&lt;MeanBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>不管第1隐藏层输出是什么，$\overline{h_1-\overline{h}_1}=\frac{\sum\limits (h_1-\overline{h}_1)}{\vert h_1\vert}=\frac{\sum h_1-\vert h_1\vert\overline{h_1}}{\vert h_1\vert}=0$ ，即 <code>Y.mean()</code> 应该为0 ，在计算机中表示为一个非常小的数</p><h3 id="5-3-2-带参数的层"><a href="#5-3-2-带参数的层" class="headerlink" title="5.3.2 带参数的层"></a>5.3.2 带参数的层</h3><p>接受层的输入与输出形状</p><p>由于参数在torch中以 <code>Paramater</code> 类封装便于计算，所以在 <code>__init__()</code> 中将参数定义为该类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line">    </span><br><span class="line">linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">linear.weight</span><br></pre></td></tr></table></figure><p>通过 <code>nn.Paramater</code> 实例化自定义参数的形状与定义时相同，即 <code>p=nn.Parameter(torch.randn(5, 3))</code> ，则 <code>p.shape=torch.Size([5,3])</code></p><ul><li><img src="/posts/958416017/image-20240323202329871.png" alt="image-20240323202329871"></li></ul><p>通过 <code>net=nn.Linear(5,3)</code> 实例化 <em>块</em> ，其参数 <code>ne.weight.shape=torch.Size([3, 5])</code></p><ul><li><img src="/posts/958416017/image-20240323202247119.png" alt="image-20240323202247119"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">#tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0.]])</span></span><br></pre></td></tr></table></figure><p><strong>也可以将自定义应用于网络模型构建</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br><span class="line"><span class="comment">#tensor([[0.],</span></span><br><span class="line"><span class="comment">#        [0.]])</span></span><br></pre></td></tr></table></figure><h3 id="5-3-3-自定义添加层—net-add"><a href="#5-3-3-自定义添加层—net-add" class="headerlink" title="5.3.3 自定义添加层—net.add()"></a>5.3.3 自定义添加层—net.add()</h3><p><code>net.add_module([key],[块声明])</code> ：向神经网络中添加模块</p><ul><li><code>net.add_module(&#39;layer1&#39;, nn.Linear(10, 20))</code></li></ul><h2 id="5-4-读写文件"><a href="#5-4-读写文件" class="headerlink" title="5.4 读写文件"></a>5.4 读写文件</h2><p>有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果</p><h3 id="5-4-1-加载和保存"><a href="#5-4-1-加载和保存" class="headerlink" title="5.4.1 加载和保存"></a>5.4.1 加载和保存</h3><p>对于基本类型：<code>tensor</code> ，<code>tensor[]</code> ，<code>mydict=&#123;&#39;a&#39;:a,&#39;b&#39;:b&#125;</code> 直接调用 <code>torch.load()</code> 和 <code>torch.save()</code> 进行读写，参数为要保存的变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量从内存保存到文件</span></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"><span class="comment"># 张量从文件取到内存</span></span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量列表保存与取回</span></span><br><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y],<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量字典的保存与取回</span></span><br><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="5-4-2-保存和加载模型参数"><a href="#5-4-2-保存和加载模型参数" class="headerlink" title="5.4.2 保存和加载模型参数"></a>5.4.2 保存和加载模型参数</h3><p>深度学习框架提供了内置函数来保存和加载整个网络</p><p>但并不保存整个模型架构，为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure><p><strong>保存参数</strong> ：将模型 <code>state_dict()</code> 输出的参数字典 <code>OrderedDict</code> 实例，保存在 mlp.params 的文件中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>恢复模型</strong> ：读取文件，并转换为 <code>OrderedDict</code> 类的实例作为参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment">#	MLP(</span></span><br><span class="line"><span class="comment">#	  (hidden): Linear(in_features=20, out_features=256, bias=True)</span></span><br><span class="line"><span class="comment">#	  (output): Linear(in_features=256, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment">#	)</span></span><br></pre></td></tr></table></figure><h2 id="5-5-GPU"><a href="#5-5-GPU" class="headerlink" title="5.5 GPU"></a>5.5 GPU</h2><p>使用GPU来进行机器学习，因为单个GPU相对运行速度快</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看nvidia驱动程序及CUDA</span></span><br><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>在torch中，每个数据都归属于一个设备 (device)，称其为环境（context）。默认情况，所有的变量和计算都分配给CPU</p><p><strong>设备间进行数据传输非常不划算</strong> ，在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收），然后才能继续进行更多的操作。</p><ul><li>根据经验，多个小操作比一个大操作糟糕得多。 此外，一次执行几个操作比代码中散布的许多单个操作要好得多。 如果一个设备必须等待另一个设备才能执行其他操作， 那么这样的操作可能会阻塞。</li><li>当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。</li></ul><p>在创建、使用变量是要合理分配环境，最大限度地减少设备间的数据传输时间</p><h3 id="5-5-1-设备查看"><a href="#5-5-1-设备查看" class="headerlink" title="5.5.1 设备查看"></a>5.5.1 设备查看</h3><p>GPU设备仅代表一个卡和相应的显存，如果有多个 GPU，使用 <code>&#39;cuda:0&#39;</code>，<code>&#39;cuda:1&#39;</code> 分别表示第1块和第2块GPU，<code>&#39;cuda&#39;</code> 默认返回第1块GPU</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.device(<span class="string">&#x27;cuda&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"><span class="comment">#	(device(type=&#x27;cpu&#x27;), device(type=&#x27;cuda&#x27;), device(type=&#x27;cuda&#x27;, index=1))</span></span><br></pre></td></tr></table></figure><p>在torch中，通过 <code>torch.device([设备字符串])</code> 查看在运算时可使用的设备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看设备数量</span></span><br><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure><h3 id="5-5-2-设备获取"><a href="#5-5-2-设备获取" class="headerlink" title="5.5.2 设备获取"></a>5.5.2 设备获取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尝试将第i块gpu拿出来返回</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span></span><br><span class="line">    devices = [torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br><span class="line"><span class="comment">#	(device(type=&#x27;cuda&#x27;, index=0),</span></span><br><span class="line"><span class="comment">#	 device(type=&#x27;cpu&#x27;),</span></span><br><span class="line"><span class="comment">#	 [device(type=&#x27;cuda&#x27;, index=0)])</span></span><br></pre></td></tr></table></figure><h3 id="5-5-3-使用GPU存储变量"><a href="#5-5-3-使用GPU存储变量" class="headerlink" title="5.5.3 使用GPU存储变量"></a>5.5.3 使用GPU存储变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"><span class="comment">#	device(type=&#x27;cpu&#x27;)</span></span><br></pre></td></tr></table></figure><p>可见，不指定设备的前提下，变量默认存储在CPU上</p><h4 id="将变量存储在GPU"><a href="#将变量存储在GPU" class="headerlink" title="将变量存储在GPU"></a>将变量存储在GPU</h4><p>若对两个张量运算，首先要确定两个张量存储在同一个设备上</p><h5 id="创建时指定设备"><a href="#创建时指定设备" class="headerlink" title="创建时指定设备"></a>创建时指定设备</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">X</span><br><span class="line"><span class="comment">#	tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#	        [1., 1., 1.]], device=&#x27;cuda:0&#x27;)</span></span><br></pre></td></tr></table></figure><h5 id="设备间复制"><a href="#设备间复制" class="headerlink" title="设备间复制"></a>设备间复制</h5><p><img src="/posts/958416017/image-20240324003641277.png" alt="image-20240324003641277"></p><p>X与Y需要再同一个设备上才可以计算，否则由于在同一设备上找不到数据导致异常。 若不在同一GPU上，需要将变量复制到同一GPU上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将X移动到GPU 1上作为变量Z</span></span><br><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="comment">#tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#        [1., 1., 1.]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="comment">#tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#        [1., 1., 1.]], device=&#x27;cuda:1&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在GPU1上进行加法</span></span><br><span class="line">Y + Z</span><br></pre></td></tr></table></figure><p>假设变量 <code>Z</code> 已经在第2个GPU上，仍会返回 <code>Z</code> ，并不会复制并分配新内存</p><h4 id="神经网络与GPU"><a href="#神经网络与GPU" class="headerlink" title="神经网络与GPU"></a>神经网络与GPU</h4><p>神经网络模型也可以指定设备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].weight.data.device</span><br><span class="line"><span class="comment"># device(type=&#x27;cuda&#x27;, index=0)</span></span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/958416017/" title="5.动手学深度学习-pytorch深度计算">https://amostian.github.io/posts/958416017/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/2645721241/" rel="prev" title="4.动手学深度学习-模型评估"><i class="fa fa-chevron-left"></i> 4.动手学深度学习-模型评估</a></div><div class="post-nav-item"><a href="/posts/3273175593/" rel="next" title="6.动手学深度学习-卷积神经网络">6.动手学深度学习-卷积神经网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#5-%E6%B7%B1%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-text">5. 深度计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E5%B1%82%E5%92%8C%E5%9D%97"><span class="nav-text">5.1 层和块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-1-%E5%9D%97%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-text">5.1.1 块的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-2-pytorc%E4%B8%AD%E7%9A%84%E5%9D%97"><span class="nav-text">5.1.2 pytorc中的块</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%97"><span class="nav-text">自定义块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A1%BA%E5%BA%8F%E5%9D%97"><span class="nav-text">顺序块</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9D%97%E8%AE%BF%E9%97%AE"><span class="nav-text">块访问</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%97%E7%9A%84%E7%89%B9%E6%AE%8A%E7%94%A8%E6%B3%95"><span class="nav-text">自定义块的特殊用法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B1%82%E5%BC%95%E5%85%A5%E5%B8%B8%E6%95%B0%E5%8F%82%E6%95%B0"><span class="nav-text">层引入常数参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9D%97%E7%BB%84%E5%90%88"><span class="nav-text">块组合</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-3-%E6%93%8D%E4%BD%9C%E6%95%88%E7%8E%87"><span class="nav-text">5.1.3 操作效率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="nav-text">5.2 参数管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="nav-text">5.2.1 参数访问</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%B1%BB"><span class="nav-text">参数类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E6%AC%A1%E6%80%A7%E8%AE%BF%E9%97%AE%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0net-named-parameters"><span class="nav-text">一次性访问所有参数net.named_parameters()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E5%B5%8C%E5%A5%97%E5%9D%97%E6%94%B6%E9%9B%86%E5%8F%82%E6%95%B0"><span class="nav-text">从嵌套块收集参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">5.2.2 参数初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">预置初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">自定义初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3-%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="nav-text">5.2.3 参数共享</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-text">5.3 自定义层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-%E4%B8%8D%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="nav-text">5.3.1 不带参数的层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-2-%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="nav-text">5.3.2 带参数的层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-3-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B7%BB%E5%8A%A0%E5%B1%82%E2%80%94net-add"><span class="nav-text">5.3.3 自定义添加层—net.add()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-text">5.4 读写文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-1-%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98"><span class="nav-text">5.4.1 加载和保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-2-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-text">5.4.2 保存和加载模型参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-GPU"><span class="nav-text">5.5 GPU</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-1-%E8%AE%BE%E5%A4%87%E6%9F%A5%E7%9C%8B"><span class="nav-text">5.5.1 设备查看</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-2-%E8%AE%BE%E5%A4%87%E8%8E%B7%E5%8F%96"><span class="nav-text">5.5.2 设备获取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-3-%E4%BD%BF%E7%94%A8GPU%E5%AD%98%E5%82%A8%E5%8F%98%E9%87%8F"><span class="nav-text">5.5.3 使用GPU存储变量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%86%E5%8F%98%E9%87%8F%E5%AD%98%E5%82%A8%E5%9C%A8GPU"><span class="nav-text">将变量存储在GPU</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%97%B6%E6%8C%87%E5%AE%9A%E8%AE%BE%E5%A4%87"><span class="nav-text">创建时指定设备</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E9%97%B4%E5%A4%8D%E5%88%B6"><span class="nav-text">设备间复制</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8EGPU"><span class="nav-text">神经网络与GPU</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">363</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">58</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">74</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">879.2k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">36:47</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>