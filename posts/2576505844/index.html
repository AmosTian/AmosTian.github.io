<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="Ceph文档-存储集群-综述"><meta property="og:url" content="https://amostian.github.io/posts/2576505844/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-3bf285dacff79a5fe5eea8dd9ca2bd41baa26061.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-06640530bbb7bfb50eff21cedb8ffb47189686e9.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-d93220ac740fbc1df6152f91b390a95f9b36f9c5.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-711402e8fde8ea6a9ae1a39bb27a1b5431a0d8de.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-af0ce2210eafb917a47884a8edbcbc94b581639b.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-445812455304f21ffe56440cf81649d3835b9a4d.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-15d56b22a50b6b8618cf5945174c6d8fef55be6a.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-0873bb9405742345397fa2e085b6ac453aa3f975.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/ditaa-49d872aa7a347dee2861e634eebbb9b8c00a4d45.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/image-20240622120533466.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/image-20240622121639823.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/image-20240622121718061.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/image-20240622121744884.png"><meta property="og:image" content="https://amostian.github.io/posts/2576505844/image-20240622122552815.png"><meta property="article:published_time" content="2024-06-23T03:53:28.000Z"><meta property="article:modified_time" content="2024-10-04T11:29:41.213Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="分布式存储"><meta property="article:tag" content="分布式"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/2576505844/ditaa-3bf285dacff79a5fe5eea8dd9ca2bd41baa26061.png"><link rel="canonical" href="https://amostian.github.io/posts/2576505844/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Ceph文档-存储集群-综述 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">65</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">82</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">220</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/2576505844/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Ceph文档-存储集群-综述</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-06-23 11:53:28" itemprop="dateCreated datePublished" datetime="2024-06-23T11:53:28+08:00">2024-06-23</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-10-04 19:29:41" itemprop="dateModified" datetime="2024-10-04T19:29:41+08:00">2024-10-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F/" itemprop="url" rel="index"><span itemprop="name">分布式</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">分布式存储</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">23.5k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>39 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="host"><a href="#host" class="headerlink" title="host"></a>host</h3><p>Ceph <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/start/hardware-recommendations/">硬件推荐</a> 提供了硬件选型指南。</p><p>单个Ceph节点可以运行多种Ceph进程，具有多个硬盘驱动器的单节点通常为每个硬盘驱动器启动一个Ceph-osd</p><p>理想情况下，每个节点分配为一个特定类型的进程。如：一些节点只运行ceph-osd进程，其他节点运行 ceph-mds 进程</p><p>每个节点都需要被命名，节点名称在 <code>host</code> 参数配置项中获取与修改。mon也能在 <code>addr</code> 中指定网络地址和端口。一个基本的配置文件通常仅指定mon进程每个实例的最小配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">mon_initial_members = ceph1</span><br><span class="line">mon_host = 10.0.0.1</span><br></pre></td></tr></table></figure><p><code>host</code> 的参数配置值可以是节点的短名称（不是域名或ip），通过 <code>hostname -s</code> 可以获取节点的短名称</p><p>除非手动部署Ceph，否则不要修改 <code>host</code> 参数配置值，在部署集群时使用部署工具，可以为集群自动填充这个参数配置项</p><h3 id="网络相关"><a href="#网络相关" class="headerlink" title="网络相关"></a>网络相关</h3><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/network-config-ref/">网络配置</a></p><h3 id="monitors"><a href="#monitors" class="headerlink" title="monitors"></a>monitors</h3><p>Ceph集群的生产环境至少需要有三个mon进程，以确保高可用性。</p><p>三个ceph-mon进程可以确保Paxos算法能确定哪个 Ceph-Cluster-Map 是最新的，通过对 <em>quorum</em> 节点的轮询投票决定。</p><blockquote><p>您可以使用单个监视器部署 Ceph，但如果实例发生故障，缺少其他监视器可能会中断数据服务的可用性。</p></blockquote><p><strong>监听端口</strong>：采用新的v2协议的 mon 节点通常监听 3300 端口；旧的 v1 协议的 mon 节点监听 6789 端口</p><p><strong>数据存储路径</strong>：默认情况下，mon节点将数据存储在 <code>/var/lib/ceph/mon/$cluster-$id</code> 下，</p><ul><li>若手动部署或使用部署工具也必须创建相应的目录<ul><li>如：<em>$cluster=ceph</em> 且 <em>$id=node1</em>的mon节点的数据存储路径为 <code>/var/lib/ceph/mon/ceph-node1</code></li></ul></li></ul><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/mon-config-ref/">mon节点详细配置</a></p><h3 id="身份认证"><a href="#身份认证" class="headerlink" title="身份认证"></a>身份认证</h3><p>身份验证模块在Ceph配置文件的 <em>[global]</em> section中显示启动或禁用，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/auth-config-ref/">鉴权详细配置</a></p><h3 id="OSDs"><a href="#OSDs" class="headerlink" title="OSDs"></a>OSDs</h3><p>Ceph集群的生产环境中，典型编排是一个节点的一个存储设备对应一个OSD进程运行Filestore，BlueStore是当前默认的存储后端</p><h4 id="Filestore需指定journal大小"><a href="#Filestore需指定journal大小" class="headerlink" title="Filestore需指定journal大小"></a>Filestore需指定journal大小</h4><p>当使用Filestore时，必须指定日志大小。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[osd]</span><br><span class="line">osd_journal_size = 10000</span><br><span class="line"></span><br><span class="line">[osd.0]</span><br><span class="line">host = &#123;hostname&#125; #manual deployments only.</span><br></pre></td></tr></table></figure><h4 id="数据存储路径"><a href="#数据存储路径" class="headerlink" title="数据存储路径"></a>数据存储路径</h4><p>默认情况下，Ceph希望将数据存储在 <code>/var/lib/ceph/osd/$cluster-$id</code> 路径下</p><p>如：集群名为 <em>ceph</em> ，osd进程id为 <em>0</em> ，该OSD进程数据存储路径为 <code>/var/lib/ceph/osd/ceph-0</code></p><p>可以通过 <code>osd_data</code> 自定义数据存储路径，但不推荐修改默认位置</p><p>OSD的数据存储路径不能是系统盘，需要使用系统盘和守护进程的设备外的设备，并将该设备盘挂载到 <code>osd_data</code> 指定的目录下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 创建数据存储目录</span></span><br><span class="line">ssh &#123;osd-host&#125;</span><br><span class="line">sudo mkdir /var/lib/ceph/osd/ceph-&#123;osd-number&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 数据盘挂载</span></span><br><span class="line">ssh &#123;new-osd-host&#125;</span><br><span class="line">sudo mkfs -t &#123;fstype&#125; /dev/&#123;disk&#125;</span><br><span class="line">sudo mount -o user_xattr /dev/&#123;disk&#125; /var/lib/ceph/osd/ceph-&#123;osd-number&#125;</span><br></pre></td></tr></table></figure><ul><li><code>fstype</code> ：推荐使用 <em>xfs</em> 文件系统，不推荐 <em>btrfs</em> 和 <em>ext4</em> 文件系统</li></ul><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/osd-config-ref">OSD Config Reference</a>.</p><h3 id="心跳"><a href="#心跳" class="headerlink" title="心跳"></a>心跳</h3><p>在运行期间，CephOSD进程会检查其他OSD进程的状态，并将结果报告给mon节点，对于存在延迟的集群网络，需要修改默认的参数配置</p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/mon-osd-interaction">Configuring Monitor/OSD Interaction</a></p><h3 id="日志或调试"><a href="#日志或调试" class="headerlink" title="日志或调试"></a>日志或调试</h3><p>Ceph遇到问题时，需要用到Ceph的日志记录和调试功能</p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/troubleshooting/log-and-debug">Debugging and Logging</a>.</p><h3 id="ceph-conf-示例"><a href="#ceph-conf-示例" class="headerlink" title="ceph.conf 示例"></a>ceph.conf 示例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">fsid = &#123;cluster-id&#125;</span><br><span class="line">mon_initial_ members = &#123;hostname&#125;[, &#123;hostname&#125;]</span><br><span class="line">mon_host = &#123;ip-address&#125;[, &#123;ip-address&#125;]</span><br><span class="line"></span><br><span class="line"># 所有集群有一个面向用户的前端公共网络</span><br><span class="line"># 如果有两个网络接口可用，则应该配置一个私有集群网络，用于RADOS的对象复制、心跳检测、备份、恢复等</span><br><span class="line">public_network = &#123;network&#125;[, &#123;network&#125;]</span><br><span class="line">#cluster_network = &#123;network&#125;[, &#123;network&#125;] </span><br><span class="line"></span><br><span class="line"># 集群默认需要安全验证</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line"></span><br><span class="line"># 选择</span><br><span class="line"># 设置合理的journals数，池副本数和PG数</span><br><span class="line">#and placement groups.</span><br><span class="line">osd_journal_size = &#123;n&#125;</span><br><span class="line">osd_pool_default_size = &#123;n&#125;  # Write an object n times.</span><br><span class="line">osd_pool_default_min size = &#123;n&#125; # Allow writing n copy in a degraded state.</span><br><span class="line">osd_pool_default_pg num = &#123;n&#125;</span><br><span class="line">osd_pool_default_pgp num = &#123;n&#125;</span><br><span class="line"></span><br><span class="line">#选择合适的CRUSH leaf 类型</span><br><span class="line">#0 for a 1-node cluster.</span><br><span class="line">#1 for a multi node cluster in a single rack</span><br><span class="line">#2 for a multi node, multi chassis cluster with multiple hosts in a chassis</span><br><span class="line">#3 for a multi node cluster with hosts across racks, etc.</span><br><span class="line">osd_crush_chooseleaf_type = &#123;n&#125;</span><br></pre></td></tr></table></figure><h2 id="Ceph网络参数配置项"><a href="#Ceph网络参数配置项" class="headerlink" title="Ceph网络参数配置项"></a>Ceph网络参数配置项</h2><p>网络配置对于高性能Ceph存储集群至关重要</p><p><strong>网络配置的重要性</strong> Ceph存储集群不代表Ceph客户端执行请求路由或调度，Ceph客户端直接向目标OSD进程发送请求，OSD进程代表Ceph客户端执行数据复制，这也意味着副本策略和其他因素会给Ceph存储集群的网络带来额外的负载。</p><p><strong>配置 cluster network必要性</strong></p><p>Ceph的快速配置仅提供简单的Ceph配置文件，仅设置了节点的 <code>ip</code> 和 <code>host</code> 。除非指定集群网络，否则Ceph会将 <em>public network</em> 作为 <em>cluster network</em> 。</p><p>虽然只设置 <em>public network</em> Ceph集群就能工作，但重负载集群设置 <em>cluster network</em> 后看到显著的性能提升。所以，建议一个Ceph存储集群使用两个网络：<em>public network</em> （客户端，前端）和 <em>cluster network</em> （专用、数据复制、后端）。</p><p>集群网络可以用于处理OSD心跳、OSD间的对象复制和数据恢复的流量。一般将 <code>192.168.0.0</code> 或 <code>10.0.0.0</code> 作为集群网络</p><p>虽然这种方法会使网络配置变得复杂(每个 Ceph 节点将需要有多个网络接口或 VLAN)，且通常不会对整体性能产生很大影响。因此，我们建议为了恢复能力和容量，双 NIC 系统要么 <em>active/active</em> 绑定这些接口，要么实施<em>layer 3</em> 多路径策略，例如： FRR。</p><p><img src="/posts/2576505844/ditaa-3bf285dacff79a5fe5eea8dd9ca2bd41baa26061.png" alt="img"></p><h3 id="IP-Table（端口防火墙相关的）"><a href="#IP-Table（端口防火墙相关的）" class="headerlink" title="IP Table（端口防火墙相关的）"></a>IP Table（端口防火墙相关的）</h3><blockquote><p>IP Table 是 Linux 内核中用于实现防火墙功能的工具</p><p>IP Table 提供了多种表（tables）和链（chains）来定义不同的防火墙规则。</p><p>表用于分类规则，而链则用于指定规则的执行顺序。</p><p>例如，在 filter 表中，有一个名为 INPUT 的链用于过滤进入本地的数据包，一个名为 OUTPUT 的链用于过滤本地生成的数据包，还有一个名为 FORWARD 的链用于过滤经过路由器的数据包。</p></blockquote><p>默认情况下，Ceph各类型进程会绑定到 <code>6800:7568</code> 范围的端口。当然，可以自行指定这些端口。</p><p>首先检查 <em>iptables</em></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -L</span><br></pre></td></tr></table></figure><ul><li><p>某些 Linux 发行版包含拒绝来自所有网络接口的所有入站请求（SSH 除外）的规则。例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">REJECT all -- anywhere anywhere reject-with icmp-host-prohibited</span><br></pre></td></tr></table></figure><p>需要在公共网络和集群网络上删除这些规则，并为待使用端口设定合适的规则</p></li></ul><h4 id="monitor-的IP-Table"><a href="#monitor-的IP-Table" class="headerlink" title="monitor 的IP Table"></a>monitor 的IP Table</h4><p>ceph-mon进程默认监听 <code>3300</code> 和 <code>6789</code> 端口。此外，ceph-mon通常在公共网络上运行。</p><p>当您使用下面的示例添加规则时，请确保将 {iface} 替换为公共网络接口（例如 eth0、eth1 等），将 {ip-address} 替换为公共网络的 IP 地址，将 {netmask} 替换与公共网络的网络掩码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -A INPUT -i &#123;iface&#125; -p tcp -s &#123;ip-address&#125;/&#123;netmask&#125; --dport 6789 -j ACCEPT</span><br></pre></td></tr></table></figure><h4 id="MDS和MGR的IP-Table"><a href="#MDS和MGR的IP-Table" class="headerlink" title="MDS和MGR的IP Table"></a>MDS和MGR的IP Table</h4><p>MDS进程和MGR进程监听公共网络上从 <code>6800</code> 开始的第一个可用端口</p><p>如果您在同一主机上运行多个 OSD 或 MDS，或者在不释放端口情况下重启进程时，新启动的进程将自动绑定到更高的端口。所以需要打开 <code>6800-7568</code> 的所有端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -A INPUT -i &#123;iface&#125; -m multiport -p tcp -s &#123;ip-address&#125;/&#123;netmask&#125; --dports 6800:7568 -j ACCEPT</span><br></pre></td></tr></table></figure><h4 id="OSD的IP-Table"><a href="#OSD的IP-Table" class="headerlink" title="OSD的IP Table"></a>OSD的IP Table</h4><p>默认情况，OSD进程绑定到Ceph节点上从6800开始的第一个可用端口，</p><p>如果您在同一主机上运行多个 OSD 或 MDS，或者在不释放端口情况下重启进程时，新启动的进程将自动绑定到更高的端口。所以需要打开 <code>6800-7568</code> 的所有端口</p><p><strong>Ceph 主机上的每个 Ceph OSD 守护进程最多使用四个端口</strong></p><ul><li>1个用于与客户端和mon节点交互</li><li>1个用于与其他OSD进程发送数据</li><li>2个用于心跳监测（对每个接口的心跳监测）</li></ul><p><img src="/posts/2576505844/ditaa-06640530bbb7bfb50eff21cedb8ffb47189686e9.png" alt="img"></p><p>如果单独设置 <em>public network</em> 和 <em>cluster network</em> ，则必须为两种网络都添加规则，因为客户端使用公共网络与OSD通信，与其他OSD进程的通信需要使用集群网络</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">公网、 内网都要打开</span></span><br><span class="line">sudo iptables -A INPUT -i &#123;iface&#125;  -m multiport -p tcp -s &#123;ip-address&#125;/&#123;netmask&#125; --dports 6800:7568 -j ACCEPT</span><br></pre></td></tr></table></figure><h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>在ceph配置文件的 <code>[global]</code> section中进行网络配置，修改之后只需要重启相应的进程即可动态绑定网络。</p><h4 id="公共网络"><a href="#公共网络" class="headerlink" title="公共网络"></a>公共网络</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">        # ... elided configuration</span><br><span class="line">        public_network = &#123;public-network/netmask&#125;</span><br></pre></td></tr></table></figure><h4 id="集群网络"><a href="#集群网络" class="headerlink" title="集群网络"></a>集群网络</h4><p>如果声明集群网络，OSD 将通过集群网络路由心跳、对象复制和恢复流量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">        # ... elided configuration</span><br><span class="line">        cluster_network = &#123;cluster-network/netmask&#125;</span><br></pre></td></tr></table></figure><p>为了增加安全性，我们希望无法从公共网络或 Internet 访问集群网络。</p><h3 id="Ceph进程的IP"><a href="#Ceph进程的IP" class="headerlink" title="Ceph进程的IP"></a>Ceph进程的IP</h3><h4 id="mon进程的IP"><a href="#mon进程的IP" class="headerlink" title="mon进程的IP"></a>mon进程的IP</h4><p>每个监视器守护进程都需要被绑定到特定的IP地址上，这些IP地址通常用部署工具配置，Ceph集群中其他组件通过 <code>[global] mon_host</code> 参数配置项发现mon进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">    mon_host = 10.0.0.2, 10.0.0.3, 10.0.0.4</span><br></pre></td></tr></table></figure><p><code>mon_host</code> 可以是IP地址，也可以是DNS名。对于具有多个A或AAAA记录的DNS名称，将探测所有记录以发现一个mon。一旦发现一个mon就会发现其他所有的mon。因此，<code>mon_host</code> 需要及时更新。</p><h4 id="其他进程的IP"><a href="#其他进程的IP" class="headerlink" title="其他进程的IP"></a>其他进程的IP</h4><p>MGR、OSD 和 MDS 守护程序将绑定到任何可用地址，并且不需要任何特殊配置。但是，可以为它们指定一个特定的 IP 地址，以便使用公共地址（和/或，对于 OSD 守护程序，集群地址）配置选项进行绑定。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[osd.0]</span><br><span class="line">        public_addr = &#123;host-public-ip-address&#125;</span><br><span class="line">        cluster_addr = &#123;host-cluster-ip-address&#125;</span><br></pre></td></tr></table></figure><h4 id="单网卡双网络"><a href="#单网卡双网络" class="headerlink" title="单网卡双网络"></a>单网卡双网络</h4><p>可行但不推荐</p><p>Generally, we do not recommend deploying an OSD host with a single network interface in a cluster with two networks. However, you may accomplish this by forcing the OSD host to operate on the public network by adding a <code>public_addr</code> entry to the <code>[osd.n]</code> section of the Ceph configuration file, where <code>n</code> refers to the ID of the OSD with one network interface. Additionally, the public network and cluster network must be able to route traffic to each other, which we don’t recommend for security reasons.</p><h3 id="网络相关的参数配置项"><a href="#网络相关的参数配置项" class="headerlink" title="网络相关的参数配置项"></a>网络相关的参数配置项</h3><p>不需要网络配置设置。 Ceph 假定有一个公共网络，集群中所有主机都在其上运行，除非您专门配置集群网络。</p><h4 id="公共网络配置"><a href="#公共网络配置" class="headerlink" title="公共网络配置"></a>公共网络配置</h4><p>公共网络可以配置特定的IP地址和子网。也可以为特定的进程专门分配静态IP</p><p><strong>public_network</strong></p><p>在 <code>[global]</code> section 中配置公网IP和公网掩码。格式为 <code>&#123;ip-address&#125;/&#123;netmask&#125;[,&#123;ip-address&#125;/&#123;netmask&#125;]</code></p><ul><li>str</li><li><code>192.168.0.0/24</code></li></ul><p><strong>public_addr</strong></p><p>为每个进程设置IP地址</p><ul><li>str</li></ul><h4 id="集群网络配置"><a href="#集群网络配置" class="headerlink" title="集群网络配置"></a>集群网络配置</h4><p><strong>cluster_network</strong></p><p>在 <code>[global]</code> section 中为集群网络设定指定的IP地址和掩码，格式为 <code>&#123;ip- address&#125;/&#123;netmask&#125; [, &#123;ip-address&#125;/&#123;netmask&#125;]</code></p><ul><li>str</li><li>10.0.0.0/24</li></ul><p><strong>cluster_addr</strong></p><p>为特定OSD设定IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[osd.0]</span><br><span class="line">    public_addr = &#123;host-public-ip-address&#125;</span><br><span class="line">    cluster_addr = &#123;host-cluster-ip-address&#125;</span><br><span class="line">[mon.noode1]</span><br><span class="line">	public_addr = &#123;host-public-ip-address&#125;</span><br><span class="line">    cluster_addr = &#123;host-cluster-ip-address&#125;</span><br></pre></td></tr></table></figure><h4 id="bind"><a href="#bind" class="headerlink" title="bind"></a>bind</h4><p>bind设置OSD和MDS进程的默认端口范围，默认为 <code>6800:7568</code> 。设置前需要确保IP Table允许使用配置的端口，也可以绑定到IPV6地址</p><p><strong>ms_bind_port_min</strong></p><p>OSD或MDS进程可绑定端口的最小值，int</p><p><strong>ms_bind_port_max</strong></p><p>OSD或MDS进程可绑定端口的最大值，int</p><p><strong>ms_bind_ipv4</strong></p><p>允许Ceph进程绑定PIv4地址，bool</p><p><strong>ms_bind_ipv6</strong></p><p>允许Ceph进程绑定PIv6地址，bool</p><p><strong>public_bind_addr</strong></p><p>在某些动态部署中，MON进程可能会绑定到本地的IP地址，这些IP地址不同于其他节点的 <code>public_addr</code> 。环境必须确保正确设置路由规则。如果设置了public_bind_addr, Ceph监控守护进程将在本地绑定它，并在monmap中使用public_addr将其地址通告给对等节点。</p><p><code>public_addr</code> 与 <code>public_bind_addr</code> 的绑定，指向同一节点，addr</p><h4 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h4><p>默认情况是不允许TCP缓存的。</p><p><strong>ms_tcp_nodelay</strong></p><p>Ceph默认将 <code>ms_tcp_nodelay=true</code> ，以便立即发送每个请求（无缓冲）。禁用 Nagle 算法会增加网络流量，从而导致延迟。</p><p>如果遇到大量小数据包，可以尝试令 <code>ms_tcp_nodelay=false</code>。</p><p><strong>ms_tcp_rcvbuf</strong></p><p>网络连接接收端的套接字缓冲区的大小。默认禁用。</p><ul><li>type：size</li><li>default：0B</li></ul><h4 id="通用设置"><a href="#通用设置" class="headerlink" title="通用设置"></a>通用设置</h4><p><strong>ms_type</strong></p><p>Async Messenger 使用的传输类型。可以是 async+posix、async+dpdk 或 async+rdma。</p><p>默认使用 Posix，其使用标准 TCP/IP 网络。其他类型是实验性的，并且支持可能有限</p><ul><li>str：“async+posix”</li><li>其取值会影响其他参数配置项的可用性<ul><li><code>ms_dpdk_XX</code></li><li><code>ms_rdma_XX</code></li></ul></li></ul><p><strong>ms_async_op_threads</strong></p><p>每个 Async Messenger 实例使用的初始工作线程数。应至少等于最大副本数，但如果 CPU 核心数较少和/或在单个服务器上托管大量 OSD，则可以减少它。</p><ul><li>uint[1,24]</li></ul><p><strong>ms_initial_backoff</strong></p><p>故障重新连接前的初始等待时间。</p><ul><li>float</li></ul><p><strong>ms_max_backoff</strong></p><p>故障重新连接所需的最长等待时间。</p><ul><li>float</li></ul><p><strong>ms_die_on_bad_msg</strong></p><p>调试选项;不要配置。</p><ul><li>bool</li></ul><p><strong>ms_dispatch_throttle_bytes</strong></p><p>限制等待分发的消息的总大小。</p><ul><li>size</li></ul><p><strong>ms_inject_socket_failures</strong></p><p>Debug option; do not configure.</p><p><strong>ms_learn_addr_from_peer</strong></p><p>使用第一个连接端的IP地址(通常是监视器)。如果客户端处于某种类型的NAT后，我们希望看到它由本地(非NAT)地址标识，这很有用。</p><h3 id="MESSENGER-V2"><a href="#MESSENGER-V2" class="headerlink" title="MESSENGER V2"></a>MESSENGER V2</h3><blockquote><p><strong>messenger 是一个用于实现集群内节点间通信的协议</strong>，Ceph的各个守护进程之间需要频繁地交换信息，以协调数据存储、副本管理、故障检测和恢复等操作。基于 Messenger，Ceph 能够有效地管理集群中的信息流动，如：进程间通信、心跳消息同步、日志消息、请求处理、进程状态更新等。</p></blockquote><p>Messenger v2 协议（或 msgr2）是 Ceph 在线协议的第二个主要修订版</p><ul><li>安全模式：加密通过网络传递的所有数据</li><li>改进了身份验证有效负载的封装，支持未来集成 Kerberos 等新身份验证模式</li><li>改进了早期的功能通告和协商，支持未来的协议修订</li></ul><p>Ceph 守护进程现在可以绑定到多个端口，允许旧版 Ceph 客户端和新的支持 v2 的客户端连接到同一集群。</p><p>默认情况下，监视器现在绑定到 IANA-分配 的采用 msgr2 协议的新端口 3300（ce4h 或 0xce4），同时还绑定到旧 v1 协议的旧默认端口 6789。</p><h4 id="地址格式"><a href="#地址格式" class="headerlink" title="地址格式"></a>地址格式</h4><p>在 Nautilus 之前，所有网络地址都呈现为 <code>1.2.3.4:567/89012</code>，</p><ul><li><p>IP:端口号/随机数</p><p>随机数用于唯一标识网络上的客户端或守护程序</p></li></ul><p>从N版开始，支持三种类型的地址格式</p><ul><li><p>v2: <code>v2:1.2.3.4:578/89012</code> 表示绑定到使用新 v2 协议的端口的守护进程</p></li><li><p>v1: <code>v1:1.2.3.4:578/89012</code> 表示绑定到使用旧版 v1 协议的端口的守护程序。以前显示的带有任何前缀的任何地址现在显示为 v1: 地址。</p></li><li><p>TYPE_ANY：<code>any:1.2.3.4:578/89012</code> 标识可以使用任一协议版本的客户端。在 Nautilus 之前，客户端将显示为 <code>1.2.3.4:0/123456</code> ，其中端口 0 表示它们是客户端并且不接受传入连接。</p><p>从 Nautilus 开始，这些客户端现在在内部由 TYPE_ANY 地址表示，并且仍然不带前缀显示，因为它们可能使用 v2 或 v1 协议连接到守护程序，具体取决于守护程序使用的协议。</p></li></ul><p>由于守护进程现在可以绑定到多个端口，因此它们由地址向量而不是单个地址来描述。例如，在 Nautilus 集群上dump monitor map 输出以下行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epoch 1</span><br><span class="line">fsid 50fcf227-be32-4bcb-8b41-34ca8370bd16</span><br><span class="line">last_changed 2019-02-25 11:10:46.700821</span><br><span class="line">created 2019-02-25 11:10:46.700821</span><br><span class="line">min_mon_release 14 (nautilus)</span><br><span class="line">0: [v2:10.0.0.10:3300/0,v1:10.0.0.10:6789/0] mon.foo</span><br><span class="line">1: [v2:10.0.0.11:3300/0,v1:10.0.0.11:6789/0] mon.bar</span><br><span class="line">2: [v2:10.0.0.12:3300/0,v1:10.0.0.12:6789/0] mon.baz</span><br></pre></td></tr></table></figure><p><code>[]</code> 中的地址列表或向量意味着可以在多个端口（和协议）上访问同一个守护进程。</p><p>如果可能的话，连接到该守护程序的任何客户端或其他守护程序都将使用 v2 协议（首先列出）；否则它将返回到旧版 v1 协议。</p><p>旧客户端将只能看到 v1 地址，并将继续像以前一样使用 v1 协议进行连接。</p><p>从 Nautilus 开始，mon_host 配置选项和 -m<mon-host>命令行选项支持相同的括号地址向量语法。</mon-host></p><h4 id="bind参数配置项"><a href="#bind参数配置项" class="headerlink" title="bind参数配置项"></a>bind参数配置项</h4><p>两个新的配置选项控制是否使用 v1 和/或 v2 协议：</p><ul><li>ms_bind_msgr1：[default: true] 控制守护进程是否绑定到使用 v1 协议的端口</li><li>ms_bind_msgr2：[default: true] 控制守护进程是否绑定到使用 v2 协议的端口</li></ul><p>同样，有两个选项控制是否使用 IPv4 和 IPv6 地址：</p><ul><li>ms_bind_ipv4： [default: true] controls whether a daemon binds to an IPv4 address</li><li>ms_bind_ipv6</li></ul><h4 id="连接模式"><a href="#连接模式" class="headerlink" title="连接模式"></a>连接模式</h4><p>v2协议支持两种连接模式：</p><ul><li><p>crc 模式：</p><p>建立连接时进行强大的初始身份验证（使用 cephx，双方进行相互身份验证，防止中间人或窃听者）</p><p>CRC32C 完整性校验用于保护因不稳定硬件或宇宙射线引起的比特翻转。</p><p>不支持：</p><ul><li>保密（网络上的窃听者可以看到所有经过身份验证的流量）</li><li>防止恶意中间人（他们可以在流量经过时故意修改流量，只要他们小心调整 crc32c 值以匹配）</li></ul></li><li><p>secure 模式</p><ul><li>建立连接时进行强大的初始身份验证（使用 cephx，双方进行相互身份验证，防止中间人或窃听者）</li><li>对所有身份验证后流量进行完全加密，包括加密完整性检查。</li></ul></li></ul><p>在 Nautilus 中，安全模式使用 AES-GCM 流密码，该密码在现代处理器上通常非常快（例如，比 SHA-256 加密哈希更快）。</p><h4 id="压缩模式"><a href="#压缩模式" class="headerlink" title="压缩模式"></a>压缩模式</h4><p>v2协议支持两种压缩模式：</p><ul><li><p><code>force</code> ：</p><p>在多可用区域(zones)中部署集群，压缩 OSD 之间的复制消息可以节省延迟。</p><p>在公有云中，可用区域(zones)的通信非常昂贵。因此，最小化消息大小可以降低云提供商的网络成本。</p><p>当在 AWS（可能还有其他公共云）上使用实例存储时，具有 NVMe 的实例提供的网络带宽相对于设备带宽较低。在这种情况下，NW 压缩可以提高整体性能，因为这显然是瓶颈。</p></li><li><p><code>none</code> ：消息在不压缩的情况下传输。</p></li></ul><p>相关参数配置项见 [参数分析]</p><h4 id="从仅-V1-过渡到-V2-PLUS-V1"><a href="#从仅-V1-过渡到-V2-PLUS-V1" class="headerlink" title="从仅 V1 过渡到 V2-PLUS-V1"></a>从仅 V1 过渡到 V2-PLUS-V1</h4><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/msgr2/#transitioning-from-v1-only-to-v2-plus-v1">https://docs.ceph.com/en/quincy/rados/configuration/msgr2/#transitioning-from-v1-only-to-v2-plus-v1</a></p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/msgr2/#updating-ceph-conf-and-mon-host">UPDATING CEPH.CONF AND MON_HOST</a></p><h2 id="Cephx"><a href="#Cephx" class="headerlink" title="Cephx"></a>Cephx</h2><p>CephX 协议默认启用。 CephX 提供的加密身份验证具有一定的计算成本，但是它们通常应该相当低。</p><p>如果连接客户端和服务器主机的网络环境非常安全并且您无法承担身份验证，则可以禁用它。<strong>通常不建议禁用身份验证</strong>。</p><ul><li>如果禁用身份验证，您将面临中间人攻击的风险，该攻击会更改您的客户端/服务器消息，这可能会产生灾难性的安全影响。</li></ul><p>For information about creating users, see <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/operations/user-management">User Management</a>.</p><p>For details on the architecture of CephX, see <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/architecture#high-availability-authentication">Architecture - High Availability Authentication</a>.</p><h3 id="部署CEPHX"><a href="#部署CEPHX" class="headerlink" title="部署CEPHX"></a>部署CEPHX</h3><p>CephX 的最初配置取决于Ceph的部署场景。部署Ceph集群有两种常见的策略</p><ul><li>如果您是第一次使用 Ceph，您可能应该采用最简单的方法：使用 cephadm 部署集群</li><li>但是，如果您的集群使用其他部署工具（例如 Ansible、Chef、Juju 或 Puppet），您将需要使用手动部署过程或配置您的部署工具，以便它引导您的监视器。</li></ul><p>手动部署：手动部署集群时，需要手动引导监视器并创建 client.admin 用户和密钥环。要引导监视器，请按照 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/install/manual-deployment#monitor-bootstrapping">Monitor Bootstrapping</a>中的步骤操作。使用第三方部署工具（例如 Chef、Puppet 和 Juju）时请按照以下步骤操作。</p><h3 id="启用-禁用-CEPHX"><a href="#启用-禁用-CEPHX" class="headerlink" title="启用/禁用 CEPHX"></a>启用/禁用 CEPHX</h3><p>仅当您的监视器、OSD 和元数据服务器的密钥已部署时，才可以启用 CephX。如果您只是打开或关闭 CephX，则无需重复引导过程。</p><h4 id="启用CEPHX"><a href="#启用CEPHX" class="headerlink" title="启用CEPHX"></a>启用CEPHX</h4><p>当CephX启用时，Ceph将在默认搜索路径中查找密钥环：该路径包括 <code>/etc/ceph/$cluster.$name.keyring</code></p><ul><li>可以通过在 Ceph 配置文件的 [global] 部分添加 <code>keyring</code> 选项来覆盖此搜索路径位置，但不建议这样做。</li></ul><p>要在已禁用身份验证的集群上启用 CephX，请执行以下过程</p><ul><li>如果您（或您的部署实用程序）已经生成了密钥，则可以跳过与生成密钥相关的步骤。</li></ul><ol><li><p>创建一个 client.admin 密钥，并为您的客户端主机保存该密钥的副本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.admin mon &#x27;allow *&#x27; mds &#x27;allow *&#x27; mgr &#x27;allow *&#x27; osd &#x27;allow *&#x27; -o /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure><p>警告：此步骤将破坏任何现有的 /etc/ceph/client.admin.keyring 文件。如果部署工具已为您生成密钥环文件，请不要执行此步骤。当心！</p></li><li><p>创建监视器密钥环并生成监视器密钥：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#x27;allow *&#x27;</span><br></pre></td></tr></table></figure></li><li><p>对于每个监视器，将监视器密钥环复制到监视器 mon 数据目录中的 ceph.mon.keyring 文件中。例如，要将监视器密钥环复制到名为 ceph 的集群中的 mon.a，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /tmp/ceph.mon.keyring /var/lib/ceph/mon/ceph-a/keyring</span><br></pre></td></tr></table></figure></li><li><p>为每个 MGR 生成一个密钥，其中 {$id} 是 MGR 的id：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create mgr.&#123;$id&#125; mon &#x27;allow profile mgr&#x27; mds &#x27;allow *&#x27; osd &#x27;allow *&#x27; -o /var/lib/ceph/mgr/ceph-&#123;$id&#125;/keyring</span><br></pre></td></tr></table></figure></li><li><p>为每个 OSD 生成一个密钥，其中 {$id} 是 OSD 编号：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create osd.&#123;$id&#125; mon &#x27;allow rwx&#x27; osd &#x27;allow *&#x27; -o /var/lib/ceph/osd/ceph-&#123;$id&#125;/keyring</span><br></pre></td></tr></table></figure></li><li><p>为每个 MDS 生成一个密钥，其中 {$id} 是 MDS 字母：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create mds.&#123;$id&#125; mon &#x27;allow rwx&#x27; osd &#x27;allow *&#x27; mds &#x27;allow *&#x27; mgr &#x27;allow profile mds&#x27; -o /var/lib/ceph/mds/ceph-&#123;$id&#125;/keyring</span><br></pre></td></tr></table></figure></li><li><p>通过在 Ceph 配置文件的 [global] 部分中设置以下选项来启用 CephX 身份验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure></li><li><p>启动或重新启动 Ceph 集群。详情请参见操作集群。</p></li></ol><h4 id="禁用CEPHX"><a href="#禁用CEPHX" class="headerlink" title="禁用CEPHX"></a>禁用CEPHX</h4><p>以下过程描述了如何禁用 CephX。如果您的集群环境是安全的，您可能需要禁用 CephX 以抵消运行身份验证的计算费用。我们不建议这样做。</p><p>但是，如果在配置和故障排除是短暂禁用身份验证并随后重新启用，可能会更容易。</p><ol><li><p>通过在 Ceph 配置文件的 [global] 部分中设置以下选项来禁用 CephX 身份验证：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br></pre></td></tr></table></figure></li><li><p>重启集群</p></li></ol><h4 id="SIGNATURES"><a href="#SIGNATURES" class="headerlink" title="SIGNATURES"></a>SIGNATURES</h4><p>Ceph 执行签名检查，提供一些有限的保护，防止消息在传输过程中被篡改（例如，“中间人”攻击）。</p><p>与 Ceph 身份验证的其他部分一样，签名允许进行细粒度控制。您可以启用或禁用客户端和 Ceph 之间的服务消息以及 Ceph 守护程序之间的消息的签名。</p><p>请注意，即使启用签名，数据也不会在传输过程中加密。</p><h3 id="KEYS"><a href="#KEYS" class="headerlink" title="KEYS"></a>KEYS</h3><p>当Ceph在启用身份验证的情况下运行时，Ceph管理命令和Ceph客户端只有使用身份验证密钥才能访问Ceph集群</p><p>使这些密钥可供 ceph 管理命令和 Ceph 客户端使用的最常见方法是在 /etc/ceph 目录下包含 Ceph keyring。</p><p>对于使用cephadm的O版及更高的版本，文件名常为 <code>ceph.client.admin.keyring</code> 。如果keyring 包含在 <em>/etc/ceph</em> 目录中，则无需再Ceph配置文件中指定keyring配置项</p><p>由于Ceph存储集群的 密钥环 文件包含 <em>client.admin</em> 密钥，因此，我们建议将密钥环文件复制到运行管理命令的管理节点</p><p>手动执行步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp &#123;user&#125;@&#123;ceph-cluster-host&#125;:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure><ul><li>确保 ceph.keyring 文件在客户端主机上设置了适当的权限（例如 chmod 644）。</li></ul><p>可以使用 Ceph 配置文件中的密钥设置来指定密钥内容（不推荐这种方法），或者使用 Ceph 配置文件中 keyfile 参数配置项指定密钥文件</p><p><code>key</code> ：指定密钥本身的文本字符串。我们不建议您使用此设置，除非您知道自己在做什么。</p><p><code>keyfile</code> ：密钥文件的路径（即仅包含密钥的文件）。</p><p><code>keyring</code> ：密钥环文件的路径。</p><h3 id="守护进程-keyrings"><a href="#守护进程-keyrings" class="headerlink" title="守护进程 keyrings"></a>守护进程 keyrings</h3><p>管理用户或部署工具（例如，cephadm）生成守护进程 keyring 的方式与生成用户 keyring 的方式相同。默认情况下，Ceph 将守护进程的 keyring 存储在该守护进程的数据目录中。默认 keyrings 位置和守护程序运行所需的 function（权限？） 如下所示。</p><div class="table-container"><table><thead><tr><th></th><th>Location</th><th>Capabilities</th></tr></thead><tbody><tr><td>ceph-mon</td><td><code>$mon_data/keyring</code></td><td><code>mon &#39;allow *&#39;</code></td></tr><tr><td>ceph-osd</td><td><code>$osd_data/keyring</code></td><td><code>mgr &#39;allow profile osd&#39; mon &#39;allow profile osd&#39; osd &#39;allow *&#39;</code></td></tr><tr><td>ceph-mds</td><td><code>$mds_data/keyring</code></td><td><code>mds &#39;allow&#39; mgr &#39;allow profile mds&#39; mon &#39;allow profile mds&#39; osd &#39;allow rwx&#39;</code></td></tr><tr><td>ceph-mgr</td><td><code>$mgr_data/keyring</code></td><td><code>mon &#39;allow profile mgr&#39; mds &#39;allow *&#39; osd &#39;allow *&#39;</code></td></tr><tr><td>radosgw</td><td><code>$rgw_data/keyring</code></td><td><code>mon &#39;allow rwx&#39; osd &#39;allow rwx&#39;</code></td></tr></tbody></table></div><p>监视器 keyring（即 mon.）包含密钥但没有权限，并且此 keyring 不是集群身份验证(auth)数据库的一部分。</p><ul><li><p>守护进程的数据目录位置默认为以下形式的目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">/var/lib/ceph/$</span><span class="language-bash"><span class="built_in">type</span>/<span class="variable">$cluster</span>-<span class="variable">$id</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">For example, osd.12 would have the following data directory:</span></span><br><span class="line">/var/lib/ceph/osd/ceph-12</span><br></pre></td></tr></table></figure></li><li><p>可以覆盖这些位置，但不建议这样做。</p></li></ul><h2 id="monitor"><a href="#monitor" class="headerlink" title="monitor"></a>monitor</h2><p>了解如何配置 Ceph Monitor 是构建可靠的 Ceph 存储集群的重要组成部分。</p><p><strong>所有 Ceph 存储集群都至少有一台监视器</strong>。</p><p>所有监视器的配置通常保持相对一致，但你可以在集群中添加、移除或替换监视器。详见添加/移除监视器的详细信息。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>Ceph 监视器维护集群映射 (cluster map) 的“主副本”。</p><p>Ceph 客户端必须连接到 Ceph Monitor，然后才能读取或写入 Ceph OSD 守护进程或 Ceph 元数据服务器。客户端通过连接到一个 Ceph Monitor 并检索当前 cluster map ，Cluster Map 使 Ceph 客户端能够确定所有 Ceph 监视器、Ceph OSD 守护进程和 Ceph 元数据服务器的位置。拥有当前 cluster map 副本和 CRUSH 算法的 Ceph 客户端可以计算集群内任何 RADOS 对象的位置。这使得 Ceph 客户端可以直接与 Ceph OSD 守护进程对话。</p><ul><li>客户端和 Ceph OSD 守护进程之间的直接通信改进了需要客户端与中央组件进行通信的传统存储架构。</li></ul><p>Ceph Monitor 的主要功能是维护 Cluster map 的主副本。监视器服务中的所有更改都由 Ceph Monitor 写入单个 Paxos 实例，Paxos 将更改写入键/值存储。这提供了强一致性。Ceph Monitors 能够在同步操作期间查询最新版本的集群映射，并且它们使用键/值存储的快照 (snapshots) 和迭代器（使用 RocksDB）来执行存储范围的同步。</p><p>监视器还提供身份验证和日志记录服务。</p><p><img src="/posts/2576505844/ditaa-d93220ac740fbc1df6152f91b390a95f9b36f9c5.png" alt="img"></p><h4 id="cluster-map"><a href="#cluster-map" class="headerlink" title="cluster map"></a>cluster map</h4><p>Cluster map 是映射的组合，包括 the monitor map, the OSD map, the placement group map and the metadata server map。</p><p>Clsuter map跟踪很多重要事情：</p><ul><li>Ceph 存储集群中有哪些进程 <code>in</code> ；</li><li>Ceph 存储集群中的哪些 <code>in</code> 进程处于启动和运行 <code>up</code> 或关闭 <code>down</code> 状态；</li><li>PG是活动的 <code>active</code> 还是非活动的 <code>inactive</code> ，干净的 <code>clean</code> 还是处于其他状态；</li><li>以及反映集群当前状态的其他详细信息，例如存储空间总量和使用的存储量。</li></ul><p>当集群状态发生重大变化时（例如，Ceph OSD 守护进程关闭、PG陷入降级状态degraded等），Cluster map 会更新以反映集群的当前状态。</p><p>此外，Ceph Monitor 还维护集群先前状态的历史记录。Mon map、OSD map、PG map和 MDS map 均维护其 map 版本的历史记录。我们将每个版本称为“epoch”。</p><p>操作 Ceph 存储集群时，跟踪这些状态是系统管理职责的重要组成部分。</p><h4 id="监控法定人数"><a href="#监控法定人数" class="headerlink" title="监控法定人数"></a>监控法定人数</h4><p>我们的 配置ceph部分 提供了一个简单的 Ceph 配置文件，该文件在测试集群中提供一个监视器。</p><p>集群只需一个监视器就可以正常运行；然而，单个监视器是单点故障。为了确保生产 Ceph 存储集群的高可用性，您应该使用多个监视器运行 Ceph，这样单个监视器的故障不会导致整个集群瘫痪。</p><p>当 Ceph 存储集群运行多个 Ceph Monitor 以实现高可用性时，Ceph Monitor 使用 Paxos 建立有关主集群映射的共识。达成共识需要运行大多数监视器来建立关于集群图的共识的法定人数（例如，1 个；3 个中的 2 个；5 个中的 3 个；6 个中的 4 个；等等）。</p><p><code>mon_force_quorum_join</code></p><h4 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h4><p>当向 Ceph 配置文件添加 监视器设置 时，您需要了解 Ceph 监视器的一些架构方面。</p><p>当发现集群中的另一个 Ceph Monitor 时，<strong>Ceph 对 Ceph Monitor 施加严格的一致性要求</strong>。然而，Ceph 客户端和其他 Ceph 守护进程使用 Ceph 配置文件来发现监视器，监视器使用监视器映射（monmap）而不是 Ceph 配置文件来发现彼此。</p><p>当发现 Ceph 存储集群中的其他 Ceph Monitor 时，Ceph Monitor 始终引用 monmap 的本地副本。使用 monmap 而不是 Ceph 配置文件可以避免可能破坏集群的错误（例如，指定监视器地址或端口时 ceph.conf 中的拼写错误）。</p><ul><li>如果 Ceph 监视器通过 Ceph 配置文件而不是通过 monmap 来发现彼此，则会引入额外的风险，因为 Ceph 配置文件不会自动更新和分发。Ceph Monitor 可能会无意中使用较旧的 Ceph 配置文件、无法识别 Ceph Monitor、超出法定人数或出现 Paxos 无法准确确定系统当前状态的情况。</li></ul><p>由于监视器使用 monmap 进行发现，并且它们与客户端和其他 Ceph 守护进程共享 monmap，因此 <strong>monmap 为监视器提供严格保证，确保其共识有效</strong>。</p><p>严格一致性也适用于 monmap 的更新。与 Ceph Monitor 上的任何其他更新一样，<strong>对 monmap 的更改始终通过称为 Paxos 的分布式共识算法运行</strong>。Ceph 监视器必须就 monmap 的每次更新达成一致，例如添加或删除 Ceph 监视器，以确保仲裁中的每个监视器都具有相同版本的 monmap。</p><ul><li>monmap 的更新是增量式的，以便 Ceph Monitors 拥有最新商定的版本和一组以前的版本。</li><li>维护历史记录使具有旧版本 monmap 的 Ceph Monitor 能够赶上 Ceph 存储集群的当前状态。</li></ul><h4 id="引导monitor"><a href="#引导monitor" class="headerlink" title="引导monitor"></a>引导monitor</h4><p>在大多数部署和配置情况下，Ceph 部署工具通过为您生成 monitor map 来帮助引导 Ceph 监控器（例如 cephadm 等）。Ceph Monitor 需要一些显式设置：</p><ul><li><p>文件系统ID：<code>fsid</code> 是对象存储的唯一标识符。由于您可以在同一硬件上运行多个集群，因此在引导监视器时必须指定对象存储的唯一 ID。</p><p>部署工具通常会为您执行此操作（例如，cephadm 可以调用 uuidgen 等工具），但您也可以手动指定 fsid。</p></li><li><p>monitor ID：监视器 ID 是分配给集群内每个监视器的唯一 ID。它是一个字母数字值，按照惯例，标识符通常遵循字母顺序增量（例如，a、b 等）。</p><p>这可以通过部署工具或使用 ceph 命令行在 Ceph 配置文件（例如 [mon.a]、[mon.b] 等）中进行设置。</p></li><li><p>Keys ：监视器必须有密钥。 cephadm 等部署工具通常会为您执行此操作，但您也可以手动执行此步骤。有关详细信息，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/dev/mon-bootstrap#secret-keys">监控密钥</a> 。</p></li></ul><p>有关引导的其他详细信息，<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/dev/mon-bootstrap">Bootstrapping a Monitor</a>.</p><h3 id="Monitor-的参数配置项"><a href="#Monitor-的参数配置项" class="headerlink" title="Monitor 的参数配置项"></a>Monitor 的参数配置项</h3><p>要将参数配置项的设置应用于整个集群，请在 [global] 下输入配置设置。</p><p>要将参数配置项的设置应用于集群中的所有监视器 (monitors)，请在 [mon] 下输入配置设置</p><p>要将参数配置项的设置应用于特定监视器(monitor)，请指定监视器实例（例如，[mon.a]）</p><p>按照约定，监视器实例名称使用字母表示法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">[mon]</span><br><span class="line">[mon.a]</span><br><span class="line">[mon.b]</span><br><span class="line">[mon.c]</span><br></pre></td></tr></table></figure><h4 id="最小配置"><a href="#最小配置" class="headerlink" title="最小配置"></a>最小配置</h4><p>通过 Ceph 配置文件对 Ceph 监视器进行的最低限度监视器设置包括每个监视器的主机名和网络地址。在 <code>[mon]</code> 或特定 monitor 条目下配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">        mon_host = 10.0.0.2,10.0.0.3,10.0.0.4</span><br><span class="line">[mon.a]</span><br><span class="line">        host = hostname1</span><br><span class="line">        mon_addr = 10.0.0.10:6789</span><br></pre></td></tr></table></figure><ul><li>此监视器的最低配置假设部署工具生成 fsid 和 mon. 密钥</li></ul><p>一旦部署了 Ceph 集群，就不应该更改monitor的 IP 地址。</p><ul><li>但是，如果您决定更改显示器的 IP 地址，则必须遵循特定程序（<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address">Changing a Monitor’s IP Address</a> ）。有关详细信息，请参阅更改监视器的 IP 地址。</li></ul><p>客户端还可以使用 DNS SRV 记录找到监视器。 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mon-lookup-dns">Monitor lookup through DNS</a></p><h4 id="集群ID"><a href="#集群ID" class="headerlink" title="集群ID"></a>集群ID</h4><p>每个 Ceph 存储集群都有一个唯一的标识符 (fsid)。如果指定，它通常出现在配置文件的 [global] 部分下。部署工具通常会生成 fsid 并将其存储在监控映射中，因此该值可能不会出现在配置文件中。fsid 使得在同一硬件上运行多个集群的守护进程成为可能。</p><p><code>fsid</code></p><h4 id="初始成员"><a href="#初始成员" class="headerlink" title="初始成员"></a>初始成员</h4><p>我们建议运行具有至少三个 Ceph Monitor 的生产 Ceph 存储集群，以确保高可用性。当您运行多个监视器时，您可以指定必须是集群成员的初始监视器才能建立仲裁。这可能会减少集群上线所需的时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mon]</span><br><span class="line">        mon_initial_members = a,b,c</span><br></pre></td></tr></table></figure><p><strong>mon_initial_members</strong></p><blockquote><p>集群中的大多数监视器必须能够相互访问才能建立法定人数。您可以使用此设置减少监视器的初始数量以建立仲裁</p></blockquote><h4 id="DATA"><a href="#DATA" class="headerlink" title="DATA"></a>DATA</h4><p>Ceph 提供了 Ceph Monitor 存储数据的默认路径。为了在生产环境的 Ceph 存储集群中获得最佳性能，我们建议在与 Ceph OSD 守护程序不同的主机和驱动器上运行 Ceph 监视器。</p><p>由于 RocksDB 使用 mmap() 写入数据，Ceph 监视器经常将数据从内存刷新到磁盘，如果数据存储与 OSD 守护进程位于同一位置，这可能会干扰 Ceph OSD 守护进程工作负载。</p><ul><li>在 Ceph 0.58 及更早版本中，Ceph 监视器将其数据存储在纯文件中。这种方法允许用户使用 ls 和 cat 等常用工具检查监控数据。然而，这种方法并没有提供强一致性。</li><li>在 Ceph 0.59 及更高版本中，Ceph 监视器将其数据存储为键/值对。 Ceph 监视器需要 ACID 事务。使用数据存储可以防止通过 Paxos 恢复运行损坏版本的 Ceph Monitor，并且它可以在单个原子批次中实现多个修改操作，还有其他优点。</li></ul><p>一般来说，我们不建议更改默认数据位置。如果您修改默认位置，我们建议您通过在配置文件的 [mon] 部分中进行设置，使其在 Ceph 监视器之间保持一致。</p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/mon-config-ref/">https://docs.ceph.com/en/quincy/rados/configuration/mon-config-ref/</a></p><h4 id="存储容量"><a href="#存储容量" class="headerlink" title="存储容量"></a>存储容量</h4><p>当 Ceph 存储集群接近其最大容量时（请参阅 <code>mon_osd_fullratio</code> ），Ceph 会阻止您写入或读取 OSD，作为防止数据丢失的安全措施。因此，让生产 Ceph 存储集群接近其满配率并不是一个好的做法，因为它会牺牲高可用性。默认满率为 0.95，即容量的 95%。对于具有少量 OSD 的测试集群来说，这是一个非常激进的设置。</p><blockquote><p>监控集群时，请警惕与 <code>nearfull</code> 相关的警告。这意味着如果一个或多个 OSD 发生故障，某些 OSD 发生故障可能会导致服务暂时中断。考虑添加更多 OSD 以增加存储容量。</p></blockquote><p>在常见的测试集群场景中，系统管理员会从 Ceph 存储集群中移除一个 OSD（对象存储守护进程），观察集群重新平衡，然后移除另一个 OSD，再移除另一个，直到至少有一个 OSD 最终达到满载比例，集群就会锁定。即使对于测试集群，我们也推荐进行一些容量规划。规划可以帮助你评估为了保持高可用性所需的备用容量。理想情况下，你应该计划一系列 Ceph OSD 守护进程的故障，使得集群能够在不立即替换这些 OSD 的情况下恢复到 <code>active+clean</code> 状态。集群操作在 <code>active+degraded</code> 状态下仍可以继续操作，但这并不理想，应该尽快解决。</p><blockquote><p>下图描述了一个简单的 Ceph 存储集群，包含 33 个 Ceph 节点，每个主机有一个 OSD，每个 OSD 读取和写入 3TB 驱动器。</p><p>因此，这个示例性 Ceph 存储集群的最大实际容量为 99TB。当 mon osd full 比率为 0.95 时，如果 Ceph 存储集群的剩余容量降至 5TB，集群将不允许 Ceph 客户端读写数据。所以Ceph存储集群的运行容量是95TB，而不是99TB。</p><p><img src="/posts/2576505844/ditaa-711402e8fde8ea6a9ae1a39bb27a1b5431a0d8de.png" alt="img"></p></blockquote><p>在这样的集群中，一两个 OSD 出现故障是正常的。一种不太常见但合理的情况是机架的路由器或电源发生故障，这会同时关闭多个 OSD（例如 OSD 7-12）。</p><p>在这种情况下，可以通过短时间内添加一些带有额外 OSD 的主机，让集群保持运行并恢复为 <code>active+clean</code> 状态</p><p>如果容量利用率太高，您可能不会丢失数据，但如果集群的容量利用率超过满配比，您在解决故障域内的中断时仍然可能会牺牲数据可用性。因此，我们建议至少进行一些粗略的容量规划。</p><blockquote><p>确定您的集群的两个数字：</p><ul><li>OSD数量</li><li>集群的总用量</li></ul><p>如果将集群的总容量除以集群中 OSD 的数量，您将得到集群中 OSD 的平均容量。考虑将 OSD 的平均容量 乘以您预计在正常操作期间将同时发生故障的 OSD 数量（相对较小的数字）。最后将集群的容量乘以满配比，得到最大运行容量；然后，从您预计无法达到合理满率的 OSD 中减去数据量。</p><p>对更多数量的 OSD 故障（例如，一个 OSD 机架）重复上述过程，以获得接近满率的合理值。</p></blockquote><p>以下设置仅适用于集群创建，然后存储在 OSDMap 中。澄清一下，在正常操作中，<strong>OSD 使用的值是 OSDMap 中的值</strong>，而不是配置文件或中央配置存储中的值。</p><ul><li><strong>这些设置仅在集群创建期间应用。之后需要使用 ceph osd set-nearfull-ratio 和 ceph osd set-full-ratio 在 OSDMap 中更改它们</strong></li></ul><p>mon_osd_full_ratio</p><p>mon_osd_backfillfull_ratio</p><p>mon_osd_nearfull_ratio</p><blockquote><p>如果某些 OSD 接近满，但其他 OSD 有足够的容量，则您为接近满的 OSD 设置的 CRUSH 权重可能不准确。</p></blockquote><h4 id="监控器存储同步-MONITOR-STORE-SYNCHRONIZATION"><a href="#监控器存储同步-MONITOR-STORE-SYNCHRONIZATION" class="headerlink" title="监控器存储同步(MONITOR_STORE_SYNCHRONIZATION)"></a>监控器存储同步(MONITOR_STORE_SYNCHRONIZATION)</h4><p>当您运行具有多个监视器的生产集群时（推荐），每个监视器都会检查相邻监视器是否具有较新版本的 cluster map （例如，相邻监视器中的 map 的一个或多个 epoch 高于当前监视器 map 中的最新epoch）。</p><p>集群中的一个监视器可能会周期性地落后于其他监视器，以至于它必须离开仲裁，同步以检索有关集群的最新信息，然后重新加入仲裁。为了同步的目的，监视器可以承担以下三个角色之一：</p><ul><li>Leader：Leader是第一个实现最新Paxos版本集群map的监视器。</li><li>Provider：Provider 是一个拥有最新版本的集群映射的监视器，但并不是第一个实现最新版本的监视器。</li><li>Requester：Requester 是落后于领导者的监视器，必须进行同步，以便在重新加入仲裁之前检索有关集群的最新信息</li></ul><p>这些角色使 Leader 能够将同步职责委托给 Provider ，从而防止过多同步请求使Leader 过载。在下图中，Requester了解到它落后于其他监视器。Requester要求Leader同步，Leader告诉Requester与哪个Provider同步。</p><p><img src="/posts/2576505844/ditaa-af0ce2210eafb917a47884a8edbcbc94b581639b.png" alt="img"></p><p>当新监视器加入集群时，总是会发生同步。在运行期间操作，监视器可以在不同时间接收 cluster map 的更新。这意味着 Provider 和 Leader 角色可能从一个监视器迁移到另一个监视器。如果在同步时发生这种情况（例如，Provider落后于 Leader），则Provider可以终止与Requester的同步。</p><p>同步完成后，Ceph 会在集群执行修剪。修剪之后要求 PG 处于 <code>active+clean</code> 状态。</p><h4 id="时钟"><a href="#时钟" class="headerlink" title="时钟"></a>时钟</h4><p>Ceph 守护进程相互传递关键消息，这些消息必须在守护进程达到超时阈值之前得到处理。如果 Ceph 监视器中的时钟不同步，可能会导致许多异常情况。</p><ul><li>守护进程忽略收到的消息（例如，时间戳已过时）</li><li>当未及时收到消息时，超时触发得太早/太晚。</li></ul><p>您必须在 Ceph 监控主机上配置 NTP 或 PTP 守护进程，以确保监控集群以同步时钟运行。让监视器主机彼此同步以及与多个质量上行时间源同步是有利的。</p><p>即使使用了 NTP，时钟偏差仍然可能明显，尽管这种偏差尚未造成危害。即使在 NTP 保持了合理的同步水平的情况下，Ceph 的时钟偏差/时钟偏差警告也可能被触发。在这种情况下，增加时钟偏差可能是可以容忍的；然而，诸如工作负载、网络延迟、配置默认超时的覆盖以及 Monitor 存储同步设置等多个因素可能会影响可接受的时钟偏差水平，而不会损害 Paxos 保证。</p><h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><h4 id="Pool-设置"><a href="#Pool-设置" class="headerlink" title="Pool 设置"></a>Pool 设置</h4><p>从版本 v0.94 开始，支持池标志，允许或禁止对池进行更改。如果配置适当，监视器还可以禁止删除池。这种拦截机制所带来的不便远不及池误删的代价。</p><h4 id="通过-DNS-查找监视器"><a href="#通过-DNS-查找监视器" class="headerlink" title="通过 DNS 查找监视器"></a>通过 DNS 查找监视器</h4><p>从 Ceph 11.0.0 版本（Kraken）开始，RADOS 支持通过 DNS 查找监视器。添加通过 DNS 查找监视器的功能意味着守护进程和客户端不需要在其 ceph.conf 配置文件中使用 mon 主机配置指令。</p><p>通过 DNS 更新，客户端和守护程序可以了解监视器拓扑中的更改。为了更精确和技术性，客户端使用 DNS SRV TCP 记录查找监视器。</p><p>默认情况下，客户端和守护程序会查找名为 ceph-mon 的 TCP 服务，该服务由 mon_dns_srv_name 配置指令进行配置。</p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/mon-lookup-dns/#example">https://docs.ceph.com/en/quincy/rados/configuration/mon-lookup-dns/#example</a></p><h3 id="Heartbeat"><a href="#Heartbeat" class="headerlink" title="Heartbeat"></a>Heartbeat</h3><p><span id="mon_osd_interaction"></span></p><p>Ceph 监视器要求每个 OSD 的报告以及从 OSD 接收有关其相邻 OSD 状态的报告，进而了解集群。Ceph为 monitor/OSD 交互提供合理的默认参数配置值；但是，您可以根据需要修改它们。有关详细信息，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/mon-osd-interaction">Monitor/OSD Interaction</a> 。</p><p>完成初始 Ceph 配置后，您可以部署并运行 Ceph。</p><p>当您执行 ceph health 或 ceph -s 等命令时，Ceph Monitor 会报告 Ceph 存储集群的当前状态。Ceph Monitor 需要来自每个 Ceph OSD 守护进程的报告以及从 Ceph OSD 守护进程接收有关其相邻 Ceph OSD 守护进程状态的报告，来了解 Ceph 存储集群。</p><p>如果 Ceph Monitor 未收到报告，或者收到有关 Ceph 存储集群中更改的报告，Ceph Monitor 会更新 Ceph Cluster Map 的状态。</p><p>Ceph 为 Ceph Monitor/Ceph OSD Daemon 交互提供合理的默认设置。但是，您可以覆盖默认值。以下部分描述了 Ceph 监视器和 Ceph OSD 守护进程如何交互以监视 Ceph 存储集群</p><h4 id="OSDS-检查心跳"><a href="#OSDS-检查心跳" class="headerlink" title="OSDS 检查心跳"></a>OSDS 检查心跳</h4><p>每个 Ceph OSD 守护进程以小于每 6 秒的随机间隔检查其他 Ceph OSD 守护进程的心跳。如果相邻的 Ceph OSD 守护进程在 20 秒的宽限期内没有显示心跳，Ceph OSD 守护进程可能会认为相邻的 Ceph OSD 守护进程已关闭 <code>down</code> ，并将其报告给 Ceph 监视器，后者将更新 Ceph 集群映射。</p><p><img src="/posts/2576505844/ditaa-445812455304f21ffe56440cf81649d3835b9a4d.png" alt="img"></p><p>您可以通过在 Ceph 配置文件的 [mon] 和 [osd] 或 [global] 部分下添加 osd 心跳宽限设置来更改此宽限期，或者通过在运行时设置该值</p><h4 id="OSDS-报告-OSDS-down"><a href="#OSDS-报告-OSDS-down" class="headerlink" title="OSDS 报告 OSDS down"></a>OSDS 报告 OSDS down</h4><p>默认情况下，来自不同主机的两个 Ceph OSD 守护进程必须向 Ceph 监视器报告另一个 Ceph OSD 守护进程已关闭 <code>down</code> ，然后 Ceph 监视器才会确认所报告的 Ceph OSD 守护进程已关闭 <code>down</code> 。</p><p>但有可能所有报告故障的 OSD 都托管在一个交换机损坏的机架中，导致无法连接到另一个 OSD。为了避免这种误报，我们将报告故障的对等点视为整个集群上潜在“子集群”的代理，该“子集群”也同样滞后。这显然并非在所有情况下都是正确的，但有时会帮助我们将宽限修正定位到不满意的系统子集。</p><p><code>mon_osd_reporter_subtree_level</code> 用于根据 CRUSH 映射中的共同祖先类型将对等点分组到“子集群”中。默认情况下，只需要来自不同子树的两个报告即可报告另一个 Ceph OSD 守护进程关闭<code>down</code>。</p><p>您可以通过在 Ceph 配置文件的 [mon] 部分下添加 <code>mon_osd_min_down_reporters</code> 和 <code>mon_osd_reporter_subtree_level</code> 设置，或者在运行时设置值，来更改从唯一子树报告者的数量和向 Ceph Monitor 报告 Ceph OSD 守护进程所需的公共祖先类型。</p><p><img src="/posts/2576505844/ditaa-15d56b22a50b6b8618cf5945174c6d8fef55be6a.png" alt="img"></p><h4 id="OSDS报告-peering-失败"><a href="#OSDS报告-peering-失败" class="headerlink" title="OSDS报告 peering 失败"></a>OSDS报告 peering 失败</h4><p>如果 Ceph OSD 守护进程无法与其 Ceph 配置文件（或集群映射）中定义的任何 Ceph OSD 守护进程对等，它将每 30 秒 ping 一次 Ceph 监视器以获取集群映射的最新副本。您可以通过在 Ceph 配置文件的 [osd] 部分下添加 <code>osd_mon_heartbeat_interval</code> 设置或在运行时设置值来更改 Ceph Monitor 心跳间隔。</p><p><img src="/posts/2576505844/ditaa-0873bb9405742345397fa2e085b6ac453aa3f975.png" alt="img"></p><h4 id="OSDS报告其-status"><a href="#OSDS报告其-status" class="headerlink" title="OSDS报告其 status"></a>OSDS报告其 status</h4><p>如果 Ceph OSD 守护进程不向 Ceph Monitor 报告，Ceph Monitor 将在 <code>mon_osd_report_timeout</code> 后认为 Ceph OSD 守护进程已关闭<code>down</code> 。</p><p>当发生可报告事件（例如故障、置放组统计数据更改、up_thru 更改或在 5 秒内启动时）时，Ceph OSD 守护进程会向 Ceph 监视器发送报告。您可以通过在 Ceph 配置文件的 [osd] 部分下添加 <code>osd_mon_report_interval</code> 设置或在运行时设置值来更改 Ceph OSD 守护进程最小报告间隔。</p><p>Ceph OSD 守护进程每 120 秒向 Ceph 监视器发送一次报告，无论是否发生任何显着变化。您可以通过在 Ceph 配置文件的 [osd] 部分下添加 <code>osd_mon_report_interval_max</code> 最大设置或在运行时设置该值来更改 Ceph 监视器报告间隔。</p><p><img src="/posts/2576505844/ditaa-49d872aa7a347dee2861e634eebbb9b8c00a4d45.png" alt="img"></p><p>修改心跳设置时，应将它们包含在配置文件的 [global] 部分中。</p><h2 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h2><p>可以在 Ceph 配置文件ceph.conf中配置 Ceph OSD 进程，或在最新的版本中，配置在配置中心数据库中。但Ceph OSD 能使用默认值进行最小配置。一个最小的Ceph OSD 进程配置是设定 <code>host</code> 并且对所有配置项使用默认值</p><p>Ceph OSD 进程由从0开始的增量方式进行数字标识</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">osd.0</span><br><span class="line">osd.1</span><br><span class="line">osd.2</span><br></pre></td></tr></table></figure><p>在配置文件中，您可以通过将配置设置添加到配置文件的 [osd] 部分来指定集群中所有 Ceph OSD 守护进程的设置。要将设置直接添加到特定的 Ceph OSD 守护进程（例如主机），请将其输入到配置文件的 OSD 特定部分。例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[osd]</span><br><span class="line">        osd_journal_size = 5120</span><br><span class="line"></span><br><span class="line">[osd.0]</span><br><span class="line">        host = osd-host-a</span><br><span class="line"></span><br><span class="line">[osd.1]</span><br><span class="line">        host = osd-host-b</span><br></pre></td></tr></table></figure><h3 id="通用配置"><a href="#通用配置" class="headerlink" title="通用配置"></a>通用配置</h3><p>以下设置提供 Ceph OSD 守护进程的 ID，并确定数据和日志的路径。 Ceph 部署脚本通常会自动生成 UUID。</p><blockquote><p>不要更改数据或日志的默认路径，因为这会使以后排除 Ceph 故障变得更加困难。</p></blockquote><p><strong>osd_uuid</strong> / <strong>osd_data</strong> / <strong>osd_max_write_size</strong> / <strong>osd_max_object_size</strong> / <strong>osd_client_message_size_cap</strong> / <strong>osd_class_dir</strong></p><h3 id="monitor与-OSD-的交互"><a href="#monitor与-OSD-的交互" class="headerlink" title="monitor与 OSD 的交互"></a>monitor与 OSD 的交互</h3><p>Ceph OSD 守护进程检查彼此的心跳并定期向监视器报告。 Ceph 在很多情况下可以使用默认值。但是，如果您的网络存在延迟问题，您可能需要采用更长的间隔。有关心跳的详细讨论，请参阅配置<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mon-osd-interaction">Configuring Monitor/OSD Interaction</a>。 <a href="#mon_osd_interaction">本文</a></p><h3 id="清洗"><a href="#清洗" class="headerlink" title="清洗"></a>清洗</h3><p>Ceph 确保数据完整性的一种方法是“清理” (scrubbing) 归置组。Ceph 清理类似于对象存储层上的 fsck。</p><p>Ceph 为 PG 中所有对象的生成一个目录，并将每个主对象与其副本进行比较，确保没有对象丢失或不匹配。</p><p>浅清洗检查对象的大小和属性，并且通常每天清洗一次。深度清洗读取数据并使用校验和来确保数据完整性，通常每周进行一次。</p><p>深度清洗和浅清洗的频率由集群的配置项决定，该配置完全由用户控制。</p><p>尽管清理对于维护数据完整性很重要，但它会降低 Ceph 集群的性能。您可以调整以下设置来增加或减少擦洗操作的频率和深度。</p><h3 id="基于MCLOCK的QOS"><a href="#基于MCLOCK的QOS" class="headerlink" title="基于MCLOCK的QOS"></a>基于MCLOCK的QOS</h3><p>Ceph 对 mClock 的使用现在更加精细，可以按照<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/configuration/mclock-config-ref">mClock 配置参考</a>中描述的步骤使用。</p><h4 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h4><p><a target="_blank" rel="noopener" href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Gulati.pdf">Ceph 的 QoS 支持使用基于dmClock 算法</a> 的队列调度器实现。该算法按权重比例分配 Ceph 集群的 I/O 资源，并<strong>强制执行最小预留和最大限制的约束，以便服务可以公平地竞争资源</strong>。目前 <em>mclock_scheduler</em>操作队列将涉及 I/O 资源的 Ceph 服务分为以下几个 bucket：</p><ul><li>client op(客户端操作): 客户端发出的iops</li><li>osd subop(OSD子操作): 主OSD发出的iops</li><li>snap trim(快照修建)：与 snap trim 相关的请求</li><li>pg recovery(PG恢复)：恢复相关请求</li><li>pg scrub (PG清理)：与scrub相关的请求</li></ul><p>并且使用以下三组标签来划分资源。换句话说，每种类型的服务的份额由三个标签控制：</p><ol><li>预留(reservation)：为服务分配的最小 IOPS。</li><li>限制(limitation)：为服务分配的最大 IOPS。当lim为0，表示最大值，无上限</li><li>权重(weight)：如果出现额外容量或系统超额认购的情况，则按比例分配容量。</li></ol><p>在 Ceph 中，操作以“成本”进行分级。而为各种服务分配的资源则由这些“cost”表示。因此，因此，例如，一个服务拥有的预留越多，只要它需要，它就能保证拥有更多的资源。假设有2种服务：恢复和客户端操作：</p><ul><li>恢复：(r:1, l:5, w:1)</li><li>客户端操作：(r:2, l:0, w:9)</li></ul><p>上述设置确保恢复服务每秒获取的服务请求数不会超过5个，并且没有其他服务与之竞争，即使它需要更多（参见下面的当前实现说明）。但是，如果客户端开始发出大量I/O请求，它们也不会耗尽所有I/O资源。只要有任何此类请求，恢复作业总是被分配每秒1个请求。因此，即使在负载较高的集群中，恢复作业也不会饿死。</p><p>与此同时，客户端操作可以享受更多的I/O资源份额，因为它的权重是“9”，而它的竞争对手是“1”。在客户端操作的情况下，它不会受到lim设置的限制，因此如果没有正在进行的恢复，它可以利用所有资源。</p><blockquote><p>【14.2.8】</p><p>随着_mclock_opclass_，另一个名为_mclock_client_的mClock操作队列也可用。它根据类别划分操作，但也根据发出请求的客户端进行划分。这不仅有助于管理不同类别操作所花费资源的分配，而且还尝试确保客户端之间的公平性。</p><p>当前实施注意事项：当前实施不强制执行 limit 值。因此，如果服务超出强制限制，则操作将保留在操作队列中，直到限制恢复。</p></blockquote><p>当前实施注意事项：当前实施强制执行 limit 值。因此，如果服务超出强制限制，则操作将保留在操作队列中，直到限制恢复。</p><h4 id="MCLOCK-的精妙之处"><a href="#MCLOCK-的精妙之处" class="headerlink" title="MCLOCK 的精妙之处"></a>MCLOCK 的精妙之处</h4><p>预留和限制值的单位是每秒请求数。然而，权重在技术上没有单位，并且权重是相对于彼此的。所以如果一个请求类别的权重是1，另一个是9，那么后一个请求类别的执行比例应该是前一个的9比1。然而，这种情况只有在满足预留值后才会发生，这些值包括在预留阶段执行的 operations。</p><p>由于算法将权重标签分配给请求的方式，即使权重没有单位，人们也必须小心选择它们的值。如果权重是W，那么对于给定的请求类别，下一个进来的请求将有一个权重标签为1/W加上前一个权重标签或当前时间，以较大者为准。这意味着如果W足够大，因此1/W足够小，计算出的标签可能永远不会被分配，因为它将得到当前时间的值。最终的教训是，<strong>权重的值不应该太大。它们应该低于你期望每秒服务的请求数量</strong>。</p><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>在Ceph中，有一些因素可以减少mClock操作队列的影响。首先，对OSD的请求根据其PG标识符进行分片。每个分片都有自己的mClock队列，这些队列之间既不交互也不共享信息。可以通过配置选项osd_op_num_shards、osd_op_num_shards_hdd和osd_op_num_shards_ssd来控制分片的数量。分片数量较少会增加mClock队列的影响，但可能会产生其他不利影响。</p><p>其次，请求从 operation queue 传输到 operation sequencer ，在其中它们经历执行阶段。</p><ul><li><p>操作队列是mClock所在的地方，mClock决定下一个传输到 操作序列器 的操作。</p></li><li><p>操作序列器中允许的操作数量是一个复杂的问题。</p><p>通常我们希望在序列器中保持足够多的操作，这样在等待磁盘和网络访问完成其他操作时，它总是在某些操作上完成工作。</p><p>另一方面，一旦操作被传输到操作序列器，mClock就不再对其有控制权。因此，为了最大化mClock的影响，我们希望操作序列器中的操作尽可能少。</p><p>所以我们有一个固有的紧张关系。</p></li><li><p>影响操作序列器中操作数量的配置选项有 bluestore_throttle_bytes 、bluestore_throttle_deferred_bytes、bluestore_throttle_cost_per_io_hdd、bluestore_throttle_cost_per_io_ssd</p></li></ul><p>影响mClock算法效果的第三个因素是，我们正在使用一个分布式系统，其中请求被发送到多个OSD，每个OSD可以有多个分片。然而，我们目前使用的是mClock算法，它并不是分布式的（注意：dmClock是mClock的分布式版本）。</p><p>各种组织和个人目前正在这个代码库中实验mClock，以及他们对代码库的修改。我们希望您能在ceph-devel邮件列表上分享您在mClock和dmClock实验中的经验。</p><h4 id="MCLOCK-配置参考"><a href="#MCLOCK-配置参考" class="headerlink" title="MCLOCK 配置参考"></a>MCLOCK 配置参考</h4><p>Ceph 中的 QoS 支持是使用基于 dmClock 算法的排队调度程序来实现的。mclock 配置文件应用的参数使得可以调整 OSD 中客户端 I/O 和后台操作之间的 QoS。</p><p>为了使 mclock 的使用更加用户友好和直观，引入了 mclock 配置文件。 mclock 配置文件向用户隐藏了底层细节，从而使配置和使用 mclock 变得更加容易。mclock 配置文件需要以下输入参数来配置 QoS 相关参数：</p><ul><li>每个 OSD 的总容量 (IOPS)（ <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mclock-config-ref/#osd-capacity-determination-automated">OSD Capacity Determination (Automated)</a>）</li><li>每个 OSD 的最大顺序带宽容量 (MiB/s)</li><li>要启用的 mclock 配置文件类型</li></ul><p>使用指定配置文件中的设置，OSD 确定并应用较低级别的 mclock 和 Ceph 参数。</p><h5 id="MCLOCK-客户端类型"><a href="#MCLOCK-客户端类型" class="headerlink" title="MCLOCK 客户端类型"></a>MCLOCK 客户端类型</h5><p>mclock 调度程序处理来自不同类型 Ceph 服务的请求。从 mclock 的角度来看，每个服务都可以被视为一种客户端。根据处理的请求类型，mclock 客户端分为不同的bucket，如下表所示：</p><p><img src="/posts/2576505844/image-20240622120533466.png" alt="image-20240622120533466"></p><p>mclock 配置文件为每种客户端类型分配不同的参数，如预留、权重和限制（请参阅基于 mClock 的 QoS）。接下来的部分将更详细地描述 mclock 配置文件。</p><h5 id="MCLOCK-配置文件-定义和目的"><a href="#MCLOCK-配置文件-定义和目的" class="headerlink" title="MCLOCK 配置文件 - 定义和目的"></a>MCLOCK 配置文件 - 定义和目的</h5><p>mclock 配置文件是“一种配置设置，当应用于正在运行的 Ceph 集群时，可以限制属于不同客户端类（后台 recovery 、scrub、snap trim、客户端操作、osd 子操作）的操作 (IOPS)”。</p><p>mclock 配置文件使用用户选择的容量限制和 mclock 配置文件类型来确定 <strong>低级 mclock 资源控制</strong> 配置参数并透明地应用它们。此外，还应用其他 Ceph 配置参数。</p><ul><li>低级 mclock 资源控制参数是提供资源共享控制的预留、限制和权重，如<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/#dmclock-qos">QoS Based on mClock</a>中所述。</li></ul><h5 id="MCLOCK-配置文件类型"><a href="#MCLOCK-配置文件类型" class="headerlink" title="MCLOCK 配置文件类型"></a>MCLOCK 配置文件类型</h5><p>mclock 配置文件可大致分为 内置配置文件(<em>built-in</em>) 和 自定义配置 (<em>custom</em> )文件，</p><ul><li>在内置配置文件中，mclock 内部的后台尽力而为客户端 包括“backfill”、“scrub”、“快照修剪snap trim ”和“pg 删除”操作。</li></ul><p><strong>内置配置文件</strong></p><p>用户可以选择以下内置配置文件类型：下表中提到的值表示为该服务类型分配的 OSD 总 IOPS 容量的比例。</p><ul><li><p>balanced (default)</p><p><img src="/posts/2576505844/image-20240622121639823.png" alt="image-20240622121639823"></p><p>平衡配置文件是默认的 mClock 配置文件。此配置文件为客户端操作和后台恢复操作分配相同的预留/优先级。后台 best_effort 的操作被给予较低的预留，因此在竞争操作时需要更长的时间才能完成。此配置文件有助于满足集群的正常/稳态要求。当外部客户端性能要求不重要并且 OSD 内仍有其他后台操作需要注意时，适用于这种配置。</p><p>但在某些情况下，可能需要为客户端操作或恢复操作提供更高的分配。为了处理这种情况，可以按照下一节中提到的步骤启用备用内置配置文件。1</p></li><li><p>high_client_ops</p><p>与 OSD 中的后台操作相比，此配置文件通过为客户端操作分配更多预留和限制来优化后台活动的客户端性能。例如，该配置文件可以在持续一段时间内为 I/O 密集型应用程序提供所需的性能，但代价是恢复速度较慢。下表显示了配置文件设置的资源控制参数：</p><p><img src="/posts/2576505844/image-20240622121718061.png" alt="image-20240622121718061"></p></li><li><p>high_recovery_ops</p><p>与外部客户端和 OSD 内的其他后台操作相比，此配置文件优化了后台恢复性能。例如，管理员可以临时启用此配置文件，以加快非高峰时段的后台恢复速度。下表显示了配置文件设置的资源控制参数：</p><p><img src="/posts/2576505844/image-20240622121744884.png" alt="image-20240622121744884"></p></li></ul><p><strong>自定义配置文件</strong></p><p>该配置文件使用户可以完全控制所有 mclock 配置参数。应谨慎使用此配置文件，它适用于了解 mclock 和 Ceph 相关配置选项的高级用户。</p><h5 id="MCLOCK-内置配置文件-锁定配置选项"><a href="#MCLOCK-内置配置文件-锁定配置选项" class="headerlink" title="MCLOCK 内置配置文件 - 锁定配置选项"></a>MCLOCK 内置配置文件 - 锁定配置选项</h5><p>下部分描述了锁定为特定值的配置选项，以确保 mClock 调度程序能够提供可预测的 QoS。</p><p><strong>mclock相关参数</strong></p><p>在使用内置配置文件的系统中，这些默认值不能使用任何配置子系统命令（如 config set）或通过 config daemon 或 config Tell 接口进行更改。尽管上述命令执行结果返回成功，但 mclock QoS 参数将恢复为其各自的内置配置文件默认值。</p><p>启用内置配置文件后，mClock 调度程序会根据为每种客户端类型启用的配置文件计算低级 mclock 参数 [预留、权重、限制]。mclock参数是根据预先提供的最大OSD容量计算的。因此，使用任何内置配置文件时无法修改以下 mclock 配置参数：</p><ul><li>osd_mclock_scheduler_client_res</li><li>osd_mclock_scheduler_client_wgt</li><li>osd_mclock_scheduler_client_lim</li><li>osd_mclock_scheduler_background_recovery_res</li><li>osd_mclock_scheduler_background_recovery_wgt</li><li>osd_mclock_scheduler_background_recovery_lim</li><li>osd_mclock_scheduler_background_best_effort_res</li><li>osd_mclock_scheduler_background_best_effort_wgt</li><li>osd_mclock_scheduler_background_best_effort_lim</li></ul><p><strong>恢复/回填相关参数</strong></p><p>建议不要更改这些选项，因为内置配置文件是基于它们进行优化的。更改这些默认值可能会导致意外的性能结果。</p><p>以下与恢复和回填相关的 Ceph 选项将覆盖 mClock 默认值：</p><ul><li>osd_max_backfills</li><li>osd_recovery_max_active</li><li>osd_recovery_max_active_hdd</li><li>osd_recovery_max_active_ssd</li></ul><p>下表显示了 mClock 默认值，与当前默认值相同。这样做是为了最大限度地提高前台（客户端）操作的性能：</p><p><img src="/posts/2576505844/image-20240622122552815.png" alt="image-20240622122552815"></p><p>上述 mClock 默认值仅在必要时可以通过启用 osd_mclock_override_recovery_settings 进行修改（默认值： false）。</p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mclock-config-ref/#steps-to-modify-mclock-max-backfills-recovery-limits">Steps to Modify mClock Max Backfills/Recovery Limits</a> 部分中讨论了此步骤。</p><p><strong>sleep相关参数</strong></p><p>如果任何 mClock 配置文件（包括“自定义”）处于活动状态，则以下 Ceph 配置睡眠选项将被禁用（设置为 0），</p><ul><li>osd_recovery_sleep</li><li>osd_recovery_sleep_hdd</li><li>osd_recovery_sleep_ssd</li><li>osd_recovery_sleep_hybrid</li><li>osd_scrub_sleep</li><li>osd_delete_sleep</li><li>osd_delete_sleep_hdd</li><li>osd_delete_sleep_ssd</li><li>osd_delete_sleep_hybrid</li><li>osd_snap_trim_sleep</li><li>osd_snap_trim_sleep_hdd</li><li>osd_snap_trim_sleep_ssd</li><li>osd_snap_trim_sleep_hybrid</li></ul><p>为了确保 mclock 调度器能够确定何时从其操作队列中挑选下一个操作并将其传输到操作序列器，上述的休眠选项被禁用。这保证了所有客户端都能获得所需的服务质量 (QoS)。</p><h5 id="启用-MCLOCK-配置文件的步骤"><a href="#启用-MCLOCK-配置文件的步骤" class="headerlink" title="启用 MCLOCK 配置文件的步骤"></a>启用 MCLOCK 配置文件的步骤</h5><p>如前所述，默认 mclock 配置文件设置为 <em>balanced</em> 。内置配置文件的其他值包括 <em>high_client_ops</em> 和 <em>high_recovery_ops</em>。</p><p>如果需要更改默认配置文件，则可以使用以下命令在运行时设置选项 <em>osd_mclock_profile</em>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd.N osd_mclock_profile &lt;value&gt;</span><br></pre></td></tr></table></figure><ul><li><p>例如，要更改配置文件以允许“osd.0”上更快的恢复，可以使用以下命令切换到 <em>high_recovery_ops</em> 配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd.0 osd_mclock_profile high_recovery_ops</span><br></pre></td></tr></table></figure></li></ul><p><strong>除非您是高级用户，否则不建议使用 自定义 custom 配置文件</strong>。</p><h5 id="在内置和自定义配置文件之间切换"><a href="#在内置和自定义配置文件之间切换" class="headerlink" title="在内置和自定义配置文件之间切换"></a>在内置和自定义配置文件之间切换</h5><p>可能存在需要从内置配置文件切换到自定义配置文件的情况，反之亦然。以下各节概述了实现此目的的步骤。</p><p><strong>从内置配置文件切换到自定义配置文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">要将所有 OSD 上的配置文件更改为自定义</span></span><br><span class="line">ceph config set osd osd_mclock_profile custom</span><br></pre></td></tr></table></figure><p>切换到自定义配置文件后，可以修改所需的 mClock 配置选项。例如，要将特定 OSD（例如 osd.0）的客户端预留 IOPS 比率更改为 0.5（或 50%），可以使用以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd.0 osd_mclock_scheduler_client_res 0.5</span><br></pre></td></tr></table></figure><blockquote><p>必须注意相应地更改恢复和后台尽力服务等其他服务的预留，以确保<strong>预留的总和不超过 OSD IOPS 容量的最大比例（1.0）</strong>。</p></blockquote><p>每个分片的预留和限制参数分配基于 OSD 下支持设备 (HDD/SSD) 的类型。</p><ul><li>osd_op_num_shards_hdd、osd_op_num_shards_ssd</li></ul><p><strong>从自定义配置文件切换到内置配置文件的步骤</strong></p><p>从自定义配置文件切换到内置配置文件需要一个中间步骤，即从中央配置数据库中删除自定义设置，以使更改生效。</p><ol><li><p>使用以下命令设置所需的内置配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd &lt;mClock Configuration Option&gt;</span><br></pre></td></tr></table></figure><p>例如，要将所有 OSD 上的内置配置文件设置为 high_client_ops，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd osd_mclock_profile high_client_ops</span><br></pre></td></tr></table></figure></li><li><p>使用以下命令确定中央配置数据库中现有的自定义 mClock 配置设置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config dump</span><br></pre></td></tr></table></figure></li><li><p>从中央配置数据库中删除上一步中确定的自定义 mClock 配置设置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config rm osd &lt;mClock Configuration Option&gt;</span><br></pre></td></tr></table></figure><p>例如，要删除在所有 OSD 上设置的配置选项 osd_mclock_scheduler_client_res，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config rm osd osd_mclock_scheduler_client_res</span><br></pre></td></tr></table></figure></li><li><p>从中央配置数据库中删除所有现有的自定义 mClock 配置设置后，与 high_client_ops 相关的配置设置将生效。例如，要验证 osd.0 上的设置，请使用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config show osd.0</span><br></pre></td></tr></table></figure></li></ol><h5 id="在-MCLOCK-配置文件之间临时切换"><a href="#在-MCLOCK-配置文件之间临时切换" class="headerlink" title="在 MCLOCK 配置文件之间临时切换"></a>在 MCLOCK 配置文件之间临时切换</h5><p>要临时在 mClock 配置文件之间切换，可以使用以下命令来覆盖设置：</p><blockquote><p>本部分适用于高级用户或实验测试。建议不要在正在运行的集群上使用以下命令，因为它可能会产生意外结果。</p><p>使用以下命令在OSD上进行的配置更改是短暂的，在重新启动时会丢失。还值得注意的是，使用以下命令覆盖的配置选项不能使用 ceph config set osd.N … 命令进一步修改。直到重新启动给定的OSD，更改才会生效。这是根据配置子系统设计有意为之的。然而，仍然可以使用下面提到的命令临时进行任何进一步的修改。1.</p></blockquote><p>运行如图所示的injectargs命令来覆盖mclock设置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph tell osd.N injectargs &#x27;--&lt;mClock Configuration Option&gt;=&lt;value&gt;&#x27;</span><br></pre></td></tr></table></figure><p>例如，以下命令会覆盖 osd.0 上的 osd_mclock_profile 选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph tell osd.0 injectargs &#x27;--osd_mclock_profile=high_recovery_ops&#x27;</span><br></pre></td></tr></table></figure><hr><p>可以使用的替代命令是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon osd.N config set &lt;mClock Configuration Option&gt; &lt;value&gt;</span><br></pre></td></tr></table></figure><p>例如，以下命令会覆盖 osd.0 上的 osd_mclock_profile 选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon osd.0 config set osd_mclock_profile high_recovery_ops</span><br></pre></td></tr></table></figure><p>还可以使用上述命令临时修改自定义配置文件的各个 QoS 相关配置选项。</p><h5 id="修改-MCLOCK-最大回填-恢复限制的步骤"><a href="#修改-MCLOCK-最大回填-恢复限制的步骤" class="headerlink" title="修改 MCLOCK 最大回填/恢复限制的步骤"></a>修改 MCLOCK 最大回填/恢复限制的步骤</h5><blockquote><p>本部分适用于高级用户或实验测试。建议保留正在运行的集群上的默认值，因为修改它们可能会产生意外的性能结果。仅当集群无法应对默认设置/表现出较差的性能或在测试集群上执行实验时，才可以修改这些值。</p><p>最大备份/恢复相关的参数在<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mclock-config-ref/#recovery-backfill-options">Recovery/Backfill Options</a> 部分。修改mClock默认的回填/恢复限制受到osd_mclock_override_recovery_settings选项的控制，该选项默认设置为false。如果没有设置门控选项而尝试修改任何默认恢复/回填限制，将重置该选项回到mClock的默认值，并在集群日志中记录一条警告消息。请注意，默认值重新生效可能需要几秒钟。使用如下所示的config show命令验证限制。</p></blockquote><ol><li><p>使用以下命令将所有 osd 上的 osd_mclock_override_recovery_settings 配置选项设置为 true：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd osd_mclock_override_recovery_settings true</span><br></pre></td></tr></table></figure></li><li><p>使用以下命令设置所需的最大回填/恢复选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd osd_max_backfills &lt;value&gt;</span><br></pre></td></tr></table></figure><p>例如，以下命令将所有 osd 上的 osd_max_backfills 选项修改为 5。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd osd_max_backfills 5</span><br></pre></td></tr></table></figure></li><li><p>等待几秒钟并使用以下命令验证特定 OSD 的运行配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config show osd.N | grep osd_max_backfills</span><br></pre></td></tr></table></figure><p>例如，以下命令显示 osd.0 上 osd_max_backfills 的运行配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config show osd.0 | grep osd_max_backfills</span><br></pre></td></tr></table></figure></li><li><p>使用以下命令将所有 osd 上的 osd_mclock_override_recovery_settings 配置选项重置为 false：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set osd osd_mclock_override_recovery_settings false</span><br></pre></td></tr></table></figure></li></ol><h4 id="OSD-容量确定（自动）"><a href="#OSD-容量确定（自动）" class="headerlink" title="OSD 容量确定（自动）"></a>OSD 容量确定（自动）</h4><p>OSD的总IOPS容量是在OSD初始化期间自动确定的。这是通过运行OSD bench工具并根据设备类型覆盖osd_mclock_max_capacity_iops_[hdd, ssd]选项的默认值来实现的。不需要用户进行任何其他操作/输入来设置OSD容量。</p><p>如果您希望手动对 OSD 进行基准测试或手动调整 Bluestore 限制参数，请参阅手动对 OSD 进行基准测试的步骤（可选）部分。</p><p>集群启动后，您可以使用以下命令验证 OSD 的容量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config show osd.N osd_mclock_max_capacity_iops_[hdd, ssd]</span><br></pre></td></tr></table></figure><p>例如，以下命令显示底层设备类型为 SSD 的 Ceph 节点上“osd.0”的最大容量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config show osd.0 osd_mclock_max_capacity_iops_ssd</span><br></pre></td></tr></table></figure><h5 id="通过自动化测试减少不切实际的-OSD-容量"><a href="#通过自动化测试减少不切实际的-OSD-容量" class="headerlink" title="通过自动化测试减少不切实际的 OSD 容量"></a>通过自动化测试减少不切实际的 OSD 容量</h5><p>在某些条件下，OSD bench工具可能会根据驱动器配置和其他与环境相关的条件显示不切实际/夸大的结果。为了减轻这种不切实际的容量对性能的影响，定义并使用了一些依赖于OSD设备类型的阈值配置选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">osd_mclock_iops_capacity_threshold_hdd = 500</span><br><span class="line">osd_mclock_iops_capacity_threshold_ssd = 80000</span><br></pre></td></tr></table></figure><p><strong>回退到使用默认 OSD 容量（自动）</strong></p><p>如果OSD bench根据底层设备类型报告的测量值超过了上述阈值，回退机制将恢复到osd_mclock_max_capacity_iops_hdd或osd_mclock_max_capacity_iops_ssd的默认值。阈值配置选项可以根据使用的驱动器类型重新配置。此外，如果测量值超过阈值，将记录一个集群警告。例如，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2022-10-27T15:30:23.270+0000 7f9b5dbe95c0  0 log_channel(cluster) log [WRN]</span><br><span class="line">: OSD bench result of 39546.479392 IOPS exceeded the threshold limit of</span><br><span class="line">25000.000000 IOPS for osd.1. IOPS capacity is unchanged at 21500.000000</span><br><span class="line">IOPS. The recommendation is to establish the osd&#x27;s IOPS capacity using other</span><br><span class="line">benchmark tools (e.g. Fio) and then override</span><br><span class="line">osd_mclock_max_capacity_iops_[hdd|ssd].</span><br></pre></td></tr></table></figure><p><strong>如果默认值不准确，则运行自定义驱动器基准（手动）</strong></p><p>如果默认的OSD容量不准确，建议使用您偏好的工具（例如Fio）在驱动器上运行自定义基准测试，</p><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mclock-config-ref/#steps-to-manually-benchmark-an-osd-optional">手动对OSD基准进行测试</a></p><p>然后按照“<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/mclock-config-ref/#specifying-max-osd-capacity">Specifying Max OSD Capacity</a>”部分中描述的方式覆盖osd_mclock_max_capacity_iops_[hdd, ssd]选项。</p><p>在找到替代机制之前，这一步是非常推荐的。</p><h3 id="回填backfilling"><a href="#回填backfilling" class="headerlink" title="回填backfilling"></a>回填backfilling</h3><p>当您向集群添加或删除 Ceph OSD 守护进程时，CRUSH 将通过将置放组移入或移出 Ceph OSD 来重新平衡集群，以恢复平衡的利用率。迁移置放群组及其包含的对象的过程会大大降低集群的运行性能。为了保持操作性能，Ceph 通过“回填” （backfill） 执行此迁移，这允许 Ceph 将回填 (backfill) 操作设置为低于读取或写入数据请求的优先级。</p><h3 id="OSD-map"><a href="#OSD-map" class="headerlink" title="OSD map"></a>OSD map</h3><p>OSD map 反映了集群中运行的 OSD 守护进程。随着时间的推移， map epoch 的数量会增加。 Ceph 提供了一些设置来确保 Ceph 在 OSD map变大时表现良好。</p><h3 id="恢复recovery"><a href="#恢复recovery" class="headerlink" title="恢复recovery"></a>恢复recovery</h3><p>当集群启动或 Ceph OSD 守护进程崩溃并重新启动时，OSD 会在写入发生之前开始与其他 Ceph OSD 守护进程对齐。有关详细信息，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg#peering">Monitoring OSDs and PGs</a></p><ul><li><p>如果 Ceph OSD 守护进程崩溃并重新上线，通常它会与 PG 中包含更新版本对象的其他 Ceph OSD 守护进程不同步。发生这种情况时，Ceph OSD 守护进程会进入恢复 recovery 模式，并寻求获取最新的数据副本并使其 map 恢复到最新状态。</p></li><li><p>根据 Ceph OSD 守护进程关闭的时间长短，OSD 的对象和归置组可能会明显过时。</p></li><li><p>此外，如果一个故障域（例如，一个机架）发生故障，多个 Ceph OSD 守护进程可能会同时恢复在线。这会使恢复过程耗时且占用资源</p></li></ul><p>为了维持操作性能，Ceph 在执行恢复时限制恢复请求数量、线程和对象块大小，这使得 Ceph 在降级状态下也能良好运行。</p><h3 id="tiering分层"><a href="#tiering分层" class="headerlink" title="tiering分层"></a>tiering分层</h3><p>有关分层代理在高速模式下刷新脏对象的信息，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/operations/pools#cache-target-dirty-high-ratio">cache target dirty high ratio</a> 。</p><h3 id="Filestore"><a href="#Filestore" class="headerlink" title="Filestore"></a>Filestore</h3><p>Ceph 为 filestore 存储后端的 OSD 构建和挂载文件系统</p><p>使用 Filestore 时，journal 大小应至少是预期驱动器速度乘以 filestore_max_sync_interval 的乘积的两倍。然而，最常见的做法是对日志驱动器（通常是 SSD）进行分区，然后挂载它，以便 Ceph 将整个分区用于日志。</p><p><code>osd_mkfs_options &#123;fs-type&#125;</code> ：</p><ul><li>创建新的 Ceph Filestore OSD 时的文件系统类型 fs-type</li><li>类型：字符串</li><li>默认使用 xfs <code>-f -i 2048</code></li><li>如：osd_mkfs_options {fs-type}</li></ul><p><code>osd_mount_options &#123;fs-type&#125;</code></p><ul><li>当挂载 Ceph Filestore OSD 时的文件系统类型 fs-type</li><li>类型：字符串</li><li>默认使用xfs <code>rw,noatime,inode64</code></li><li>使用其他文件系统 <code>rw, noatime</code></li><li>如：osd_mount_options_xfs = rw, noatime, inode64, logbufs=8</li></ul><h4 id="Journal-设置"><a href="#Journal-设置" class="headerlink" title="Journal 设置"></a>Journal 设置</h4><p>这部分仅用于旧的Filestore OSD 存储后端。尽管L版已经推荐Bluestore为默认和推荐的</p><p>默认情况下，Ceph 希望用户在以下路径中配置 Ceph OSD 守护进程的 journal ，该路径通常是设备或分区的符号链接：<code>/var/lib/ceph/osd/$cluster-$id/journal</code></p><ul><li>当使用单一设备类型（例如，旋转驱动器）时，journal 应位于同一位置：逻辑卷（或分区）应与 <code>data</code> 逻辑卷位于同一设备中。</li><li>当混合使用快速（SSD、NVMe）设备和慢速设备（如旋转驱动器）时，将 journal 放置在较快的设备上是有意义的，而 <code>data</code> 则完全占用较慢的设备。</li></ul><p>默认 osd_journal_size 值为 5120（5 GB），但它可以更大，在这种情况下，需要在 ceph.conf 文件中设置它。 10 GB 的值在实践中很常见：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd_journal_size = 10240</span><br></pre></td></tr></table></figure><p><strong>osd_journal</strong></p><p><strong>osd_journal_size</strong></p><h3 id="Bluestore"><a href="#Bluestore" class="headerlink" title="Bluestore"></a>Bluestore</h3><h4 id="设备"><a href="#设备" class="headerlink" title="设备"></a>设备</h4><p>Ceph Bluestore 直接使用原始块设备，并不会在其使用的设备上创建或安装传统文件系统(如 XFS 或 EXT4等)；以“raw” (原始)方式直接读取和写入设备，能够实现更好的性能和效率，为 Ceph 集群提供了更高效和高性能的存储后端。</p><ul><li><p>避免使用传统文件系统，带来的好处：</p><ul><li><p><strong>性能</strong>：直接写入原始块设备，避免传统文件系统的开销，Bluestore 可以实现更高的 I/O 吞吐量和更低的延迟。</p><p>将元数据和数据分开管理，从而更有效地利用存储并提升性能。</p><p>集成的键值存储，内部使用RocksDB键值存储进行元数据操作，有助于高效地管理对象元数据。</p></li><li><p><strong>灵活性</strong>：Bluestore 的设计专为 Ceph 的特定需求量身定制，允许更好的优化和功能，如内联压缩和校验和。</p></li><li><p><strong>控制权</strong>：对块设备的直接控制使得更复杂和高效的存储管理技术成为可能。</p></li><li><p><strong>数据校验和</strong>：Bluestore 在存储引擎层提供数据校验和压缩，增强了数据完整性和存储效率。</p></li></ul></li></ul><p>BlueStore 管理一个、两个或在某些情况下三个存储设备。Linux/Unix 意义上的 “devices” ，即/dev 或 /devices 下列出的资源。每个设备可以是整个存储驱动器 (storage drive) 、存储驱动器的分区(partition)、逻辑卷(logical volume)。 在最简单的情况下，BlueStore 会消耗单个存储设备的全部。该设备称为主设备。主设备由数据目录中的块符号 <code>block</code> 链接标识。</p><p>数据目录是一种 tmpfs 挂载。当这个数据目录被 ceph-volume 启动或激活时，会填充元数据文件链接，这些元数据文件和链接保存有关OSD的信息。这些文件包括：</p><ul><li>OSD 的标识符</li><li>OSD所述集群的名称</li><li>OSD 的私钥 keyring</li></ul><p>在更复杂的情况下，BlueStore 部署在一台或两台附加设备上：</p><ul><li><p>一个写前日志（write-ahead log, WAL）设备（在数据目录中标识为block.wal）可以被用来分离 <strong>BlueStore的内部 journal</strong> (write-ahead log)。只有在WAL设备比主设备更快的情况下，使用WAL设备才有优势（例如，如果WAL设备是SSD而主设备是HDD）。</p></li><li><p>一个数据库（DB device, DB）设备（在数据目录中标识为block.db）可以被用来存储 <strong>BlueStore的内部元数据</strong>。BlueStore（或者更准确地说，嵌入的RocksDB）会尽可能多地在DB设备上放置元数据，以提高性能。</p><p>如果DB device满了，元数据将回溢到主设备上（在没有DB device的情况下，元数据就会位于主设备上）。再次强调，只有在DB设备比主设备更快的情况下，才应该配置一个DB设备。</p></li></ul><p>如果只有少量可用的快速存储（例如，小于 1 GB），我们建议使用可用空间作为 WAL 设备。</p><p>但如果有更快的存储可用，那么配置 DB device 就更有意义。但如果有更快的存储可用，那么配置数据库设备就更有意义。</p><p>由于 BlueStore journal 总是被放置在可用的最快设备上，使用DB设备提供了与使用WAL设备相同的优势，同时还可以允许将额外的元数据存储在主设备之外（前提是它能够容纳）。DB设备使得这成为可能，因为 <strong>每当指定了DB设备但未明确指定WAL设备时，WAL将隐式地与DB一起位于更快的设备上</strong>。</p><p>要配置单设备（并置）BlueStore OSD，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-volume lvm prepare --bluestore --data &lt;device&gt;</span><br></pre></td></tr></table></figure><p>要指定 WAL 设备或 DB 设备，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-volume lvm prepare --bluestore --data &lt;device&gt; --block.wal &lt;wal-device&gt; --block.db &lt;db-device&gt;</span><br></pre></td></tr></table></figure><h5 id="配置策略"><a href="#配置策略" class="headerlink" title="配置策略"></a>配置策略</h5><p>BlueStore 与 Filestore 的不同之处在于，部署 BlueStore OSD 的方法有多种。但是，需检查以下两种常见的安排即可阐明 BlueStore 的整体部署策略：</p><p><strong>仅块（数据）</strong></p><p>如果所有设备都是同一类型（例如，它们都是 HDD），并且没有可用于存储 bluestore元数据的快速设备，则仅指定块设备并保留 block.db 和 block.wal 的并置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-volume lvm create --bluestore --data /dev/sda</span><br></pre></td></tr></table></figure><p>如果用于 BlueStore OSD 的设备是预先创建的逻辑卷，则名为 ceph-vg/block-lv 的逻辑卷的 lvm 调用如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-volume lvm create --bluestore --data ceph-vg/block-lv</span><br></pre></td></tr></table></figure><p><strong>块和 bloak.db</strong></p><p><span id="mixture_spining_solid_drive"></span>&gt;</p><p>如果您混合使用快速和慢速设备（例如 SSD 或 HDD），那么我们建议将 block.db 放置在较快的设备上，而块（即数据）存储在较慢的设备（即较慢的设备，即HDD）上。</p><p>您必须手动创建 volume groups 和其 逻辑卷(logical volume)。因为 ceph-volume 工具目前无法自动执行此操作。以下过程说明了手动创建volume groups和逻辑卷</p><p>假设有四个旋转驱动器（sda、sdb、sdc 和 sdd）和一个（快速）SSD (sdx)。请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">首先，要创建卷组，</span></span><br><span class="line">vgcreate ceph-block-0 /dev/sda</span><br><span class="line">vgcreate ceph-block-1 /dev/sdb</span><br><span class="line">vgcreate ceph-block-2 /dev/sdc</span><br><span class="line">vgcreate ceph-block-3 /dev/sdd</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">接下来，要为块创建逻辑卷</span></span><br><span class="line">lvcreate -l 100%FREE -n block-0 ceph-block-0</span><br><span class="line">lvcreate -l 100%FREE -n block-1 ceph-block-1</span><br><span class="line">lvcreate -l 100%FREE -n block-2 ceph-block-2</span><br><span class="line">lvcreate -l 100%FREE -n block-3 ceph-block-3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">因为有四个 HDD，所以会有四个 OSD。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">假设/dev/sdx中有一个200GB的SSD，我们可以通过运行以下命令创建四个50GB的逻辑卷：</span></span><br><span class="line">vgcreate ceph-db-0 /dev/sdx</span><br><span class="line">lvcreate -L 50GB -n db-0 ceph-db-0</span><br><span class="line">lvcreate -L 50GB -n db-1 ceph-db-0</span><br><span class="line">lvcreate -L 50GB -n db-2 ceph-db-0</span><br><span class="line">lvcreate -L 50GB -n db-3 ceph-db-0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">最后，创建四个 OSD</span></span><br><span class="line">ceph-volume lvm create --bluestore --data ceph-block-0/block-0 --block.db ceph-db-0/db-0</span><br><span class="line">ceph-volume lvm create --bluestore --data ceph-block-1/block-1 --block.db ceph-db-0/db-1</span><br><span class="line">ceph-volume lvm create --bluestore --data ceph-block-2/block-2 --block.db ceph-db-0/db-2</span><br><span class="line">ceph-volume lvm create --bluestore --data ceph-block-3/block-3 --block.db ceph-db-0/db-3</span><br></pre></td></tr></table></figure><p>此过程完成后，应该有四个 OSD，块应该位于四个 HDD 上，每个 HDD 在共享 SSD 上应该有一个 50GB 的逻辑卷（具体来说，一个 DB 设备）。</p><h5 id="设备容量"><a href="#设备容量" class="headerlink" title="设备容量"></a>设备容量</h5><p>使用 <a href="#mixture_spining_solid_drive">混合HDD和 SDD 设置</a> 时，为 BlueStore 创建足够大的 block.db 逻辑卷非常重要。与 block.db 关联的逻辑卷应具有尽可能大的逻辑卷。</p><p>通常建议 block.db 的大小介于块大小 (逻辑卷大小) 的 1% 到 4% 之间。</p><ul><li><p>对于RGW的工作负载，建议 block.db 至少为 块大小的 4%，因为 RGW 大量使用 block.db 来存储元数据，特别是 omap keys</p><p>如果块大小为 1TB，则 block.db 的大小应至少为 40GB。</p></li><li><p>然而，对于 RBD 工作负载，block.db 通常需要不超过块大小的 1% 到 2%。</p></li></ul><p>在较旧的版本中，内部 level sizes 使得 DB 只能充分利用与 L0、L0+L1、L1+L2 等总和相对应的特定分区/逻辑卷大小——也就是说，特定默认设置，大小大约为 3GB、30GB、300GB 等。尽管可以通过将这些数字加倍到 6GB、60GB 和 600GB 来促进数据库压缩，但大多数部署并不会从适应 L3 及更高级别的大小中获益。</p><p>Nautilus 14.2.12、Octopus 15.2.6 及后续版本中的改进允许更好地利用任意大小的 DB device。此外，P版本还带来了实验性的动态 level 支持。由于这些进步，旧版本的用户可能希望通过配置更大的数据库设备来提前计划，以便在将来进行升级时可以实现规模优势。</p><p>当不混合使用快速和慢速设备时，不需要为 block.db 或 block.wal 创建单独的逻辑卷。 BlueStore 会自动将这些设备放置在 block 空间内。</p><h4 id="自动调整缓存大小"><a href="#自动调整缓存大小" class="headerlink" title="自动调整缓存大小"></a>自动调整缓存大小</h4><p>在满足某些条件的情况下，可以配置BlueStore自动调整其缓存大小：</p><ul><li>必须将TCMalloc配置为内存分配器，</li><li>必须启用 bluestore_cache_autotune 配置选项（注意，目前它是默认启用的）。</li></ul><p>当自动缓存大小调整生效时，BlueStore 会尝试将 OSD 堆内存使用量保持在特定目标大小（由 osd_memory_target 确定）以下。这种方法使用了一种尽力而为的算法，并且缓存不会缩小到小于osd_memory_cache_min定义的大小。缓存比例是根据优先级层次结构选择的。但是，如果没有优先级信息，则使用bluestore_cache_meta_ratio和bluestore_cache_kv_ratio选项中指定的值作为备用缓存比例。</p><h4 id="手动调整缓存大小"><a href="#手动调整缓存大小" class="headerlink" title="手动调整缓存大小"></a>手动调整缓存大小</h4><p>如果在选项中没有指定（即，如果它保持为0），那么Ceph将使用不同的配置选项来确定默认的内存预算：如果主设备是HDD，则使用bluestore_cache_size_hdd，如果主设备是SSD，则使用bluestore_cache_size_ssd。</p><p>BlueStore 和 Ceph OSD 守护进程的其余部分 best-effort 的作业 在此内存预算内。请注意，除了配置的缓存大小外，OSD 本身也会消耗内存。由于内存碎片和其他分配器开销，存在额外的利用率。</p><p>配置的缓存内存预算用于存储以下类型的内容：</p><ul><li>Key/Value metadata（即RocksDB的内部缓存）</li><li>BlueStore metadata</li><li>BlueStore数据（即最近读取或最近写入的对象数据）</li></ul><p>缓存内存使用情况由配置选项 bluestore_cache_meta_ratio 和 bluestore_cache_kv_ratio 控制。缓存中用于数据的保留部分由有效的BlueStore缓存大小（这取决于相关的 <code>bluestore_cache_size[_ssd|_hdd]</code> 参数和主设备的设备类别）以及“meta”和“kv”比例共同决定。这个数据比例可以通过以下公式计算：</p><p><code>&lt;effective_cache_size&gt; * (1 - bluestore_cache_meta_ratio - bluestore_cache_kv_ratio)</code></p><h4 id="校验和"><a href="#校验和" class="headerlink" title="校验和"></a>校验和</h4><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#checksums">https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#checksums</a></p><h4 id="内联压缩"><a href="#内联压缩" class="headerlink" title="内联压缩"></a>内联压缩</h4><p>BlueStore 支持使用 snappy、zlib、lz4 或 zstd 进行内联压缩。</p><p>在Bluestore中的数据是否压缩取决于两个因素：</p><ol><li>压缩模式 compression mode<ul><li>none：从不压缩数据。</li><li>passive：除非写入操作具有可压缩提示集，否则不压缩数据。</li><li>aggressive：除非写入操作具有不可压缩提示集，否则压缩数据。</li><li>force：无论如何都尝试压缩数据。</li></ul></li><li>与写入操作相关的任何客户端提示。</li></ol><p>有关可压缩和不可压缩 I/O 提示的更多信息 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/api/librados/#c.rados_set_alloc_hint"><code>rados_set_alloc_hint()</code></a></p><p>请注意，只有当数据块的大小充分减小时（由 bluestore 压缩所需比率设置确定），Bluestore 中的数据才会被压缩。无论使用哪种压缩模式，如果数据块太大，则会被丢弃，并存储原始（未压缩）数据。例如，如果bluestore压缩所需比率设置为0.7，则仅当压缩数据的大小不超过原始数据大小的70%时才会存储数据压缩版本。</p><p>压缩模式、压缩算法、压缩所需比率、最小 blob 大小和最大 blob 大小设置可以通过每个池属性或通过全局配置选项指定。要指定池属性，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; compression_algorithm &lt;algorithm&gt;</span><br><span class="line">ceph osd pool set &lt;pool-name&gt; compression_mode &lt;mode&gt;</span><br><span class="line">ceph osd pool set &lt;pool-name&gt; compression_required_ratio &lt;ratio&gt;</span><br><span class="line">ceph osd pool set &lt;pool-name&gt; compression_min_blob_size &lt;size&gt;</span><br><span class="line">ceph osd pool set &lt;pool-name&gt; compression_max_blob_size &lt;size&gt;</span><br></pre></td></tr></table></figure><h4 id="RocksDB-分片"><a href="#RocksDB-分片" class="headerlink" title="RocksDB 分片"></a>RocksDB 分片</h4><p>BlueStore 维护了多种类型的内部键值数据，所有这些数据都存储在 RocksDB 中。 BlueStore 中的每种数据类型都分配有一个唯一的前缀。</p><ul><li>在 Pacific 版本之前，所有键值数据都存储在单个 RocksDB 列族中：“默认”。</li><li>在 Pacific 及更高版本中，BlueStore 可以将键值数据划分为多个 RocksDB 列族。</li></ul><p>当键相似时，BlueStore 可以实现更好的缓存和更精确的压缩：具体来说，当键具有相似的访问频率、相似的修改频率和相似的生命周期时。在这种情况下，性能得到提高，压缩期间所需的磁盘空间也更少（因为每个列族更小并且能够独立于其他列族进行压缩）。</p><p>Pacific 或更高版本中部署的 OSD 默认使用 RocksDB 分片。但是，如果 Ceph 已从先前版本升级到 Pacific 或更高版本，则在 Pacific 之前创建的任何 OSD 上都会禁用分片。要启用分片并将 Pacific 默认值应用于特定 OSD，请停止 OSD 并运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph-bluestore-tool \</span><br><span class="line"> --path &lt;data path&gt; \</span><br><span class="line"> --sharding=&quot;m(3) p(3,0-12) O(3,0-13)=block_cache=&#123;type=binned_lru&#125; L P&quot; \</span><br><span class="line"> reshard</span><br></pre></td></tr></table></figure><h4 id="SPDK的使用"><a href="#SPDK的使用" class="headerlink" title="SPDK的使用"></a>SPDK的使用</h4><p>使用 NVMe 设备的 SPDK 驱动程序<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#spdk-usage">https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#spdk-usage</a></p><h4 id="最小分配规模"><a href="#最小分配规模" class="headerlink" title="最小分配规模"></a>最小分配规模</h4><p>BlueStore在底层存储设备上分配了一定额度的最小存储空间。实际上，这是即使是最小的RADOS对象在每个OSD的主设备上也需要占用的最少容量。</p><p>相关的配置选项——bluestore_min_alloc_size，其值来源于bluestore_min_alloc_size_hdd 或 bluestore_min_alloc_size_ssd，具体取决于OSD的rotational 属性。因此，如果在HDD上创建OSD，BlueStore将使用bluestore_min_alloc_size_hdd 的当前值进行初始化；而对于SSD OSD（包括NVMe设备），Bluestore将使用bluestore_min_alloc_size_ssd的当前值进行初始化。</p><ul><li><p>在Mimic及更早的版本中，旋转介质（HDD）的默认值是64KB，非旋转介质（SSD）的默认值是16KB。Octopus版本将非旋转介质（SSD）的默认值改为4KB，而Pacific版本将旋转介质（HDD）的默认值也改为4KB。</p></li><li><p>这些变化是由托管大量小文件（S3/Swift 对象）的 Ceph RADOS Gateway (RGW) 部署所经历的空间放大导致的。</p><p>例如，当RGW客户端存储一个1KB的S3对象时，该对象被写入一个单一的RADOS对象。根据默认的min_alloc_size值，底层驱动器空间分配了4KB。这意味着大约有3KB（即4KB减去1KB）被分配但从未使用：这对应于300%的额外开销或25%的效率。同样，一个5KB的用户对象将作为两个RADOS对象存储，一个是4KB的RADOS对象，另一个是1KB的RADOS对象，结果是4KB的设备容量被浪费。然而，在这种情况下，额外开销的百分比要小得多。可以考虑这类似于模运算的余数。因此，随着对象大小的增加，额外开销的百分比会迅速下降。</p><p>还有一个容易被忽视的细微之处：前面描述的放大现象在每个副本上都会发生。例如，当使用数据的三个副本（3R）作为默认设置时，一个1KB的S3对象实际上会浪费大约9KB的存储设备容量。如果使用纠删码（EC）而不是复制，放大可能会更高：对于一个k=4, m=2的池，我们的1KB S3对象会分配24KB（即4KB乘以6）的设备容量。</p></li></ul><p>当RGW存储桶池包含许多相对较大的用户对象时，这种现象的影响通常是微不足道的。然而，对于那些预期会有大量相对较小的用户对象的部署，应该考虑到这种现象的影响。</p><p>4KB的默认值与传统的HDD和SSD设备非常匹配。然而，某些新颖的 coarse-IU（间接单元）QLC SSDs在创建OSD时，如果将bluestore_min_alloc_size_ssd指定为与设备的IU相匹配，则性能和开销表现最佳：这可能是8KB、16KB甚至64KB。这些新颖的存储驱动器可以实现与传统TLC SSDs相媲美的读取性能，以及比HDD更快的写入性能，同时具有比TLC SSDs更高的密度和更低的成本。</p><ul><li>请注意，在这些新颖设备上创建OSD时，必须小心地将非默认值仅应用于适当的设备，而不是传统的HDD和SSD设备。通过仔细安排OSD的创建顺序，使用自定义OSD设备类别，特别是使用中心配置掩码，可以避免错误。</li></ul><p>在Quincy及以后的版本中，您可以使用bluestore_use_optimal_io_size_for_min_alloc_size选项来允许在创建每个OSD时自动发现正确的值。请注意，使用bcache、OpenCAS、dmcrypt、ATA over Ethernet、iSCSI或其他设备层和抽象技术可能会混淆正确值的确定。此外，部署在VMware存储之上的OSD有时会被发现报告的旋转属性与底层硬件不匹配。</p><p>我们建议在启动时通过日志和管理套接字检查此类OSD，以确保它们的行为是正确的。请注意，这种检查可能不会按预期在旧内核上工作。要检查这个问题，请检查/sys/block/<drive>/queue/optimal_io_size的存在和取值。</drive></p><p>为检查某个OSD，运行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd metadata osd.1701 | egrep rotational\|alloc</span><br></pre></td></tr></table></figure><p>这种空间放大可能表现为 ceph df 报告的原始数据与存储数据之比异常高。<code>ceph osd df</code> 报告的 %USE / VAR 值与其他看似相同的OSD相比可能也异常高。最后，如果池中使用的 OSD 的 min_alloc_size 值不匹配，可能会出现意外的均衡器行为。</p><p>这个BlueStore属性仅在OSD创建时生效；如果后来更改了该属性，特定OSD的行为不会改变，除非且直到OSD被销毁并使用适当的选项值重新部署。升级到较新的Ceph版本不会改变在旧版本下部署或使用其他设置的OSD所使用的值。</p><h4 id="DSA（数据流加速器）的使用"><a href="#DSA（数据流加速器）的使用" class="headerlink" title="DSA（数据流加速器）的使用"></a>DSA（数据流加速器）的使用</h4><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#dsa-data-streaming-accelerator-usage">https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#dsa-data-streaming-accelerator-usage</a></p><h3 id="数据放置"><a href="#数据放置" class="headerlink" title="数据放置"></a>数据放置</h3><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref">Pool &amp; PG Config Reference</a></p><p>CRUSH 算法分配给每个池的PG数量由监视器集群中的集中配置数据库中的参数配置值决定。</p><ul><li>Ceph 的容器化部署（使用 cephadm 或 Rook 进行的部署）和 Ceph 的非容器化部署都依赖于监视器集群中的中央配置数据库中的值来将 PG 分配给池。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">要查看控制给定池中归置组数量的变量值，请运行以下形式的命令：</span></span><br><span class="line">ceph config get osd osd_pool_default_pg_num</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">要设置控制给定池中归置组数量的变量值</span></span><br><span class="line">ceph config set osd osd_pool_default_pg_num</span><br></pre></td></tr></table></figure><p>在某些情况下，建议覆盖某些默认值。例如，您可能认为设置池的副本大小并覆盖池中PG的默认数量是明智之举。</p><p>默认情况下，Ceph为每个RADOS对象设置三副本。如果想为RADOS对象设置四副本(一个主副本和三个备份)，需要修改 <code>osd_pool_default_size</code> 中的默认值。</p><p>如果您想让 Ceph 接受对降级 PG 的 I/O 操作，将 <code>osd_pool_default_min_size</code> 设置为小于 <code>osd_pool_default_size</code> 。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">osd_pool_default_size = 3  # Write an object 3 times.</span><br><span class="line">osd_pool_default_min_size = 2 # Accept an I/O operation to a PG that has two copies of an object.</span><br></pre></td></tr></table></figure><p>确保您拥有 <strong>PG数</strong> 是一个实数，推荐每个OSD大概100个PG。</p><ul><li><p>在设置PG数时，OSD总数*100/对象的副本数</p></li><li><p>如，当我们有10个OSD，且副本数设置为4，则 (10*100/4=250)，使用最近的2次幂，即设为 256</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">osd_pool_default_pg_num = 256</span><br><span class="line">osd_pool_default_pgp_num = 256</span><br></pre></td></tr></table></figure></li></ul></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/2576505844/" title="Ceph文档-存储集群-综述">https://amostian.github.io/posts/2576505844/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/" rel="tag"><i class="fa fa-tags"></i> 分布式存储</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"><i class="fa fa-tags"></i> 分布式</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/2576505842/" rel="prev" title="Ceph文档-存储集群-综述"><i class="fa fa-chevron-left"></i> Ceph文档-存储集群-综述</a></div><div class="post-nav-item"><a href="/posts/3924783311/" rel="next" title="Ceph参数注解-mgr">Ceph参数注解-mgr <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#host"><span class="nav-text">host</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3"><span class="nav-text">网络相关</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#monitors"><span class="nav-text">monitors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81"><span class="nav-text">身份认证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OSDs"><span class="nav-text">OSDs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Filestore%E9%9C%80%E6%8C%87%E5%AE%9Ajournal%E5%A4%A7%E5%B0%8F"><span class="nav-text">Filestore需指定journal大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E8%B7%AF%E5%BE%84"><span class="nav-text">数据存储路径</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%83%E8%B7%B3"><span class="nav-text">心跳</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E6%88%96%E8%B0%83%E8%AF%95"><span class="nav-text">日志或调试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ceph-conf-%E7%A4%BA%E4%BE%8B"><span class="nav-text">ceph.conf 示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-text">Ceph网络参数配置项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IP-Table%EF%BC%88%E7%AB%AF%E5%8F%A3%E9%98%B2%E7%81%AB%E5%A2%99%E7%9B%B8%E5%85%B3%E7%9A%84%EF%BC%89"><span class="nav-text">IP Table（端口防火墙相关的）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#monitor-%E7%9A%84IP-Table"><span class="nav-text">monitor 的IP Table</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MDS%E5%92%8CMGR%E7%9A%84IP-Table"><span class="nav-text">MDS和MGR的IP Table</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSD%E7%9A%84IP-Table"><span class="nav-text">OSD的IP Table</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE"><span class="nav-text">网络配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%AC%E5%85%B1%E7%BD%91%E7%BB%9C"><span class="nav-text">公共网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C"><span class="nav-text">集群网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph%E8%BF%9B%E7%A8%8B%E7%9A%84IP"><span class="nav-text">Ceph进程的IP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#mon%E8%BF%9B%E7%A8%8B%E7%9A%84IP"><span class="nav-text">mon进程的IP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E8%BF%9B%E7%A8%8B%E7%9A%84IP"><span class="nav-text">其他进程的IP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E7%BD%91%E5%8D%A1%E5%8F%8C%E7%BD%91%E7%BB%9C"><span class="nav-text">单网卡双网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E7%9A%84%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-text">网络相关的参数配置项</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%AC%E5%85%B1%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE"><span class="nav-text">公共网络配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE"><span class="nav-text">集群网络配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bind"><span class="nav-text">bind</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TCP"><span class="nav-text">TCP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E8%AE%BE%E7%BD%AE"><span class="nav-text">通用设置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MESSENGER-V2"><span class="nav-text">MESSENGER V2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%B0%E5%9D%80%E6%A0%BC%E5%BC%8F"><span class="nav-text">地址格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bind%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-text">bind参数配置项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%BC%8F"><span class="nav-text">连接模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%8B%E7%BC%A9%E6%A8%A1%E5%BC%8F"><span class="nav-text">压缩模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E4%BB%85-V1-%E8%BF%87%E6%B8%A1%E5%88%B0-V2-PLUS-V1"><span class="nav-text">从仅 V1 过渡到 V2-PLUS-V1</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cephx"><span class="nav-text">Cephx</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2CEPHX"><span class="nav-text">部署CEPHX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-%E7%A6%81%E7%94%A8-CEPHX"><span class="nav-text">启用&#x2F;禁用 CEPHX</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E7%94%A8CEPHX"><span class="nav-text">启用CEPHX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A6%81%E7%94%A8CEPHX"><span class="nav-text">禁用CEPHX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SIGNATURES"><span class="nav-text">SIGNATURES</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KEYS"><span class="nav-text">KEYS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B-keyrings"><span class="nav-text">守护进程 keyrings</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#monitor"><span class="nav-text">monitor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-text">背景</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cluster-map"><span class="nav-text">cluster map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7%E6%B3%95%E5%AE%9A%E4%BA%BA%E6%95%B0"><span class="nav-text">监控法定人数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-text">一致性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%95%E5%AF%BCmonitor"><span class="nav-text">引导monitor</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monitor-%E7%9A%84%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-text">Monitor 的参数配置项</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E9%85%8D%E7%BD%AE"><span class="nav-text">最小配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4ID"><span class="nav-text">集群ID</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%88%90%E5%91%98"><span class="nav-text">初始成员</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DATA"><span class="nav-text">DATA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E5%AE%B9%E9%87%8F"><span class="nav-text">存储容量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7%E5%99%A8%E5%AD%98%E5%82%A8%E5%90%8C%E6%AD%A5-MONITOR-STORE-SYNCHRONIZATION"><span class="nav-text">监控器存储同步(MONITOR_STORE_SYNCHRONIZATION)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%92%9F"><span class="nav-text">时钟</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="nav-text">客户端</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pool-%E8%AE%BE%E7%BD%AE"><span class="nav-text">Pool 设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87-DNS-%E6%9F%A5%E6%89%BE%E7%9B%91%E8%A7%86%E5%99%A8"><span class="nav-text">通过 DNS 查找监视器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Heartbeat"><span class="nav-text">Heartbeat</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#OSDS-%E6%A3%80%E6%9F%A5%E5%BF%83%E8%B7%B3"><span class="nav-text">OSDS 检查心跳</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSDS-%E6%8A%A5%E5%91%8A-OSDS-down"><span class="nav-text">OSDS 报告 OSDS down</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSDS%E6%8A%A5%E5%91%8A-peering-%E5%A4%B1%E8%B4%A5"><span class="nav-text">OSDS报告 peering 失败</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSDS%E6%8A%A5%E5%91%8A%E5%85%B6-status"><span class="nav-text">OSDS报告其 status</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OSD"><span class="nav-text">OSD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="nav-text">通用配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#monitor%E4%B8%8E-OSD-%E7%9A%84%E4%BA%A4%E4%BA%92"><span class="nav-text">monitor与 OSD 的交互</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B8%85%E6%B4%97"><span class="nav-text">清洗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EMCLOCK%E7%9A%84QOS"><span class="nav-text">基于MCLOCK的QOS</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-text">核心概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MCLOCK-%E7%9A%84%E7%B2%BE%E5%A6%99%E4%B9%8B%E5%A4%84"><span class="nav-text">MCLOCK 的精妙之处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MCLOCK-%E9%85%8D%E7%BD%AE%E5%8F%82%E8%80%83"><span class="nav-text">MCLOCK 配置参考</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MCLOCK-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%B1%BB%E5%9E%8B"><span class="nav-text">MCLOCK 客户端类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MCLOCK-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-%E5%AE%9A%E4%B9%89%E5%92%8C%E7%9B%AE%E7%9A%84"><span class="nav-text">MCLOCK 配置文件 - 定义和目的</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MCLOCK-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B"><span class="nav-text">MCLOCK 配置文件类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MCLOCK-%E5%86%85%E7%BD%AE%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-%E9%94%81%E5%AE%9A%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9"><span class="nav-text">MCLOCK 内置配置文件 - 锁定配置选项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-MCLOCK-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-text">启用 MCLOCK 配置文件的步骤</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9C%A8%E5%86%85%E7%BD%AE%E5%92%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%B9%8B%E9%97%B4%E5%88%87%E6%8D%A2"><span class="nav-text">在内置和自定义配置文件之间切换</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9C%A8-MCLOCK-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E4%B9%8B%E9%97%B4%E4%B8%B4%E6%97%B6%E5%88%87%E6%8D%A2"><span class="nav-text">在 MCLOCK 配置文件之间临时切换</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9-MCLOCK-%E6%9C%80%E5%A4%A7%E5%9B%9E%E5%A1%AB-%E6%81%A2%E5%A4%8D%E9%99%90%E5%88%B6%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-text">修改 MCLOCK 最大回填&#x2F;恢复限制的步骤</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSD-%E5%AE%B9%E9%87%8F%E7%A1%AE%E5%AE%9A%EF%BC%88%E8%87%AA%E5%8A%A8%EF%BC%89"><span class="nav-text">OSD 容量确定（自动）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E5%87%8F%E5%B0%91%E4%B8%8D%E5%88%87%E5%AE%9E%E9%99%85%E7%9A%84-OSD-%E5%AE%B9%E9%87%8F"><span class="nav-text">通过自动化测试减少不切实际的 OSD 容量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%A1%ABbackfilling"><span class="nav-text">回填backfilling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OSD-map"><span class="nav-text">OSD map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%81%A2%E5%A4%8Drecovery"><span class="nav-text">恢复recovery</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tiering%E5%88%86%E5%B1%82"><span class="nav-text">tiering分层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Filestore"><span class="nav-text">Filestore</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Journal-%E8%AE%BE%E7%BD%AE"><span class="nav-text">Journal 设置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bluestore"><span class="nav-text">Bluestore</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87"><span class="nav-text">设备</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5"><span class="nav-text">配置策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E5%AE%B9%E9%87%8F"><span class="nav-text">设备容量</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E8%B0%83%E6%95%B4%E7%BC%93%E5%AD%98%E5%A4%A7%E5%B0%8F"><span class="nav-text">自动调整缓存大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E8%B0%83%E6%95%B4%E7%BC%93%E5%AD%98%E5%A4%A7%E5%B0%8F"><span class="nav-text">手动调整缓存大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%A1%E9%AA%8C%E5%92%8C"><span class="nav-text">校验和</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E8%81%94%E5%8E%8B%E7%BC%A9"><span class="nav-text">内联压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RocksDB-%E5%88%86%E7%89%87"><span class="nav-text">RocksDB 分片</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SPDK%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">SPDK的使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E5%88%86%E9%85%8D%E8%A7%84%E6%A8%A1"><span class="nav-text">最小分配规模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DSA%EF%BC%88%E6%95%B0%E6%8D%AE%E6%B5%81%E5%8A%A0%E9%80%9F%E5%99%A8%EF%BC%89%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">DSA（数据流加速器）的使用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%94%BE%E7%BD%AE"><span class="nav-text">数据放置</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">220</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">82</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1192.1k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">48:20</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>