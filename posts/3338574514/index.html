<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="ceph quincy 版本部署"><meta property="og:url" content="https://amostian.github.io/posts/3338574514/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2024-05-05T14:15:10.000Z"><meta property="article:modified_time" content="2024-10-04T11:29:41.167Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="存储"><meta property="article:tag" content="分布式存储"><meta property="article:tag" content="Ceph"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://amostian.github.io/posts/3338574514/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>ceph quincy 版本部署 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">65</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">82</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">217</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/3338574514/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ceph quincy 版本部署</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-05-05 22:15:10" itemprop="dateCreated datePublished" datetime="2024-05-05T22:15:10+08:00">2024-05-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-10-04 19:29:41" itemprop="dateModified" datetime="2024-10-04T19:29:41+08:00">2024-10-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">存储</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%AD%98%E5%82%A8/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">分布式存储</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%AD%98%E5%82%A8/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/Ceph/" itemprop="url" rel="index"><span itemprop="name">Ceph</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">6.5k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>24 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h2 id="ceph-quincy-版本部署"><a href="#ceph-quincy-版本部署" class="headerlink" title="ceph quincy 版本部署"></a>ceph quincy 版本部署</h2><blockquote><p><a target="_blank" rel="noopener" href="https://quay.io/repository/ceph/ceph?tab=tags">ceph 容器镜像</a></p><p><a target="_blank" rel="noopener" href="https://quay.io/repository/ceph/ceph-grafana?tab=info">https://quay.io/repository/ceph/ceph-grafana?tab=info</a></p></blockquote><h3 id="实验环境说明"><a href="#实验环境说明" class="headerlink" title="实验环境说明"></a>实验环境说明</h3><ol><li>集群使用5台主机，每台主机配置为4C8G，共3块硬盘。1块200G SSD用作系统盘，1块100GB SSD盘用作OSD，1块100GB SATA盘用作OSD</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">free -m</span></span><br><span class="line">               total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            7403         696        5089           9        1923        6706</span><br><span class="line">Swap:           8191           0        8191</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> /proc/cpuinfo |grep processor</span></span><br><span class="line">processor       : 0</span><br><span class="line">processor       : 1</span><br><span class="line">processor       : 2</span><br><span class="line">processor       : 3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">lsblk</span></span><br><span class="line">NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS</span><br><span class="line">sda             8:0    0  100G  0 disk </span><br><span class="line">sr0            11:0    1 16.5G  0 rom  </span><br><span class="line">nvme0n1       259:0    0  200G  0 disk </span><br><span class="line">├─nvme0n1p1   259:1    0    2G  0 part /boot</span><br><span class="line">└─nvme0n1p2   259:2    0  198G  0 part </span><br><span class="line">  ├─vg00-root 253:0    0  120G  0 lvm  /</span><br><span class="line">  └─vg00-swap 253:1    0    8G  0 lvm  [SWAP]</span><br><span class="line">nvme0n2       259:3    0  100G  0 disk </span><br></pre></td></tr></table></figure><ol><li>OS 使用<code>openEuler 22.03 (LTS-SP1)</code></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> /etc/os-release</span> </span><br><span class="line">NAME=&quot;openEuler&quot;</span><br><span class="line">VERSION=&quot;22.03 (LTS-SP1)&quot;</span><br><span class="line">ID=&quot;openEuler&quot;</span><br><span class="line">VERSION_ID=&quot;22.03&quot;</span><br><span class="line">PRETTY_NAME=&quot;openEuler 22.03 (LTS-SP1)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br></pre></td></tr></table></figure><ol><li>OS 内核<code>5.10.0-136.34.0.110.oe2203sp1.x86_64</code></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">uname</span> -r</span></span><br><span class="line">5.10.0-136.34.0.110.oe2203sp1.x86_64</span><br></pre></td></tr></table></figure><blockquote><p>说明：这里选择openeuler也是有理由的，现在都强调自主可控哈，openeuler也会发展的越来越好。</p></blockquote><ol><li>Ceph版本使用<code>Quincy 17.2.6</code></li></ol><blockquote><p>说明：openeuler yum源中默认版本为<code>Pacific 16.2.x</code>，ceph在维护的版本<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/releases/#active-releases">ceph活跃版本</a></p></blockquote><ol><li>docker与containerd版本分别为<code>docker-20.10.23</code>和<code>containerd-1.7.0</code></li></ol><blockquote><p>说明：openeuler 22.03自带的docker版本是<code>18.09</code>。</p></blockquote><h2 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h2><ol><li>修改主机名和<code>hosts</code></li></ol><blockquote><p>修改所有节点(ceph01-05)的主机名</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 例如，在192.168.59.241上执行命令</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hostnamectl set-hostname ceph01</span></span><br></pre></td></tr></table></figure><blockquote><p>修改有节点(ceph01-05)<code>/etc/hosts</code>文件</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/hosts &lt;&lt;EOF</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">public network</span></span><br><span class="line">192.168.59.241 ceph01</span><br><span class="line">192.168.59.242 ceph02</span><br><span class="line">192.168.59.243 ceph03</span><br><span class="line">192.168.59.244 ceph04</span><br><span class="line">192.168.59.245 ceph05</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cluster network</span></span><br><span class="line">10.168.59.241 ceph01-cl</span><br><span class="line">10.168.59.242 ceph02-cl</span><br><span class="line">10.168.59.243 ceph03-cl</span><br><span class="line">10.168.59.244 ceph04-cl</span><br><span class="line">10.168.59.245 ceph05-cl</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">manage network</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ol><li>统一网卡命名(选做)</li></ol><blockquote><p>有部分生产环境机型不同，网卡名有差异，统一命名便于后续管理。</p><p>注意：在线修改网卡名，可能会导致网络中断，建议从BMC操作，请谨慎操作！！</p><p>在openeuler 22.03使用在centos7.9上udev绑定网卡名的方式已经不生效，rocky linux 9.2 也不生效，应该是绑定udev方式发生了变化，此处通过修改grub的方式统一网卡名。</p><ul><li>public 网络：eth0</li></ul></blockquote><ul><li>cluster 网络：eth1</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在虚拟机中，public 网络的网卡名默认是 ens160（操作系统及版本不同，网卡名可能不同），现在统一命名为 eth0</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">mv</span> /etc/sysconfig/network-scripts/ifcfg-ens160 /etc/sysconfig/network-scripts/ifcfg-eth0</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=no</span><br><span class="line">IPV6_AUTOCONF=no</span><br><span class="line">IPV6_DEFROUTE=no</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">modify, default ens160</span></span> </span><br><span class="line">NAME=eth0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">modify,default ens160</span></span></span><br><span class="line">DEVICE=eth0</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.59.241</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.168.59.2</span><br><span class="line">DNS1=223.5.5.5</span><br><span class="line">DNS2=114.114.114.114</span><br><span class="line">EOF</span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 在虚拟机中，cluster 网络的网卡名默认是 ens192（操作系统及版本不同，网卡名可能不同），现在统一命名为 eth1</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 从 public 网络登录修改 cluster 网络！！！</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">mv /etc/sysconfig/network-scripts/ifcfg-ens192 /etc/sysconfig/network-scripts/ifcfg-eth1</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">cat &gt; /etc/sysconfig/network-scripts/ifcfg-eth1 &lt;&lt;EOF</span></span></span><br><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=no</span><br><span class="line">IPV6_AUTOCONF=no</span><br><span class="line">IPV6_DEFROUTE=no</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">modify,default ens192</span></span><br><span class="line">NAME=eth1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">modify,default ens192</span></span><br><span class="line">DEVICE=eth1</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=10.168.59.241</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 上面用`cat`命令修改网卡，也可以直接用`sed`命令替换网卡中的名字</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sed -i <span class="string">&#x27;s/ens160/eth0/g&#x27;</span> /etc/sysconfig/network-scripts/ifcfg-eth0</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sed -i <span class="string">&#x27;s/ens192/eth1/g&#x27;</span> /etc/sysconfig/network-scripts/ifcfg-eth1</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 修改GRUB，同时关闭net.ifnames和biosdevname命名规则。</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 生产环境操作前，建议备份数据</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在GRUB_CMDLINE_LINUX中增加`net.ifnames=0 biosdevname=0`</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vi /etc/default/grub</span></span><br><span class="line">GRUB_TIMEOUT=5</span><br><span class="line">GRUB_DISTRIBUTOR=&quot;$(sed &#x27;s, release .*$,,g&#x27; /etc/system-release)&quot;</span><br><span class="line">GRUB_DEFAULT=saved</span><br><span class="line">GRUB_DISABLE_SUBMENU=true</span><br><span class="line">GRUB_TERMINAL_OUTPUT=&quot;console&quot;</span><br><span class="line">GRUB_CMDLINE_LINUX=&quot;resume=/dev/mapper/vg00-swap rd.lvm.lv=vg00/root rd.lvm.lv=vg00/swap net.ifnames=0 biosdevname=0 cgroup_disable=files apparmor=0 crashkernel=512M rhgb quiet&quot;</span><br><span class="line">GRUB_DISABLE_RECOVERY=&quot;true</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 更新grub并重启生效</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">grub2-mkconfig -o /boot/grub2/grub.cfg</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">reboot</span></span><br><span class="line"></span><br><span class="line">如果系统是UEFI启动，则要执行以下命令</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">reboot</span></span><br></pre></td></tr></table></figure><blockquote><p>上述修改网卡名称，需在ceph01-05上操作，注意配置文件中IP需按实际修改</p></blockquote><ul><li>存储网络配置巨帧(实验环境选做)</li></ul><blockquote><p>在存储网络的网卡和交换机上使用巨型帧，配置巨型帧最大传输单元 (MTU) 9000，特别是在后端或集群网络上。<br>Ceph 中大多数与性能相关的问题通常是因为网络问题造成的。简单的网络问题（Cat-6 电缆）可能会导致带宽下降。至少<br>将10Gb网络用于public网络。对于大型集群，请考虑将40Gb网络用于集群网络。</p></blockquote><ul><li>设置服务器BIOS注意事项(实验环境选做)</li></ul><blockquote><p>当全部使用 SSD（Solid State Drives），或每个节点都配置了大量驱动器时，支持在 Ceph 中以独立驱动器模式使用 Just a Bunch Drives (JBOD)。例如，有 20个驱动器附加到一个控制器。在这种情况下，<br>回写缓存可能会形成一个 I/O 资源竞争。由于<code>JBOD</code>禁用回写缓存，因此在这种情况下是理想的选择。<br>使用<code>JBOD</code>模式的一个优点是易于添加或替换驱动器，然后在物理插入后立即向操作系统公开驱动器。</p></blockquote><ol><li>关闭防火墙和selinux</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br></pre></td></tr></table></figure><ol><li>关闭交换分区(选做)，生产环境建议关闭交换分区</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line">sed -i &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab</span><br></pre></td></tr></table></figure><ol><li>修改内核参数及资源限制参数</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 转发 IPv4 并让 iptables 看到桥接流量(选做)</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF | sudo tee /etc/modules-load.d/ceph.conf</span></span></span><br><span class="line">overlay</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">modprobe overlay</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">modprobe br_netfilter</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">lsmod | grep br_netfilter</span></span> </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">验证br_netfilter模块</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 修改内核参数</span></span></span><br><span class="line">cat &lt;&lt;EOF | tee /etc/sysctl.d/ceph.conf</span><br><span class="line">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.ip_forward                 = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">1. 用于对外连接的随机端口范围。缺省是# 32768    60999</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">端口范围开始和结束要奇偶不同，如果设置为1024 65530则在dmesg中会报ip_local_port_range: prefer different parity for start/end values.</span></span></span><br><span class="line">net.ipv4.ip_local_port_range = 1024 65335 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">如果dmesg中有类似“nf_conntrack: table full, dropping packet”日志，则需要调大 conntrack 参数，默认是2621440，该值不能太大，否则会出现：nf_conntrack: falling back to vmalloc.</span></span></span><br><span class="line">net.netfilter.nf_conntrack_max = 2621440</span><br><span class="line">net.nf_conntrack_max = 2621440</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">指定了进程可以拥有的内存映射区域的最大数目。这个设置对于使用大量内存映射的应用程序很重要</span></span> </span><br><span class="line">vm.max_map_count = 1048576                       </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">2. 如果 netstat -s | grep &quot;buffer errors&quot; 中errors数在增加，则需要调整如下参数</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.ipv4.tcp_wmem 默认值：4096        16384   4194304</span></span></span><br><span class="line">net.ipv4.tcp_wmem = 4096        16384   4194304</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string"> net.ipv4.tcp_rmem 默认值：4096  87380  6291456</span></span></span><br><span class="line">net.ipv4.tcp_rmem = 4096  87380  6291456</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.ipv4.tcp_mem 默认值：381462  508616  762924</span></span></span><br><span class="line">net.ipv4.tcp_mem = 381462  508616  762924</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.core.rmem_default 默认值：212992</span></span></span><br><span class="line">net.core.rmem_default = 8388608</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.core.rmem_max 默认值：212992</span></span></span><br><span class="line">net.core.rmem_max = 26214400</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.core.wmem_max 默认值：212992</span></span></span><br><span class="line">net.core.wmem_max = 26214400</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">调大文件句柄数</span></span></span><br><span class="line">fs.nr_open = 16777216</span><br><span class="line">fs.file-max = 16777216</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">3.如果dmesg中有类似&quot;arp_cache: neighbor table overflow&quot;，则需要调整如下参数</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.ipv4.neigh.default.gc_thresh1 默认值 128</span></span></span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 40960</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.ipv4.neigh.default.gc_thresh2 默认值 512</span></span></span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 81920</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.ipv4.neigh.default.gc_thresh3 默认值 1024</span></span></span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 102400</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">4. 连接队列满导致丢包，需要调整半连接队列和全连接队列</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">TCP 连接请求队列长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。</span></span></span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 65535 </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">调整全连接队列上限，即服务器同时接受连接的数量</span></span></span><br><span class="line">net.core.somaxconn = 65535</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">网络设备最大接收队列长度</span></span></span><br><span class="line">net.core.netdev_max_backlog = 250000</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">5. 在低版本内核中(比如 3.10)，支持使用 tcp_tw_recycle 内核参数来开启 TIME_WAIT 的快速回收，但如果 client 也开启了 timestamp (一般默认开启)，同时也就会导致在 NAT 环境丢包，甚至没有 NAT 时，稍微高并发一点，也会导致PAWS校验失败，导致丢包，所以生产环境不建议开启。</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">### TIME_WAIT</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">默认0</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">用 SYN Cookie 防御机制</span></span></span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">开启 TIME-WAIT 状态的重用，此处为0，未开启</span></span></span><br><span class="line">net.ipv4.tcp_tw_reuse = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">不建议启用tcp_tw_recycle，会导致数据错乱，4.12内核已去掉这个参数</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">net.ipv4.tcp_tw_recycle = 0</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">默认60</span></span></span><br><span class="line">net.ipv4.tcp_fin_timeout = 30</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string">6.启用fastopen，跳过tcp3次握手;第 1 个比特位为 1 时，表示作为客户端时支持 TFO；第 2 个比特位为 1 时，表示作为服务器时支持 TFO，所以当 tcp_fastopen 的值为 3 时（比特为 0x11）就表示完全支持 TFO 功能。</span></span></span><br><span class="line">net.ipv4.tcp_fastopen = 3</span><br><span class="line">net.ipv4.tcp_orphan_retries = 3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">默认0，表示如果三次握手第三步的时候 accept queue 满了，则 server 丢弃 client 发过来的 ack；为1表示第三步的时候如果全连接队列满了，server 发送一个 rst 包给 client ，表示拒绝这个握手过程和这个连接</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">只有确信守护进程真的不能完成连接请求时才打开该选项，该选项会影响客户的使用</span></span></span><br><span class="line">net.ipv4.tcp_abort_on_overflow = 1</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">sysctl -p  /etc/sysctl.d/ceph.conf</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 修改资源限制参数</span></span></span><br><span class="line">cat &gt; /etc/security/limits.d/ceph.conf &lt;&lt;EOF</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">End of file</span></span></span><br><span class="line">*               hard    nofile         655360</span><br><span class="line">*               soft    nofile         655360</span><br><span class="line">*               soft    core           655360</span><br><span class="line">*               hard    core           655360</span><br><span class="line">*          soft    nproc     unlimited</span><br><span class="line">root       soft    nproc     unlimited</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ol><li>配置时间同步服务，以ceph01作为时间同步服务器</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在ceph01上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">yum -y install chrony</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vi /etc/chrony.conf</span> </span><br><span class="line">pool ntp.aliyun.com iburst</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">allow 192.168.59.0/24</span><br><span class="line">allow 10.168.59.0/24</span><br><span class="line">local stratum 10</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl start chronyd &amp;&amp; systemctl <span class="built_in">enable</span> chronyd</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在ceph02-ceph05上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">yum -y install chrony</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vi /etc/chrony.conf</span> </span><br><span class="line">pool ceph01 iburst</span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl restart chronyd &amp;&amp; systemctl <span class="built_in">enable</span> chronyd;chronyc sources</span></span><br><span class="line">MS Name/IP address         Stratum Poll Reach LastRx Last sample               </span><br><span class="line">===============================================================================</span><br><span class="line">^* ceph01                        3   6    17     8    -34us[  -28us] +/-   35ms</span><br></pre></td></tr></table></figure><ol><li>配置ssh免密登录</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在ceph01上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-keygen -t rsa</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id ceph01</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id ceph02</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id ceph03</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id ceph04</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id ceph05</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 验证ssh免密，从ceph01登录ceph01-05看是否需要密码</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh ceph01</span></span><br></pre></td></tr></table></figure><ol><li>安装containerd和docker</li></ol><blockquote><p><a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/releases">下载页</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 先在ceph01上操作</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 直接下载：cri-containerd-1.7.2-linux-amd64.tar.gz，该包里面包含了containerd、 ctr、crictl、containerd-shim等二进制文件，还有启动命令等，只要在/下解压即可。</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">tar xf cri-containerd-1.7.2-linux-amd64.tar.gz -C /</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &gt; /etc/crictl.yaml &lt;&lt; <span class="string">EOF</span></span></span><br><span class="line">runtime-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">image-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">timeout: 10</span><br><span class="line">debug: false</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 生成containerd配置文件</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">mkdir /etc/containerd</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">containerd config default &gt; /etc/containerd/config.toml</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 修改 /etc/containerd/config.toml</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 把SystemdCgroup = false修改为：SystemdCgroup = true</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">vi /etc/containerd/config.toml</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 调整日志</span></span></span><br><span class="line">max_container_log_line_size = 163840</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 用于从安全上下文中提取设备所有权信息，kubevirt cdi依赖该参数，不设置该参数可能出现无权限</span></span></span><br><span class="line">device_ownership_from_security_context = true</span><br><span class="line">SystemdCgroup = true</span><br></pre></td></tr></table></figure><blockquote><p>修改/etc/systemd/system/containerd.service，将<code>LimitNOFILE=infinity</code>改为<code>LimitNOFILE=655360</code></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl <span class="built_in">enable</span> containerd &amp;&amp; systemctl restart containerd</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 使用 crictl info 查看配置是否生效</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 其他节点只需将相关文件拷贝过去启动即可</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在ceph01上操作</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..5&#125;;<span class="keyword">do</span> scp -rp /usr/local/bin ceph0<span class="variable">$i</span>:/usr/local/;scp -rp /usr/local/sbin ceph0<span class="variable">$i</span>:/usr/local/;scp /etc/containerd ceph0<span class="variable">$i</span>:/etc/; scp /etc/systemd/system/containerd.service ceph0<span class="variable">$i</span>:/etc/systemd/system/;<span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line">for i in &#123;1..3&#125;;do scp -rp /usr/local/bin node0$i:/usr/local/;scp -rp /usr/local/sbin node0$i:/usr/local/;scp /etc/containerd node0$i:/etc/; scp /etc/systemd/system/containerd.service node0$i:/etc/systemd/system/;done</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在ceph02-05上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..5&#125;;<span class="keyword">do</span> cmd=$(systemctl daemon-reload &amp;&amp; systemctl <span class="built_in">enable</span> containerd &amp;&amp; systemctl restart containerd);ssh ceph0<span class="variable">$i</span> <span class="string">&quot;<span class="variable">$cmd</span>&quot;</span>;<span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line">for i in &#123;1..3&#125;;do cmd=$(systemctl daemon-reload &amp;&amp; systemctl enable containerd &amp;&amp; systemctl restart containerd);ssh node0$i &quot;$cmd&quot;;done</span><br></pre></td></tr></table></figure><ol><li>安装docker</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 先在ceph01上操作</span></span></span><br><span class="line">1). 下载docker安装包 https://download.docker.com/linux/static/stable/x86_64/docker-20.10.23.tgz</span><br><span class="line">2). 解压 </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">tar xf docker-20.10.23.tgz</span></span><br><span class="line">3). 需要用到的二进制文件包括：docker 和 dockerd，拷贝到 /usr/bin/目录下即可。</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">mv</span> docker/docker* /usr/bin/</span></span><br><span class="line">4). 创建 docker 用户 useradd -s /sbin/nologin docker   # 如果docker组存在，则使用 useradd -s /sbin/nologin docker -g docker</span><br><span class="line">5). 启动的配置文件 docker.serivce 和 docker.socket 拷贝到 /usr/lib/systemd/system/，daemon.json文件放到/etc/docker目录</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &gt; /usr/lib/systemd/system/docker.service  &lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">After=network-online.target docker.socket firewalld.service containerd.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Requires=docker.socket</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">the default is not to use systemd for cgroups because the delegate issues still</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">exists and systemd currently does not support the cgroup feature set required</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">for containers run by docker</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">ExecStart=/usr/bin/dockerd --graph=/data/docker -H fd:// --containerd=/run/containerd/containerd.sock --cri-containerd --debug</span></span></span><br><span class="line">ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --cri-containerd --debug</span><br><span class="line">ExecReload=/bin/kill -s HUP \$MAINPID</span><br><span class="line">TimeoutSec=0</span><br><span class="line">RestartSec=2</span><br><span class="line">Restart=always</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229.</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Both the old, and new location are accepted by systemd 229 and up, so using the old location</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">to make them work for either version of systemd.</span></span></span><br><span class="line">StartLimitBurst=3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Both the old, and new name are accepted by systemd 230 and up, so using the old name to make</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">this option work for either version of systemd.</span></span></span><br><span class="line">StartLimitInterval=60s</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Having non-zero Limit*s causes performance problems due to accounting overhead</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">in the kernel. We recommend using cgroups to do container-local accounting.</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">可能会出现错误：&quot;Failed at step LIMITS spawning /usr/bin/dockerd: Operation not permitted&quot;，则需要将LimitNOFILE=infinity 改成：LimitNOFILE=65530</span></span></span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Comment TasksMax if your systemd version does not support it.</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">Only systemd 226 and above support this option.</span></span></span><br><span class="line">TasksMax=infinity</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">set delegate yes so that systemd does not reset the cgroups of docker containers</span></span></span><br><span class="line">Delegate=yes</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">kill only the docker process, not all processes in the cgroup</span></span></span><br><span class="line">KillMode=process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">cat &gt; /usr/lib/systemd/system/docker.socket &lt;&lt;EOF</span></span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Socket for the API</span><br><span class="line">PartOf=docker.service</span><br><span class="line"></span><br><span class="line">[Socket]</span><br><span class="line">ListenStream=/var/run/docker.sock</span><br><span class="line">SocketMode=0660</span><br><span class="line">SocketUser=root</span><br><span class="line">SocketGroup=docker</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=sockets.target</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 启动docker</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl daemon-reload &amp;&amp; systemctl <span class="built_in">enable</span> docker --now</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 配置镜像加速和私有仓库</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &gt; /etc/docker/daemon.json &lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://vty0b0ux.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;500m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;registry.demo.com&quot;,&quot;192.168.59.249:5000&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">systemctl restart docker</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 在ceph01上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">for i in &#123;2..5&#125;;do scp /usr/bin/docker* ceph0$i:/usr/bin/;scp -rp /etc/docker ceph0$i:/etc/;scp /usr/lib/systemd/system/docker.service ceph0$i:/usr/lib/systemd/system/;scp /usr/lib/systemd/system/docker.socket ceph0$i:/usr/lib/systemd/system/;done</span></span></span><br><span class="line"></span><br><span class="line">for i in &#123;2..3&#125;;do scp /usr/bin/docker* node0$i:/usr/bin/;scp -rp /etc/docker node0$i:/etc/;scp /usr/lib/systemd/system/docker.service node0$i:/usr/lib/systemd/system/;scp /usr/lib/systemd/system/docker.socket node0$i:/usr/lib/systemd/system/;done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="string"># 在ceph02-05上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">useradd -s /sbin/nologin docker</span></span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">systemctl daemon-reload &amp;&amp; systemctl enable docker --now</span></span></span><br></pre></td></tr></table></figure><ol><li>安装cephadm，O版开始就不再支持ceph-deploy工具</li></ol><blockquote><p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/cephadm/install/#install-cephadm">cephadm安装</a><br>cephadm安装前提</p></blockquote><ul><li>Python3</li><li>Systemd</li><li>Podman or Docker</li><li>Chrony or NTP</li><li>LVM2</li></ul><blockquote><p>cephadm工作原理：cephadm命令管理ceph集群完整生命周期。包括创建引导集群，启动一个提供shell的容器来管理集群。cephadm使用ssh与集群各节点通信。<br>cephadm bootstrap 就是在单一节点上创建一个小型的ceph集群，包括一个ceph monitor和一个ceph mgr，监控组件包括prometheus、node-exporter等。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在ceph所有点上执行</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">CEPH_RELEASE=17.2.6 <span class="comment"># replace this with the active release</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">curl --remote-name --location https://download.ceph.com/rpm-<span class="variable">$&#123;CEPH_RELEASE&#125;</span>/el8/noarch/cephadm</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">chmod</span> +x cephadm</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">mv</span> cephadm /usr/sbin/</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 执行cephadm install 会在当前节点上安装cephadm依赖相关的软件包，版本较低，所以不建议执行</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cephadm install</span></span><br><span class="line">ERROR: Distro openeuler version 22.03 not supported</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 修改 /usr/sbin/cephadm ，在</span></span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vi /usr/sbin/cephadm</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># DISTRO_NAMES 这个字典中增加 openeuler</span></span></span><br><span class="line">   7654     DISTRO_NAMES = &#123;</span><br><span class="line">   7655         &#x27;centos&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7656         &#x27;rhel&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7657         &#x27;scientific&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7658         &#x27;rocky&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7659         &#x27;openeuler&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7660         &#x27;almalinux&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7661         &#x27;ol&#x27;: (&#x27;centos&#x27;, &#x27;el&#x27;),</span><br><span class="line">   7662         &#x27;fedora&#x27;: (&#x27;fedora&#x27;, &#x27;fc&#x27;),</span><br><span class="line">   7663         &#x27;mariner&#x27;: (&#x27;mariner&#x27;, &#x27;cm&#x27;),</span><br><span class="line">   7664     &#125;</span><br><span class="line">   7665  </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 将 cephadm 文件拷贝到其他节点上</span></span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..5&#125;;<span class="keyword">do</span> scp -rp /usr/sbin/cephadm ceph0<span class="variable">$i</span>:/usr/sbin/;<span class="keyword">done</span></span></span><br></pre></td></tr></table></figure><blockquote><p>如果是Centos8，可直接yum源安装</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">yum download cephadm</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">rpm -ivh cephadm-17.2.6-0.el8.noarch.rpm</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">rpm -ql cephadm-17.2.6-0.el8</span></span><br><span class="line">/usr/sbin/cephadm</span><br><span class="line">/usr/share/man/man8/cephadm.8.gz</span><br><span class="line">/var/lib/cephadm</span><br><span class="line">/var/lib/cephadm/.ssh</span><br><span class="line">/var/lib/cephadm/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><blockquote><p>如果是CentOS 7.9，先安装python3</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">yum -y install python3</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 下载 cephadm，与CentOS8环境相同，修改python运行环境，注释原有的。</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /usr/bin/python3 -s</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="meta">#! /usr/libexec/platform-python -s</span></span></span><br></pre></td></tr></table></figure><ol><li>检查ceph各节点是否满足安装ceph集群，该命令需要在当前节点执行，比如要判断ceph02是否支持安装ceph集群，则在ceph02上执行</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cephadm check-host --expect-hostname ceph02</span></span><br><span class="line">docker (/usr/bin/docker) is present</span><br><span class="line">systemctl is present</span><br><span class="line">lvcreate is present</span><br><span class="line">Unit chronyd.service is enabled and running</span><br><span class="line">Hostname &quot;ceph02&quot; matches what is expected.</span><br><span class="line">Host looks OK</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 也可以使用以下命令检查</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cephadm check-host --expect-hostname `hostname`</span></span><br></pre></td></tr></table></figure><ol><li>初始化mon</li></ol><blockquote><p>cephadm bootstrap 过程是在单一节点上创建一个小型的ceph集群，包括一个ceph monitor和一个ceph mgr，监控组件包括prometheus、node-exporter等。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">cephadm bootstrap --mon-ip 192.168.59.241 --cluster-network 10.168.59.0/24 --initial-dashboard-user mxy --initial-dashboard-password ceph123</span><br><span class="line">cephadm bootstrap --mon-ip 192.168.5.210 --allow-overwrite --skip-dashboard</span><br><span class="line"></span><br><span class="line">cephadm bootstrap --mon-ip 192.168.5.210 --cluster-network 10.168.5.0/24 --initial-dashboard-user admin --initial-dashboard-password admin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 初始化时，指定了mon-ip、集群网段、dashboard初始用户名和密码</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cephadm --image registry.demo.com/ceph/ceph:v17 \</span></span><br><span class="line"><span class="language-bash">bootstrap --mon-ip 192.168.59.241 \</span></span><br><span class="line"><span class="language-bash">--cluster-network 10.168.59.0/24 \</span></span><br><span class="line"><span class="language-bash">--initial-dashboard-user admin \</span></span><br><span class="line"><span class="language-bash">--initial-dashboard-password demo2023 \</span></span><br><span class="line"><span class="language-bash">--ssh-private-key /root/.ssh/id_rsa \</span></span><br><span class="line"><span class="language-bash">--ssh-public-key /root/.ssh/id_rsa.pub \</span></span><br><span class="line"><span class="language-bash">--registry-url registry.demo.com \</span></span><br><span class="line"><span class="language-bash">--registry-username admin \</span></span><br><span class="line"><span class="language-bash">--registry-password Harbor12345</span> </span><br><span class="line"></span><br><span class="line">Creating directory /etc/ceph for ceph.conf</span><br><span class="line">Verifying podman|docker is present...</span><br><span class="line">Verifying lvm2 is present...</span><br><span class="line">Verifying time synchronization is in place...</span><br><span class="line">Unit chronyd.service is enabled and running</span><br><span class="line">Repeating the final host check...</span><br><span class="line">docker (/usr/bin/docker) is present</span><br><span class="line">systemctl is present</span><br><span class="line">lvcreate is present</span><br><span class="line">Unit chronyd.service is enabled and running</span><br><span class="line">Host looks OK</span><br><span class="line">Cluster fsid: 2e1228b0-0781-11ee-aa8a-000c2921faf1</span><br><span class="line">Verifying IP 192.168.59.241 port 3300 ...</span><br><span class="line">Verifying IP 192.168.59.241 port 6789 ...</span><br><span class="line">Mon IP `192.168.59.241` is in CIDR network `192.168.59.0/24`</span><br><span class="line">Mon IP `192.168.59.241` is in CIDR network `192.168.59.0/24`</span><br><span class="line">Pulling container image quay.io/ceph/ceph:v17...</span><br><span class="line">Ceph version: ceph version 17.2.6 (d7ff0d10654d2280e08f1ab989c7cdf3064446a5) quincy (stable)</span><br><span class="line">Extracting ceph user uid/gid from container image...</span><br><span class="line">Creating initial keys...</span><br><span class="line">Creating initial monmap...</span><br><span class="line">Creating mon...</span><br><span class="line">Waiting for mon to start...</span><br><span class="line">Waiting for mon...</span><br><span class="line">mon is available</span><br><span class="line">Assimilating anything we can from ceph.conf...</span><br><span class="line">Generating new minimal ceph.conf...</span><br><span class="line">Restarting the monitor...</span><br><span class="line">Setting mon public_network to 192.168.59.0/24</span><br><span class="line">Setting cluster_network to 10.168.59.0/24</span><br><span class="line">Wrote config to /etc/ceph/ceph.conf</span><br><span class="line">Wrote keyring to /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">Creating mgr...</span><br><span class="line">Verifying port 9283 ...</span><br><span class="line">Waiting for mgr to start...</span><br><span class="line">Waiting for mgr...</span><br><span class="line">mgr not available, waiting (1/15)...</span><br><span class="line">mgr not available, waiting (2/15)...</span><br><span class="line">mgr not available, waiting (3/15)...</span><br><span class="line">mgr not available, waiting (4/15)...</span><br><span class="line">mgr is available</span><br><span class="line">Enabling cephadm module...</span><br><span class="line">Waiting for the mgr to restart...</span><br><span class="line">Waiting for mgr epoch 5...</span><br><span class="line">mgr epoch 5 is available</span><br><span class="line">Setting orchestrator backend to cephadm...</span><br><span class="line">Generating ssh key...</span><br><span class="line">Wrote public SSH key to /etc/ceph/ceph.pub</span><br><span class="line">Adding key to root@localhost authorized_keys...</span><br><span class="line">Adding host ceph01...</span><br><span class="line">Deploying mon service with default placement...</span><br><span class="line">Deploying mgr service with default placement...</span><br><span class="line">Deploying crash service with default placement...</span><br><span class="line">Deploying prometheus service with default placement...</span><br><span class="line">Deploying grafana service with default placement...</span><br><span class="line">Deploying node-exporter service with default placement...</span><br><span class="line">Deploying alertmanager service with default placement...</span><br><span class="line">Enabling the dashboard module...</span><br><span class="line">Waiting for the mgr to restart...</span><br><span class="line">Waiting for mgr epoch 9...</span><br><span class="line">mgr epoch 9 is available</span><br><span class="line">Generating a dashboard self-signed certificate...</span><br><span class="line">Creating initial admin user...</span><br><span class="line">Fetching dashboard port number...</span><br><span class="line">Ceph Dashboard is now available at:</span><br><span class="line"></span><br><span class="line">             URL: https://ceph01:8443/</span><br><span class="line">            User: admin</span><br><span class="line">        Password: p5tuqo17we</span><br><span class="line"></span><br><span class="line">Enabling client.admin keyring and conf on hosts with &quot;admin&quot; label</span><br><span class="line">Saving cluster configuration to /var/lib/ceph/2e1228b0-0781-11ee-aa8a-000c2921faf1/config directory</span><br><span class="line">Enabling autotune for osd_memory_target</span><br><span class="line">You can access the Ceph CLI as following in case of multi-cluster or non-default config:</span><br><span class="line"></span><br><span class="line">        sudo /usr/sbin/cephadm shell --fsid 2e1228b0-0781-11ee-aa8a-000c2921faf1 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line">Or, if you are only running a single cluster on this host:</span><br><span class="line"></span><br><span class="line">        sudo /usr/sbin/cephadm shell </span><br><span class="line"></span><br><span class="line">Please consider enabling telemetry to help improve Ceph:</span><br><span class="line"></span><br><span class="line">        ceph telemetry on</span><br><span class="line"></span><br><span class="line">For more information see:</span><br><span class="line"></span><br><span class="line">        https://docs.ceph.com/docs/master/mgr/telemetry/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">ls</span> /etc/ceph/</span></span><br><span class="line">ceph.client.admin.keyring  ceph.conf  ceph.pub  rbdmap</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 指定dashboard用户名和密码</span></span>  </span><br><span class="line">--initial-dashboard-user  admin </span><br><span class="line">--initial-dashboard-password demo2023</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#  指定私钥和公钥</span></span></span><br><span class="line">--ssh-private-key /root/.ssh/id_rsa</span><br><span class="line">--ssh-public-key /root/.ssh/id_rsa.pub</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 启动前不拉取默认镜像</span></span></span><br><span class="line">--skip-pull</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 指定私有镜像仓库</span></span></span><br><span class="line">--registry-url registry.demo.com \</span><br><span class="line">--registry-username admin \</span><br><span class="line">--registry-password Harbor12345 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 需要指定监控组件镜像</span></span></span><br><span class="line">ceph config set mgr mgr/cephadm/container_image_prometheus registry.demo.com/prometheus/prometheus:v2.33.4</span><br><span class="line">ceph config set mgr mgr/cephadm/container_image_grafana registry.demo.com/ceph/ceph-grafana:8.3.5</span><br><span class="line">ceph config set mgr mgr/cephadm/container_image_alertmanager registry.demo.com/prometheus/alertmanager:v0.23.0</span><br><span class="line">ceph config set mgr mgr/cephadm/container_image_node_exporter registry.demo.com/prometheus/node-exporter:v1.3.1</span><br></pre></td></tr></table></figure><ul><li>ceph.client.admin.keyring 是具有ceph管理员的秘钥</li><li>ceph.conf 是最小化配置文件</li><li>ceph.pub 是一个公钥，拷贝到其他节点后，可以免密登录。</li></ul><blockquote><p>在5个以上ceph节点时，默认会将其中5个节点当做mon，这可以从<code>ceph orch ls</code>中看出来</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch <span class="built_in">ls</span></span></span><br><span class="line">NAME           PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  </span><br><span class="line">alertmanager   ?:9093,9094      1/1  7m ago     46m  count:1    </span><br><span class="line">crash                           1/1  7m ago     46m  *          </span><br><span class="line">grafana        ?:3000           1/1  7m ago     46m  count:1    </span><br><span class="line">mgr                             1/2  7m ago     46m  count:2    </span><br><span class="line">mon                             1/5  7m ago     46m  count:5    </span><br><span class="line">node-exporter  ?:9100           1/1  7m ago     46m  *          </span><br><span class="line">prometheus     ?:9095           1/1  7m ago     46m  count:1   </span><br></pre></td></tr></table></figure><blockquote><p>初始化mon后，此时集群还处于WARN状态，没有OSD，MON的数量也才只有1个，MGR也只有1个，所以接下来就是先添加ceph节点。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sudo /usr/sbin/cephadm shell --fsid 081e7500-01f4-11ef-b254-b32273ab02d2 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line">Or, if you are only running a single cluster on this host:</span><br><span class="line"></span><br><span class="line">	sudo /usr/sbin/cephadm shell </span><br><span class="line"></span><br><span class="line">Please consider enabling telemetry to help improve Ceph:</span><br><span class="line"></span><br><span class="line">	ceph telemetry on</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    id:     67ccccf2-07f6-11ee-a1c2-000c2921faf1</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph01 (age 9m)</span><br><span class="line">    mgr: ceph01.sdqukl(active, since 7m)</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     </span><br></pre></td></tr></table></figure><ol><li>添加ceph节点</li></ol><blockquote><p>将ceph镜像导出到其他ceph节点上</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker images  |grep -v <span class="string">&quot;REPOSITORY&quot;</span> |awk <span class="string">&#x27;&#123;print $1&quot;:&quot;$2&#125;&#x27;</span> |xargs docker save -o ceph-images.tar</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..5&#125;;<span class="keyword">do</span> scp ceph-images.tar ceph0<span class="variable">$i</span>:/root/;<span class="keyword">done</span></span></span><br><span class="line">for i in &#123;2..3&#125;;do scp ceph-images.tar node0$i:/root/;done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 在ceph02-05上导入镜像</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker load -i ceph-images.tar</span></span><br></pre></td></tr></table></figure><blockquote><p>将ceph.pub公钥拷贝到其他ceph节点</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id -f -i /etc/ceph/ceph.pub ceph02</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id -f -i /etc/ceph/ceph.pub ceph03</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id -f -i /etc/ceph/ceph.pub ceph04</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ssh-copy-id -f -i /etc/ceph/ceph.pub ceph05</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 也可以用以下命令拷贝，在ceph01上操作</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..5&#125;;<span class="keyword">do</span> ssh-copy-id -f -i /etc/ceph/ceph.pub ceph0<span class="variable">$i</span>;<span class="keyword">done</span></span></span><br></pre></td></tr></table></figure><blockquote><p>使用cephadm将主机添加到存储集群中,执行添加节点命令后，会在目标节点拉到ceph/node-exporter镜像，需要一定时间，所以可提前在节点上将镜像导入。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cephadm shell ceph orch host add ceph02 192.168.59.242 --labels=mon,mgr</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host add ceph03 192.168.59.243 --labels=mon</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host add ceph04 192.168.59.244</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host add ceph05 192.168.59.245</span></span><br></pre></td></tr></table></figure><blockquote><p>查看加入到集群的节点</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host <span class="built_in">ls</span></span></span><br><span class="line">HOST    ADDR            LABELS   STATUS  </span><br><span class="line">ceph01  192.168.59.241  _admin           </span><br><span class="line">ceph02  192.168.59.242  mon mgr          </span><br><span class="line">ceph03  192.168.59.243  mon              </span><br><span class="line">ceph04  192.168.59.244                   </span><br><span class="line">ceph05  192.168.59.245                   </span><br><span class="line">5 hosts in cluster</span><br></pre></td></tr></table></figure><ol><li>给节点添加标签、删除标签</li></ol><blockquote><p>给节点打上指标标签后，后续可以按标签进行编排。</p><p>给节点打_admin标签，默认情况下，_admin标签应用于存储集群中的 bootstrapped 主机， client.admin密钥被分发到该主机(ceph orch client-keyring {ls|set|rm})。<br>将这个标签添加到其他主机后，其他主机的/etc/ceph下也将有client.admin密钥。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 给 ceph02、ceph03加上 _admin 标签</span></span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph02 _admin</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph03 _admin</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 给 ceph01-ceph04加上 mon 标签</span></span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph01 mon</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph02 mon</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph03 mon</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph04 mon</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 给 ceph01、ceph02加上 mgr 标签</span></span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> </span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host label add ceph02 mgr</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 列出节点，查看节点上标签</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch host <span class="built_in">ls</span></span></span><br><span class="line">HOST    ADDR            LABELS          STATUS  </span><br><span class="line">ceph01  192.168.59.241  _admin mon mgr          </span><br><span class="line">ceph02  192.168.59.242  mon mgr _admin          </span><br><span class="line">ceph03  192.168.59.243  mon _admin              </span><br><span class="line">ceph04  192.168.59.244  mon                     </span><br><span class="line">ceph05  192.168.59.245                          </span><br><span class="line">5 hosts in cluster</span><br></pre></td></tr></table></figure><blockquote><p>删除标签<br>注意：删除节点上的_admin标签，并不会删除该节点上已有的<code>ceph.client.admin.keyring</code>密钥文件</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch host label rm ceph03 label=_admin</span><br></pre></td></tr></table></figure><ol><li>添加osd</li></ol><blockquote><p>说明：添加OSD时，建议将磁盘先格式化为无分区的原始磁盘</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># https://rook.github.io/docs/rook/v1.10/Getting-Started/ceph-teardown/?h=sgdisk#zapping-devices</span></span></span><br><span class="line">DISK=&quot;/dev/sdX&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)</span></span></span><br><span class="line">sgdisk --zap-all $DISK</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Wipe a large portion of the beginning of the disk to remove more LVM metadata that may be present</span></span></span><br><span class="line">dd if=/dev/zero of=&quot;$DISK&quot; bs=1M count=100 oflag=direct,dsync</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># SSDs may be better cleaned with blkdiscard instead of dd</span></span></span><br><span class="line">blkdiscard $DISK</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># Inform the OS of partition table changes</span></span></span><br><span class="line">partprobe $DISK</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 查看各ceph节点有哪些磁盘是可用的，关注`AVAILABLE`列</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch device <span class="built_in">ls</span></span></span><br><span class="line">HOST    PATH          TYPE  DEVICE ID                                             SIZE  AVAILABLE  REFRESHED  REJECT REASONS  </span><br><span class="line">ceph01  /dev/nvme0n2  ssd   VMware_Virtual_NVMe_Disk_VMware_NVME_0000             107G  Yes        17m ago                    </span><br><span class="line">ceph01  /dev/sda      hdd   VMware_Virtual_SATA_Hard_Drive_00000000000000000001   107G  Yes        17m ago                    </span><br><span class="line">ceph02  /dev/nvme0n2  ssd   VMware_Virtual_NVMe_Disk_VMware_NVME_0000             107G  Yes        18m ago                    </span><br><span class="line">ceph02  /dev/sda      hdd   VMware_Virtual_SATA_Hard_Drive_00000000000000000001   107G  Yes        18m ago                    </span><br><span class="line">ceph03  /dev/nvme0n2  ssd   VMware_Virtual_NVMe_Disk_VMware_NVME_0000             107G  Yes        25m ago                    </span><br><span class="line">ceph03  /dev/sda      hdd   VMware_Virtual_SATA_Hard_Drive_00000000000000000001   107G  Yes        25m ago                    </span><br><span class="line">ceph04  /dev/nvme0n2  ssd   VMware_Virtual_NVMe_Disk_VMware_NVME_0000             107G  Yes        17m ago                    </span><br><span class="line">ceph04  /dev/sda      hdd   VMware_Virtual_SATA_Hard_Drive_00000000000000000001   107G  Yes        17m ago                    </span><br><span class="line">ceph05  /dev/nvme0n2  ssd   VMware_Virtual_NVMe_Disk_VMware_NVME_0000             107G  Yes        17m ago                    </span><br><span class="line">ceph05  /dev/sda      hdd   VMware_Virtual_SATA_Hard_Drive_00000000000000000001   107G  Yes        17m ago  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 接下来初始化osd</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 将指定的磁盘格式化为无分区的原始磁盘</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">blkdiscard /dev/nvme0n2</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cephadm shell ceph orch device zap ceph01 /dev/sda</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 接着初始化其他节点上磁盘</span></span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 添加OSD</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph01:/dev/nvme0n2</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph01:/dev/sda</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph02:/dev/nvme0n2</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph02:/dev/sda</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph03:/dev/nvme0n2</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph03:/dev/sda</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph04:/dev/nvme0n2</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph04:/dev/sda</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph05:/dev/nvme0n2</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch daemon add osd ceph05:/dev/sda</span>             </span><br></pre></td></tr></table></figure><ol><li>添加池</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd pool create ssdpool 256 256</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd pool create hddpool 256 256</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 列出池</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd lspools</span></span><br><span class="line">1 .mgr</span><br><span class="line">2 ssdpool</span><br><span class="line">3 hddpool</span><br></pre></td></tr></table></figure><ol><li>创建规则以使用该设备</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd crush rule create-replicated ssd default host ssd</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd crush rule create-replicated hdd default host hdd</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 查看池规则</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd crush rule <span class="built_in">ls</span></span></span><br><span class="line">replicated_rule</span><br><span class="line">ssd</span><br><span class="line">hdd</span><br></pre></td></tr></table></figure><ol><li>将池设置为使用规则</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set ssdpool crush_rule ssd</span><br><span class="line">ceph osd pool set hddpool crush_rule hdd</span><br></pre></td></tr></table></figure><ol><li>删除池</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 删除池时，池的名字要输入2次</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd pool <span class="built_in">rm</span> testpool --yes-i-really-really-mean-it</span></span><br><span class="line">Error EPERM: WARNING: this will *PERMANENTLY DESTROY* all data stored in pool testpool.  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, followed by --yes-i-really-really-mean-it.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd pool <span class="built_in">rm</span> testpool testpool --yes-i-really-really-mean-it</span></span><br></pre></td></tr></table></figure><ol><li>MDS守护进程用于Cephfs(文件系统)，MDS采用的是主备模式，即cephfs仅使用1个活跃的MDS守护进程，配置MDS服务有多种方法，此处介绍2种，大同小异。</li></ol><blockquote><p>先创建CephFS，然后使用placement部署MDS</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 1. 先创建CephFS池</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd pool create cephfs_data 128 128</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph osd pool create cephfs_metadata 64 64</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 2. 为数据池和元数据池创建文件系统</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph fs new cephfs cephfs_metadata cephfs_data</span></span><br><span class="line">new fs with metadata pool 5 and data pool 4</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 3. 使用ceph orch apply 命令部署MDS服务</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch apply mds cephfs --placement=<span class="string">&quot;3 ceph01 ceph02 ceph03&quot;</span></span></span><br><span class="line">Scheduled mds.cephfs update...</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 最后查看状态</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph fs <span class="built_in">ls</span></span></span><br><span class="line">name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph fs status</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch ps --daemon_type=mds</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph -s</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 列出服务</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">ceph orch <span class="built_in">ls</span></span></span><br></pre></td></tr></table></figure><ol><li>可以删除不需要的组件</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph orch rm prometheus</span><br><span class="line">ceph orch rm grafana</span><br><span class="line">ceph orch rm alertmanager</span><br><span class="line">ceph orch rm node-exporter</span><br><span class="line"></span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/3338574514/" title="ceph quincy 版本部署">https://amostian.github.io/posts/3338574514/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E5%AD%98%E5%82%A8/" rel="tag"><i class="fa fa-tags"></i> 存储</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/" rel="tag"><i class="fa fa-tags"></i> 分布式存储</a> <a href="/tags/Ceph/" rel="tag"><i class="fa fa-tags"></i> Ceph</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/2133318637/" rel="prev" title="gym"><i class="fa fa-chevron-left"></i> gym</a></div><div class="post-nav-item"><a href="/posts/3976404923/" rel="next" title="DP算法">DP算法 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#ceph-quincy-%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2"><span class="nav-text">ceph quincy 版本部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E"><span class="nav-text">实验环境说明</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4"><span class="nav-text">部署步骤</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">217</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">65</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">82</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1151.9k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">46:53</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>