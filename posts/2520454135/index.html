<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="强化学习——蘑菇书"><meta property="og:url" content="https://amostian.github.io/posts/2520454135/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240215101428098.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240120181930032.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240121105831597.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240121110703615.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240121151255942.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240213154216608.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240213154436559.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240212095633737.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240122154119532.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240122104847005.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240122105239298.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240122105854628.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240122203239729.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240122203341762.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240206103605935.png"><meta property="og:image" content="https://amostian.github.io/posts/2520454135/image-20240215232529968.png"><meta property="article:published_time" content="2024-01-21T16:38:22.000Z"><meta property="article:modified_time" content="2024-02-17T07:13:16.202Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="强化学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/2520454135/image-20240215101428098.png"><link rel="canonical" href="https://amostian.github.io/posts/2520454135/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>强化学习——蘑菇书 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">58</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">74</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">353</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/2520454135/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">强化学习——蘑菇书</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-01-22 00:38:22" itemprop="dateCreated datePublished" datetime="2024-01-22T00:38:22+08:00">2024-01-22</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-02-17 15:13:16" itemprop="dateModified" datetime="2024-02-17T15:13:16+08:00">2024-02-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">1.6k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>3 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><p>强化学习中，智能体与环境的交互过程可以用马尔科夫过程表示</p><ul><li><p>马尔科夫过程(Markov process, MP)以及马尔科夫奖励过程(Markov reward process, MRP) 是马尔科夫决策过程(Markov decision process,MDP) 的简化过程</p></li><li><p>预测(策略评估)和控制是马尔科夫决策过程中的两种价值函数计算方法</p><ul><li><p>策略评估(policy evaluation)是给定决策后，怎么计算它的价值函数</p></li><li><p>控制(prediction)是寻找最优策略</p><p>具体有两种算法：策略迭代(policy iteration)和价值迭代(value iteration)</p></li></ul></li></ul><p>在马尔科夫决策过程中，环境是全部可观测的</p><ul><li>很多时候，环境中有些量是不可观测的，这种部分可预测的问题也可以转换成马尔科夫决策过程</li></ul><h2 id="2-0-从随机变量到马尔科夫过程"><a href="#2-0-从随机变量到马尔科夫过程" class="headerlink" title="2.0 从随机变量到马尔科夫过程"></a>2.0 从随机变量到马尔科夫过程</h2><p>对于两个相互独立的随机变量 $X$ 和 $Y$ ，可以通过寻找二者的映射关系进行数据分析</p><p>对于一组相互影响、有关联关系的非独立随机变量 $s_t,s_{t+1},s_{t+2},\cdots$ ，无法通过拟合的方式寻找变量间的关系，即这些随机变量间构成一个随机过程 $\{s_t\}_{t=1}^{\infty}$</p><p>具有马尔科夫性质的随机过程称为马尔科夫链/马尔科夫过程</p><ul><li>一阶马尔科夫链 $P(s_{t+1}\vert s_t,s_{t-1},\cdots)=P(s_{t+1}\vert s_t)$</li><li>二阶马尔科夫链 $P(s_{t+1}\vert s_t,s_{t-1},\cdots)=P(s_{t+1}\vert s_t,s_{t-1})$</li></ul><p>马尔科夫过程中引入状态空间模型(HMM，Kalman Filter，Praticle Filter)</p><ul><li><p>当前观测 $o_i$ 仅与 $s_i$ 有关，与 $s_{i-1}$ 无关</p><p><img src="/posts/2520454135/image-20240215101428098.png" alt="image-20240215101428098"></p></li></ul><p>马尔科夫奖励过程获得的奖励是随机的</p><p>马尔科夫决策过程获得的奖励是定向的，即通过策略影响获得的奖励</p><ul><li><p>动态特性是马尔科夫决策过程的一个重要特点</p><script type="math/tex;mode=display">P(s',r\vert s,a)\overset{\Delta}{=}P_{r}(S_{t+1}=s',R_{t+1}=r\vert S_t=s,A_t=a)</script><p>其中 $r$ 也是随机变量，即采取同样的动作从当前状态转移到同一未来状态，奖励也是不确定的</p></li><li><p>状态转移与动态特性的关系</p><script type="math/tex;mode=display">P(s'\vert s,a)=\sum\limits_{r\in \mathcal{R}}P(s',r\vert s,a)</script></li></ul><h2 id="2-1-马尔可夫过程"><a href="#2-1-马尔可夫过程" class="headerlink" title="2.1 马尔可夫过程"></a>2.1 马尔可夫过程</h2><p>Markov——马尔科夫性质</p><p>decision——策略</p><p>process——条件概率表示状态转移</p><h3 id="2-1-1-马尔可夫性质"><a href="#2-1-1-马尔可夫性质" class="headerlink" title="2.1.1 马尔可夫性质"></a>2.1.1 马尔可夫性质</h3><blockquote><p>马尔科夫性质(Markov property)指一个随机过程在给定现在状态及所有过去状态下，其未来状态的条件概率分布仅依赖于当前状态。即在给定当前状态下，将来的状态和过去的状态是条件独立的</p></blockquote><p>假设变量 $X_0,X_1,\cdots,X_T$ 构成一个随机过程，这些变量所有可能的取值集合为状态空间，若未来状态 $X_{t+1}$ 对于过去状态序列 $X_{0:t}$ 的条件概率分布仅是 $X_t$ 的一个函数</p><script type="math/tex;mode=display">P(X_{t+1}=s_{t+1}\vert X_{0:t}=s_{0:t})=P(X_{t+1}=s_{t+1}\vert X_t=s_t)</script><p>则该随机过程具有马尔科夫性质</p><h2 id="2-1-2-马尔可夫过程的数学表示"><a href="#2-1-2-马尔可夫过程的数学表示" class="headerlink" title="2.1.2 马尔可夫过程的数学表示"></a>2.1.2 马尔可夫过程的数学表示</h2><p>马尔科夫过程是一组具有马尔科夫性质的随机变量序列 $s_1,s_2,\cdots,s_t$ ，其中下一个时刻的状态 $s_{t+1}$ 只取决于当前状态 $s_t$ ，设状态历史为 $h_t=\{s_1,s_2,\cdots,s_t\}$ ，则马尔科夫过程满足：</p><script type="math/tex;mode=display">P(s_{t+1}\vert s_t)=P(s_{t+1}\vert h_t)</script><ul><li>离散时间的马尔科夫过程称为马尔科夫链(Markov chain)，其状态是有限的</li></ul><p>用状态转移矩阵(state transition matrix)P表示状态转移 $P(s_{t+1}=s’\vert s_t=s)$ ，假设有 $N$ 个状态</p><script type="math/tex;mode=display">\mathbf{P}=\begin{pmatrix}
P(s_1\vert s_1)&P(s_2\vert s_1)&\cdots&P(s_N\vert s_1)\\
P(s_2\vert s_1)&P(s_2\vert s_2)&\cdots&P(s_N\vert s_2)\\
\vdots&\vdots&\ddots&\vdots\\
P(s_N\vert s_1)&P(s_N\vert s_1)&\cdots&P(s_N\vert s_N)\\
\end{pmatrix}</script><p><img src="/posts/2520454135/image-20240120181930032.png" alt="image-20240120181930032"></p><p>在给定马尔科夫链后，对马尔科夫链的一次采样，即可得到一个轨迹</p><p><strong>状态转移是确定的，轨迹是随机的</strong></p><h2 id="2-2-马尔可夫奖励过程"><a href="#2-2-马尔可夫奖励过程" class="headerlink" title="2.2 马尔可夫奖励过程"></a>2.2 马尔可夫奖励过程</h2><blockquote><p>马尔科夫奖励过程是马尔科夫链加上奖励函数</p></blockquote><p>奖励函数用期望 $R(s)$ 表示，当到达某一个状态时可以获得多大的奖励，一般用向量表示</p><h3 id="2-2-1-回报与价值函数"><a href="#2-2-1-回报与价值函数" class="headerlink" title="2.2.1 回报与价值函数"></a>2.2.1 回报与价值函数</h3><p><strong>范围</strong> ：每个回合的最大时间步数，用 $T$ 表示</p><ul><li>一个状态可以多次出现在一个回合中，所以 $T &gt;/&lt;/= N$ 都可能出现，$N$ 为马尔科夫过程的状态总数</li></ul><p><strong>回报</strong> ：沿着轨迹的奖励逐步叠加，假设时刻 $t$ 后的奖励序列为 $r_{t+1},r_{t+2},\cdots$ ，则回报定义为</p><script type="math/tex;mode=display">G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\cdots+\gamma^{T-t-1} r_{T}=\sum\limits_{i=t}^T\gamma^{i-t}r_{i+1},\quad \gamma\in[0,1]\tag{2.1}\label{2.1}</script><p>其中，$\gamma$ 为折扣因子，越往后得到的奖励，打的折扣越多，表明我们期望更快地得到尽可能多的奖励</p><p><strong>状态价值函数</strong> ：以某个状态为起始，获得回报的期望</p><script type="math/tex;mode=display">V_t(s)=E[G_t\vert s_t=s]=E[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\cdots+\gamma ^{T-t-1} r_{T}\vert s_t=s]\tag{2.2}\label{2.2}</script><p>回报通过单个轨迹计算所得，状态价值通过多个轨迹计算所得。即回报无法作为状态好坏的评价指标，需要获取以当前状态为起点可以获得回报的期望来作为状态好坏的评价指标(状态价值)</p><ul><li>当采取确定性策略时，从一个状态出发会得到固定的轨迹，此时 $G_t=V(s_t)$<h4 id="折扣因子"><a href="#折扣因子" class="headerlink" title="折扣因子"></a>折扣因子</h4></li></ul><p>智能体在选择动作时，会更多地考虑那些能够带来长期利益的行动，而不是追求短期的高回报</p><p>有些马尔可夫过程是带环的，我们想量化状态价值就必须避免无穷奖励</p><ul><li><p>只是简单的回报相加，对于无穷轨迹，得到的回报是发散的</p></li><li><p>带折扣因子的回报，即使所有奖励都是 $+1$ ，也会收敛</p><script type="math/tex;mode=display">G=1+\gamma\cdot 1+\gamma^2\cdot 1+\gamma^3 \cdot 1+\cdots=\frac{1\cdot(1-\gamma^n)}{1-\gamma}\rightarrow\frac{1}{1-\gamma}<1</script></li></ul><p>我们不能完全信任模型（并不能建立完全模拟环境的模型，对未来的评估未必准确），为了表示这种不确定性，所以对未来的奖励打折扣</p><p>若把价值函数看做未来可能获得累积奖励的当前价值表现，则未来奖励与即时奖励不总具有同等价值，其受时间，不确定性和其他因素的影响</p><p>若奖励是有实际价值的，我们可能更希望尽快获得奖励（现在的钱比以后更有实际价值）</p><p><strong>折扣因子为强化学习超参数</strong></p><ul><li>$\gamma=0$ ，只关注当前奖励</li><li>$\gamma=1$ ，当前奖励与未来奖励具有同等价值</li></ul><h3 id="2-2-2-回报的计算"><a href="#2-2-2-回报的计算" class="headerlink" title="2.2.2 回报的计算"></a>2.2.2 回报的计算</h3><p>若 $s_1,s_2,\cdots,s_7$ 构成一个马尔科夫过程，设其奖励函数表示为 $R=[5,0,0,0,0,0,10]$</p><p><img src="/posts/2520454135/image-20240121105831597.png" alt="image-20240121105831597"></p><p>以 $s_{t+1}=s_4$ 为下一未来状态 ($\gamma=0.5$)来计算回报 $G$ ，计算每个轨迹的回报</p><p><img src="/posts/2520454135/image-20240121110703615.png" alt="image-20240121110703615"></p><h3 id="2-2-3-价值函数的计算方法"><a href="#2-2-3-价值函数的计算方法" class="headerlink" title="2.2.3 价值函数的计算方法"></a>2.2.3 价值函数的计算方法</h3><h4 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h4><p>蒙特卡洛方法(Monte Carlo,MC)：</p><p>计算 $E[G_t\vert s_t=s]$</p><p>生成很多轨迹，计算每条轨迹的回报，回报叠加后取平均值作为价值函数</p><h4 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h4><p>贝尔曼方程定义了当前状态与未来状态间的价值关系</p><p>令 $s_t=s,s_{t+1}=s’$</p><script type="math/tex;mode=display">V(s)=\underbrace{R(s)}_{即时奖励}+\underbrace{\gamma\sum\limits_{s'\in S}P(s'\vert s)V(s')}_{未来奖励的折扣总和}\tag{贝尔曼方程}\label{BellmanEquation}</script><h5 id="贝尔曼方程推导"><a href="#贝尔曼方程推导" class="headerlink" title="贝尔曼方程推导"></a>贝尔曼方程推导</h5><script type="math/tex;mode=display">\begin{align}
V(s)&=E[G_t\vert s_t=s]\\
&=E[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\cdots+\gamma^{T-t-1}r_{T}\vert s_t=s]\\
&=E[r_{t+1}\vert s_t=s]+\gamma E[r_{t+2}+\gamma r_{t+3}+\cdots+\gamma^{T-t-2}r_{T}\vert s_t=s]\\
&=R(s)+\gamma E[G_{t+1}\vert s_t=s] \tag{2.3}\label{2.3}
\end{align}</script><p>令 $G_{t+1}=g’$ ，$E[G_{t+1}\vert s_t=s]=E[g’\vert s]$ ，且 $\sum\limits_{s’\in S}P(s’\vert s)V(s’)=E\left[V(s’)\vert s\right]$</p><p>相当于求证 $E[g’\vert s]=E\left[V(s’)\vert s\right]$</p><script type="math/tex;mode=display">\begin{aligned}
E\left[V(s')\vert s\right]&=E\left[E[g'\vert s']\big\vert s\right]\\
&=E\left[\sum\limits_{g'}P(g'\vert s')\cdot g'\Bigg\vert s\right]\\
&=\sum\limits_{s'}P(s'\vert s)\cdot \sum\limits_{g'}P(g'\vert s',s)\cdot g'\\
&=\sum\limits_{s'}\sum\limits_{g'}\frac{P(g'\vert s',s)\cdot g'\cdot P(s'\vert s)\cdot P(s)}{P(s)}\\
&=\sum\limits_{s'}\sum\limits_{g'}\frac{P(g'\vert s',s)\cdot P(s',s)\cdot g'}{P(s)}\\
&=\sum\limits_{s'}\sum\limits_{g'}\frac{P(g',s',s)\cdot g'}{P(s)}\\
&=\sum\limits_{s'}\sum\limits_{g'}g'\cdot P(g',s'\vert s)\\
&=\sum\limits_{g'}\sum\limits_{s'}g'\cdot P(g',s'\vert s)\\
&=\sum\limits_{g'}g'\cdot P(g'\vert s)\\
&=E\left[g'\vert s\right]
\end{aligned}</script><p>故代入 $\eqref{2.3}$ ，得证贝尔曼方程 $\eqref{BellmanEquation}$</p><script type="math/tex;mode=display">\begin{align}
V(s)&=R(s)+\gamma E\left[V(s')\vert s\right]\\
&=R(s)+\gamma \sum\limits_{s'\in S}P(s'\vert s)\cdot V(s')\\
&=E[r_{t+1}+\gamma V(s')\big\vert s]
\end{align}</script><h5 id="贝尔曼方程的解析解"><a href="#贝尔曼方程的解析解" class="headerlink" title="贝尔曼方程的解析解"></a>贝尔曼方程的解析解</h5><p>对于一个马尔科夫过程的 $N$ 个状态 $s_i\in \mathcal{S},i\in [1,N]$ ，可以列出 $N$ 个贝尔曼方程，解得每个状态的价值</p><p><img src="/posts/2520454135/image-20240121151255942.png" alt="image-20240121151255942"></p><ul><li><p>状态价值向量 $\mathbf{V}=\begin{pmatrix}V(s_1)\\V(s_2)\\\vdots\\V(S_N)\end{pmatrix}$</p></li><li><p>即时奖励 $\mathbf{R}=\begin{pmatrix}R(s_1)\\R(s_2)\\\vdots\\R(s_N)\end{pmatrix}$</p></li><li><p>状态转移矩阵</p><script type="math/tex;mode=display">\mathbf{P}=\begin{pmatrix}
P(s_1\vert s_1)&P(s_2\vert s_1)&\cdots&P(s_N\vert s_1)\\
P(s_2\vert s_1)&P(s_2\vert s_2)&\cdots&P(s_N\vert s_2)\\
\vdots&\vdots&\ddots&\vdots\\
P(s_N\vert s_1)&P(s_N\vert s_1)&\cdots&P(s_N\vert s_N)\\
\end{pmatrix}</script></li></ul><p>则代入 $\eqref{BellmanEquation}$ ，表示为</p><script type="math/tex;mode=display">\begin{pmatrix}V(s'_1)\\V(s'_2)\\\vdots\\V(S'_N)\end{pmatrix}=\begin{pmatrix}R(s_1)\\R(s_2)\\\vdots\\R(s_N)\end{pmatrix}+\gamma\begin{pmatrix}
P(s_1\vert s_1)&P(s_2\vert s_1)&\cdots&P(s_N\vert s_1)\\
P(s_2\vert s_1)&P(s_2\vert s_2)&\cdots&P(s_N\vert s_2)\\
\vdots&\vdots&\ddots&\vdots\\
P(s_N\vert s_1)&P(s_N\vert s_1)&\cdots&P(s_N\vert s_N)\\
\end{pmatrix}\begin{pmatrix}V(s_1)\\V(s_2)\\\vdots\\V(S_N)\end{pmatrix}</script><p>即</p><script type="math/tex;mode=display">\begin{aligned}
\mathbf{V}&=\mathbf{R}+\gamma\mathbf{P}\mathbf{V}\\
(\mathbf{I-\gamma P})\mathbf{V}&=\mathbf{R}\\
\mathbf{V}&=(\mathbf{I}-\gamma\mathbf{P})^{-1}\mathbf{R}
\end{aligned}</script><p>但矩阵求逆的时间复杂度为 $O(N^3)$ ，这种通过解析解的方法只适用于很小量的马尔科夫奖励过程</p><p>$I-\gamma P$ 是可逆的：</p><p><img src="/posts/2520454135/image-20240213154216608.png" alt="image-20240213154216608"></p><p>$(I-\gamma P)^{-1}\ge I$ ：</p><p>每个分量都是非负的，且不小于单位值</p><p><img src="/posts/2520454135/image-20240213154436559.png" alt="image-20240213154436559"></p><p>对每个奖励向量，$(I-\gamma P)^{-1}r\ge r\ge 0$</p><h5 id="贝尔曼方程的数值解"><a href="#贝尔曼方程的数值解" class="headerlink" title="贝尔曼方程的数值解"></a>贝尔曼方程的数值解</h5><p><strong>迭代法</strong></p><ul><li>蒙特卡洛方法</li><li>动态规划法</li><li>时序差分学习(temporal-difference learning, TD learning)，前两者的结合</li></ul><h6 id="蒙特卡洛方法-1"><a href="#蒙特卡洛方法-1" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h6><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&i\leftarrow 0,G_t\leftarrow 0\\
&当i\neq N时，执行\\
&\qquad生成一个回合的轨迹，从状态s和时刻t开始\\
&\qquad使用生成的轨迹计算回报 g=\sum\limits_{j=t}^{T-1}\gamma^{j-t}r_{j+1}\\
&\qquad G_t\leftarrow G_t+g,i\leftarrow i+1\\
&结束循环\\
&V_t(s)=\frac{G_t}{N}\\
\hline
\end{array}</script><h6 id="动态规划方法"><a href="#动态规划方法" class="headerlink" title="动态规划方法"></a>动态规划方法</h6><p>通过自举(bootstrapping)的方法，不停地迭代贝尔曼方程，当最后更新的状态与上一状态区别不大时，就可以停止迭代，将输出的 $V’(s)$ 作为当前的状态的价值</p><script type="math/tex;mode=display">\begin{array}{ll}
\hline
&对于所有状态s\in S,V'(s)\leftarrow 0,V(s)\leftarrow\infty\\
&当 \Vert V-V'\Vert>\epsilon，执行\\
&\qquad V\leftarrow V'\\
&\qquad 对于所有状态 s\in S,V'(s)=R(s)+\gamma\sum\limits_{s'\in S}P(s'\vert s)V(s')\\
&结束循环\\
& 返回V'(s)\\
\hline
\end{array}</script><h2 id="2-3-马尔科夫决策过程"><a href="#2-3-马尔科夫决策过程" class="headerlink" title="2.3 马尔科夫决策过程"></a>2.3 马尔科夫决策过程</h2><h3 id="2-3-1-MDP与MP-MRP"><a href="#2-3-1-MDP与MP-MRP" class="headerlink" title="2.3.1 MDP与MP/MRP"></a>2.3.1 MDP与MP/MRP</h3><p>马尔科夫决策过程在马尔科夫奖励的过程基础上多了决策 $a_t$</p><ul><li><p>马尔科夫性质：$P(s_{t+1}\vert s_t,a_t)=P(s_{t+1}\vert h_t,a_t)$</p></li><li><p>奖励函数：$R(s_t=s,a_t=a)$</p></li><li><p>状态转移：$P(s_{t+1}=s’\vert s_t=s,a_t=a)$</p><ul><li><p>马尔科夫奖励过程/马尔科夫过程的状态转移是直接的，直接通过状态转移概率决定下一状态 $s_{t+1}$</p></li><li><p>马尔科夫决策过程的状态转移具有一定的不确定性，当前状态与未来状态之间多了一个决策过程。在当前状态下，智能体首先要决定采取哪一种动作，所以智能体进入的未来状态也是一种概率分布</p><p><img src="/posts/2520454135/image-20240212095633737.png" alt="image-20240212095633737"></p></li></ul></li></ul><p>对于马尔科夫过程/马尔科夫奖励过程，到达某个状态的价值是确定的，但由于轨迹是随机的，所以最终得到的回报不确定。</p><p><img src="/posts/2520454135/image-20240122154119532.png" alt="image-20240122154119532"></p><p>对于马尔科夫决策过程，策略产生决策的决策决定状态的变化方向，通过智能体的策略，我们可以尽快地获得尽可能多的回报</p><h4 id="MDP中的策略"><a href="#MDP中的策略" class="headerlink" title="MDP中的策略"></a>MDP中的策略</h4><p>策略定义了在某一状态应该采取什么样的动作。有两种形式</p><ul><li><p>概率表示，当前状态下每个采取每个动作的可能性</p><script type="math/tex;mode=display">\begin{array}{ll}
随机性策略&\pi(a\vert s)=P(a_t=a\vert s_t=s)\\
\end{array}</script></li><li><p>直接输出当前状态应该采取那种动作</p><script type="math/tex;mode=display">\begin{array}{ll}
确定性策略&a\overset{\Delta}{=}\pi(s)或\pi(a\vert s)\overset{\Delta}{=}P(a_t=a\vert s_t=s)=1\\
\end{array}</script></li></ul><p><strong>强化学习中的归纳偏置</strong> ：假设策略函数都是稳定的，故不同时间采取的动作都是对策略函数的采样</p><h4 id="MDP与MRP"><a href="#MDP与MRP" class="headerlink" title="MDP与MRP"></a>MDP与MRP</h4><p>若已知马尔科夫决策过程及策略 $\pi$ ，就可以将马尔科夫决策过程转换为马尔科夫奖励过程</p><p>若已知策略函数，相当于在某个状态下，已知可能采取的每个动作的可能性，进而可以知道状态转移的概率</p><script type="math/tex;mode=display">P_\pi(s'\vert s)=\sum\limits_{a\in A}\pi(a\vert s)P(s'\vert s,a)</script><p>对于奖励函数，也是类似的</p><script type="math/tex;mode=display">R_\pi(s)=\sum\limits_{a\in A}\pi(a\vert s)R(s,a)</script><h3 id="2-3-2-MDP中的价值函数"><a href="#2-3-2-MDP中的价值函数" class="headerlink" title="2.3.2 MDP中的价值函数"></a>2.3.2 MDP中的价值函数</h3><p>状态价值函数：在当前状态下，基于当前策略可以获得的期望回报</p><script type="math/tex;mode=display">V_\pi(s)=E[G_t\vert s_t=s]</script><p>动作价值函数：在某一状态下，基于当前策略生成的某一动作可能得到的期望回报</p><script type="math/tex;mode=display">Q_{\pi}(s,a)=E_{\pi}[G_t\vert s_t=s,a_t=a]</script><p>对于一个马尔科夫过程，策略确定后，对动作采样可以使动作价值转换为状态价值</p><script type="math/tex;mode=display">V_\pi(s)=\sum\limits_{a\in A}\pi(a\vert s)Q_\pi(s,a)\tag{V=f(Q)}\label{V=f(Q)}</script><ul><li>可以将状态价值函数理解为动作价值函数的加权平均，权重为策略</li></ul><h4 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h4><h5 id="贝尔曼动作价值期望方程"><a href="#贝尔曼动作价值期望方程" class="headerlink" title="贝尔曼动作价值期望方程"></a>贝尔曼动作价值期望方程</h5><p>对动作价值函数分解，得到Q函数的贝尔曼期望方程</p><script type="math/tex;mode=display">\begin{align}
Q_\pi(s,a)&=E_\pi[G_t\vert s_t=s,a_t=a]\\
&=E_\pi[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots\vert s_t=s,a_t=a]\\
&=E_\pi\left[r_{t+1}+\gamma G_{t+1}\big\vert s_t=s,a_t=a\right]\tag{贝尔曼期望方程-动作}\label{BellmanExpectation_Q}
\end{align}</script><ul><li><p>立即奖励</p><script type="math/tex;mode=display">\begin{align}
E_{\pi}[R_{t+1}\vert s_t=s,a_t=a]&=\sum\limits_{r'}P(r'\vert s,a)r'\\
&=R(s,a)\tag{立即动作奖励}\label{Q_immediate_reward}
\end{align}</script></li><li><p>折扣奖励</p><script type="math/tex;mode=display">\begin{align}
E_{\pi}[G_{t+1}\vert s_t=s,a_t=a]&=\sum\limits_{g’}g’P(g’\vert s,a)\\
&=\sum\limits_{g’}g’\sum\limits_{s’} P(g’,s’\vert s,a)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’ P(g’,s’\vert s,a)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’\frac{P(g’,s’,s,a)}{P(s,a)}\cdot\frac{P(s,a)}{P(s’,s,a)}P(s’\vert s,a)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’\frac{P(g’,s’,s,a)}{P(s’,s,a)}P(s’\vert s,a)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’P(g’\vert s’,s,a)P(s’\vert s,a)\\
&=\sum\limits_{s’}E_{\pi}[g’\vert s’,s,a]P(s’\vert s,a)\\
&\xlongequal{马尔可夫性质}\sum\limits_{s’}E_{\pi}[g’\vert s’]P(s’\vert s,a)\\
&=\sum\limits_{s’}V_{\pi}(s’)P(s’\vert s,a)\tag{折扣奖励}\label{Q_discounted_reward}\\
&=E[V_{\pi}(s')\vert s,a]
\end{align}</script></li></ul><p>将 $\eqref{Q_immediate_reward}$ 与 $\eqref{Q_discounted_reward}$ 代入 $\eqref{BellmanExpectation_Q}$ 可得</p><script type="math/tex;mode=display">\begin{align}
Q_\pi(s,a)&=E_\pi\left[r_{t+1}\big\vert s_t=s,a_t=a\right]+\gamma E_\pi\left[G_{t+1}\big\vert s_t=s,a_t=a\right]\\
&=R(s,a)+\gamma E[V_{\pi}(s')\vert s,a]\\
&=\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s’}V_{\pi}(s’)P(s’\vert s,a)\tag{Q=f(V)}\label{Q=f(V)}
\end{align}</script><h5 id="贝尔曼状态价值期望方程"><a href="#贝尔曼状态价值期望方程" class="headerlink" title="贝尔曼状态价值期望方程"></a>贝尔曼状态价值期望方程</h5><p>将状态价值函数分解为即时奖励与未来折扣奖励，可以得到状态价值的贝尔曼期望方程</p><script type="math/tex;mode=display">\begin{align}
V_\pi(s)&=E_\pi[G_{t}\vert s_t=s]\\
&=E_\pi\left[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots\big\vert s_t=s\right]\\
&=E_\pi\left[r_{t+1}+\gamma G_{t+1}\big\vert s_t=s\right]
\tag{贝尔曼期望方程-状态}\label{BellmanExpectation_V}
\end{align}</script><ul><li><p>立即奖励</p><script type="math/tex;mode=display">\begin{align}
E_{\pi}[R_{t+1}\vert s_t=s]&=\sum\limits_{a}\pi(a\vert s)E[R_{t+1}\vert s_t=s,a_t=a]\\
&=\sum\limits_{a}\pi(a\vert s)\sum\limits_{r'}P(r'\vert s,a)r'\\
&=R(s)\tag{立即奖励}\label{V_immediate_reward}
\end{align}</script></li><li><p>未来折扣奖励</p><script type="math/tex;mode=display">\begin{align}
E_{\pi}[G_{t+1}\vert s_t=s]&=\sum\limits_{g’}g’P(g’\vert s)\\
&=\sum\limits_{g’}g’\sum\limits_{s’} P(g’,s’\vert s)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’ P(g’,s’\vert s)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’\frac{P(g’,s’,s)}{P(s)}\cdot\frac{P(s)}{P(s’,s)}P(s’\vert s)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’\frac{P(g’,s’,s)}{P(s,s’)}P(s’\vert s)\\
&=\sum\limits_{s’}\sum\limits_{g’}g’P(g’\vert s’,s)P(s’\vert s)\\
&=\sum\limits_{s’}E_{\pi}[g’\vert s’,s]P(s’\vert s)\\
&\xlongequal{马尔可夫性质}\sum\limits_{s’}E_{\pi}[g’\vert s’]P(s’\vert s)\\
&=\sum\limits_{s’}V_{\pi}(s’)P(s’\vert s)\\
&=\sum\limits_{s’}V_{\pi}(s’)\sum\limits_{a}P(s'\vert s,a)\pi(a\vert s)\\
&=\sum\limits_{a}\pi(a\vert s)\sum\limits_{s’}V_{\pi}(s’)P(s'\vert s,a)\tag{折扣奖励}\label{V_discounted_reward}\\
&=\sum\limits_{a}\pi(a\vert s)E[V_{\pi}(s')\vert s,a]
\end{align}</script></li></ul><p>将 $\eqref{V_immediate_reward}$ 与 $\eqref{V_discounted_reward}$ 代入 $\eqref{BellmanExpectation_V}$ 可得</p><script type="math/tex;mode=display">\begin{align}
V_\pi(s)&=E_\pi\left[R_{t+1}\big\vert s_t=s\right]+\gamma E_\pi\left[G_{t+1}\big\vert s_t=s\right]\\
&=R(s)+\gamma\sum\limits_{a}\pi(a\vert s)E[V_{\pi}(s')\vert s,a]\\
&=\underbrace{\sum\limits_{a}\pi(a\vert s)\sum\limits_{r'}P(r'\vert s,a)r'}_{立即奖励期望}+\underbrace{\gamma\sum\limits_{a}\pi(a\vert s)\sum\limits_{s’}V_{\pi}(s’)P(s'\vert s,a)}_{未来折扣奖励期望}\\
&=\sum\limits_{a}\pi(a\vert s)\left[\sum\limits_{r'}P(r'\vert s,a)r'+\sum\limits_{s’}V_{\pi}(s’)P(s'\vert s,a)\right]\\
&=\sum\limits_{a}\pi(a\vert s)Q_{\pi}(s,a)
\end{align}</script><h5 id="价值函数的自举"><a href="#价值函数的自举" class="headerlink" title="价值函数的自举"></a>价值函数的自举</h5><p>将 $\eqref{Q=f(V)}$ 与 $\eqref{V=f(Q)}$ 互相代入可得贝尔曼期望方程的迭代形式</p><script type="math/tex;mode=display">\begin{align}
V_\pi(s)&=\sum\limits_{a\in A}\pi(a\vert s)Q_\pi(s,a)\\
&=\sum\limits_{a\in A}\pi(a\vert s)\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_\pi(s')\right)\tag{贝尔曼期望方程-状态迭代式}\label{BellmanExpectation_itV}\\
Q_\pi(s)&=R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_\pi(s')\\
&=R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot\sum\limits_{a'\in A}\pi(a'\vert s')Q_\pi(s',a')\tag{贝尔曼期望方程-动作迭代式}\label{BellmanExpectation_itQ}
\end{align}</script><h4 id="备份图角度理解价值函数关系"><a href="#备份图角度理解价值函数关系" class="headerlink" title="备份图角度理解价值函数关系"></a>备份图角度理解价值函数关系</h4><p>备份：迭代关系，对于某一个状态，它的当前价值是与未来价值线性相关的</p><p><img src="/posts/2520454135/image-20240122104847005.png" alt="image-20240122104847005"></p><p>备份图：对备份图的备份/更新操作，将价值信息从后继未来状态转移回当前状态</p><p>状态价值函数的计算分解：定义了未来下一时刻状态价值函数与上一时刻状态价值函数间的关联</p><p><img src="/posts/2520454135/image-20240122105239298.png" alt="image-20240122105239298"></p><ul><li>对于 $(c)$ ，通过 $\eqref{Q=f(V)}$ 计算，对叶子结点的累加，向上备份一层。可以将未来的状态价值 $s’$ 备份到上一层</li><li>对于 $(b)$ ，通过 $\eqref{V=f(Q)}$ 计算，对父节点的累加，再向上备份一层。可得到当前状态的价值 $s$</li><li>可得，$V(s)$ 的迭代式 $\eqref{BellmanExpectation_itV}$</li></ul><p><img src="/posts/2520454135/image-20240122105854628.png" alt="image-20240122105854628"></p><ul><li>对于 $(c)$ ，通过 $\eqref{V=f(Q)}$ 计算，对叶子结点的累加，向上备份一层。可以将未来的动作价值 $s’$ 备份到上一层</li><li>对于 $(b)$ ，通过 $\eqref{Q=f(V)}$ 计算，对父节点的累加，再向上备份一层。可得到当前动作的价值 $s$</li><li>可得，$V(s)$ 的迭代式 $\eqref{BellmanExpectation_itQ}$</li></ul><h4 id="自举法收敛性证明"><a href="#自举法收敛性证明" class="headerlink" title="自举法收敛性证明"></a>自举法收敛性证明</h4><p>在使用自举法($\eqref{BellmanExpectation_itQ}$ 与 $\eqref{BellmanExpectation_itV}$)求解贝尔曼期望方程前，还需要考虑 <strong>迭代是否收敛的问题</strong></p><p>以状态价值函数为例，使用迭代算法计算贝尔曼方程写为向量形式如下</p><script type="math/tex;mode=display">V^{(k+1)}=R_{\pi}+\gamma P_{\pi}V^{(k)}</script><p>算法迭代过程中会生成一系列状态迭代值 $\{V^{(0)},V^{(1)},V^{(2)},\cdots\}$ ，其中 $R^{(0)}\in R^n$ 是 $V_\pi$ 的一个初始猜测，需要证明的是</p><script type="math/tex;mode=display">\mathbf{V}^{(k)}\rightarrow \mathbf{V}_\pi=(\mathbf{I}-\gamma\mathbf{P}_{\pi})^{-1}\mathbf{R}_{\pi}</script><p>证明：</p><p>定义状态价值误差 $\delta^{(k)}=V^{(k)}-V_{\pi}$ ，我们的证明转为 $\delta^{(k)}\rightarrow 0$</p><ul><li>$V^{(k+1)}=\delta^{(k+1)}+V_{\pi}$</li><li>$V^{(k+1)}=R_{\pi}+\gamma P_{\pi}V^{(k)}$<ul><li>$V^{(k)}=\delta^{(k)}+V_{\pi}$</li></ul></li></ul><script type="math/tex;mode=display">\begin{aligned}
\delta^{(k+1)}+V_{\pi}&=R_{\pi}+\gamma P_{\pi}\left(\delta^{(k)}+V_{\pi}\right)\\
\delta^{(k+1)}&=-V_{\pi}+R_{\pi}+\gamma P_{\pi}\left(\delta^{(k)}+V_{\pi}\right)\\
&=\gamma P_{\pi}\delta^{(k)}-V_{\pi}+R_{\pi}+\gamma P_{\pi}V_{\pi}\\
&=\gamma P_{\pi}\delta^{(k)}\\
&=\gamma^2 P^2_{\pi}\delta^{(k-1)}\cdots=\gamma^{k+1} P^{k+1}_{\pi}\delta^{(0)}
\end{aligned}</script><p>对于每轮迭代的状态转移矩阵 $P_{\pi}^{(k)}$ 的每个元素都是非负的并且小于1。且 $0\le \gamma \le 1\Rightarrow \gamma ^{k+1}\rightarrow 0$ ，因此，$\delta^{(k+1)}=\gamma^{k+1} P^{k+1}_{\pi}\delta^{(0)} \xrightarrow{k\rightarrow \infty} 0$</p><p>故自举法求解贝尔曼期望方程的数值解是收敛于解析解</p><h4 id="数值解计算"><a href="#数值解计算" class="headerlink" title="数值解计算"></a>数值解计算</h4><div class="table-container"><table><thead><tr><th></th><th>预测(策略评估)问题</th><th>控制问题</th></tr></thead><tbody><tr><td>定义</td><td>已知马尔科夫决策过程以及要采取的策略 $\pi$ ，计算最大价值函数 $V_{\pi}(s)$ 的过程就是策略评估，或(价值)预测</td><td>已知马尔科夫决策过程，在所有可能的策略中寻找一个最优的价值函数和最佳策略</td></tr><tr><td>输入</td><td>马尔科夫决策过程 $<s ,a,p,r,\gamma>$ ；策略 $\pi$</s></td><td>马尔科夫决策过程 $<s ,a,p,r,\gamma>$</s></td></tr><tr><td>输出</td><td>价值函数 $V_{\pi}$</td><td>最佳价值函数 $V^{<em>}$ 和最佳策略 $\pi^</em>$</td></tr></tbody></table></div><p>在马尔科夫决策过程中，预测和控制二者是递进关系，通过解决预测问题进而解决控制问题。预测问题和控制问题都可以通过动态规划方法解决</p><h5 id="MDP与动态规划"><a href="#MDP与动态规划" class="headerlink" title="MDP与动态规划"></a>MDP与动态规划</h5><p><strong>动态规划(dynamic programming, DP)</strong> 适合解决具有 <strong>最优子结构(optimal substructure)</strong> 和 <strong>重叠子问题(overlapping subproblem)</strong> 两个性质的问题</p><ul><li>最优子结构：问题可由拆分成许多小问题，组合这些小问题的答案，能得到原问题的答案</li><li>重叠子问题：子问题出现多次，且子问题的解决方案能被重复使用</li></ul><p>可以将贝尔曼期望方程分解为递归结构，即 $\eqref{BellmanExpectation_itQ}$ 与 $\eqref{BellmanExpectation_itV}$ 。未来状态的价值函数作为子问题的解与当前状态的价值函数是直接相关的。通过备份图，子问题的价值函数可以被存储并重用。</p><p>动态规划应用于马尔科夫决策过程的 <strong>规划</strong> 问题，即环境是已知的，必须知道状态转移概率和对应的奖励</p><h3 id="2-3-3-DP解决预测问题"><a href="#2-3-3-DP解决预测问题" class="headerlink" title="2.3.3 DP解决预测问题"></a>2.3.3 DP解决预测问题</h3><p><strong>同步迭代</strong> ：将贝尔曼期望备份(Bellman expection backup) 变为迭代过程，反复迭代直到收敛</p><ul><li>每次迭代都会完全更新所有的状态</li><li><strong>异步迭代</strong> ：通过某种方式，使得每次迭代不需要更新所有状态</li></ul><p>若 $\pi$ 为随机策略，当前状态下采取一个随机动作进入下一个状态，则由 $\eqref{BellmanExpectation_itV}$ 不断迭代，最后价值函数会收敛</p><script type="math/tex;mode=display">V_\pi^{(k)}(s)=\sum\limits_{a\in A}\pi(a\vert s)\left(R(s,a)+\gamma \sum\limits_{s'\in S}P(s'\vert s,a)\cdot V_\pi^{(k-1)}(s')\right)</script><p>若 $\pi$ 为确定性策略，只要处于当前状态就采取一个确定动作从而进入下一状态，则贝尔曼决策过程变为一个贝尔曼奖励过程，可对 $\eqref{BellmanEquation}$ 迭代，最后价值函数会收敛</p><script type="math/tex;mode=display">V_\pi^{(k)}(s)=R(s,a)+\gamma P_\pi(s'\vert s)\cdot V_\pi^{(k-1)}(s')</script><p><strong>网格世界例子</strong></p><p><img src="/posts/2520454135/image-20240122203239729.png" alt="image-20240122203239729"></p><p>基于当前策略 $\pi$ 迭代一次后，正奖励涂绿一次，负奖励涂红一次</p><p><img src="/posts/2520454135/image-20240122203341762.png" alt="image-20240122203341762"></p><p>再次迭代，第一次策略评估的状态的周围状态有值，相当于周围状态可以转移到已知状态，所以策略评估每迭代一次，相当于状态价值备份图向上备份一次，周围状态的状态价值可以通过已知状态的状态价值得到</p><p>当多次迭代后，各个状态的奖励值都稳定下来，最后值会确定不变，收敛后每个状态的值就是它的状态价值</p><h3 id="2-3-4-DP解决控制问题"><a href="#2-3-4-DP解决控制问题" class="headerlink" title="2.3.4 DP解决控制问题"></a>2.3.4 DP解决控制问题</h3><p>若只是已知马尔科夫决策过程，解决控制问题相当于寻找最佳策略，从而得到最佳价值函数</p><h4 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h4><h5 id="策略对比"><a href="#策略对比" class="headerlink" title="策略对比"></a>策略对比</h5><p>对于两个策略 $\pi_1$ 和 $\pi_2$ ，计算在两个策略下每个状态的状态价值，若</p><script type="math/tex;mode=display">V_{\pi_1}(s)\ge V_{\pi_2}(s),s\in \mathcal{S}</script><p>则称策略 $\pi_1$ 优于 $\pi_2$</p><h5 id="最佳状态价值"><a href="#最佳状态价值" class="headerlink" title="最佳状态价值"></a>最佳状态价值</h5><p>最佳价值函数定义为</p><script type="math/tex;mode=display">V^*(s)=\max\limits_{\pi}V_{\pi}(s)</script><h5 id="最优策略-1"><a href="#最优策略-1" class="headerlink" title="最优策略"></a>最优策略</h5><p>最优策略指策略 $\pi^<em>$ ，能让该策略下所有状态的价值达到最佳状态价值，即 $V_{\pi^</em>}(s)\ge V_{\pi}(s),s\in \mathcal{S}$ 。则称 $\pi^*$ 为最佳策略</p><h4 id="策略改进定理与贝尔曼最优方程推导"><a href="#策略改进定理与贝尔曼最优方程推导" class="headerlink" title="策略改进定理与贝尔曼最优方程推导"></a>策略改进定理与贝尔曼最优方程推导</h4><h5 id="引例——求解策略相当于求解每个Q值的概率"><a href="#引例——求解策略相当于求解每个Q值的概率" class="headerlink" title="引例——求解策略相当于求解每个Q值的概率"></a>引例——求解策略相当于求解每个Q值的概率</h5><p>假设在状态 $s$ 下有三个动作 $a_1,a_2,a_2$ ，其相应的 $Q$ 值为 $q_1,q_2,q_3\in \R$ ，我们试图寻找其权重 $c_1^<em>,c_2^</em>,c_3^*$ 使得 $c_1q_1+c_2q_2+c_3q_3$ 最大化，即</p><script type="math/tex;mode=display">\max\limits_{c_1,c_2,c_3}c_1q_1+c_2q_2+c_3q_3,且c_1+c_2+c_3=1,c_1,c_2,c_3\ge 0</script><p>假设 $q_3\ge q_1,q_2$ ，对于最优策略，应该让 $c_3^<em>=1,c_1^</em>,c_2^*=0$</p><script type="math/tex;mode=display">q_3=(c_1^*+c_2^*+c_3^*)q_3\ge c_1^*q_1+c_2^*q_2+c_3^*q_3</script><p>受上例启发，$\sum\limits_{a\in \mathcal{A}}\pi(a\vert s)q(s,a)=\max\limits_{a\in \mathcal{A}}q(s,a)$</p><p>在最优策略下</p><script type="math/tex;mode=display">\pi^*(a\vert s)=\begin{cases}
1&a=\mathop{\mathrm{argmax}}\limits_{a\in\mathcal{A}}q(s,a)\\
0&a\neq \mathop{\mathrm{argmax}}\limits_{a\in\mathcal{A}}q(s,a)
\end{cases}</script><h5 id="贝尔曼最优方程推导"><a href="#贝尔曼最优方程推导" class="headerlink" title="贝尔曼最优方程推导"></a>贝尔曼最优方程推导</h5><p>若采取确定性策略 $a=\pi(s)$ ，策略改进公式为</p><script type="math/tex;mode=display">\pi'(a\vert s)=\mathop{\mathrm{argmax}}_aQ_{\pi}(s,a)</script><blockquote><p>当采取 $\mathop{\mathrm{argmax}}$ 操作时，会得到单调递增的 $V$ 与 $Q$ ：对 $\forall s$ ，若 $Q_{\pi}(s,\pi’(s))\ge V_\pi(s)$ ，则策略 $\pi’$ 一定优于 $\pi$</p><ul><li>相当于证明：从给定状态 $s$ 由 $\pi’$ 决定动作优于由 $\pi$ 决定动作</li></ul></blockquote><ul><li><p>确定性策略：当前策略下最优动作的 $Q$ 值大于非最优动作的 $Q$ 值，且由于 $\pi’$ 是一种确定性策略，当前状态下最优动作 $a$ 的概率 $\pi(a)=1$ ，故 $V_\pi(s)=\sum\limits_{a\in A}\pi(a\vert s)Q_{\pi}(s,a)\iff Q_{\pi}(s,a)$</p><script type="math/tex;mode=display">Q_{\pi}(s,\pi'(s))=\max\limits_{a\in \mathcal{A}}Q_{\pi}(s,a)\ge Q_{\pi}(s,\pi(s))=V_{\pi}(s)</script></li><li><p>若 $\pi$ 是随机策略，则 $Q_{\pi}(s,\pi(s))$ 未必等于 $V_{\pi}(s)$ ，只有在 $Q_{\pi}(s,\pi’(s))\ge V_\pi(s)$ 的前提下，才有非递增性</p></li></ul><script type="math/tex;mode=display">\begin{aligned}
V_{\pi}(s)&\le Q_{\pi}(s,\pi'(s))\xlongequal{\eqref{BellmanExpectation_Q}}\sum\limits_{a}\pi(a\vert s)E\left[r_{t+1}+\gamma V_{\pi}(s_{t+1})\big\vert s_t=s,a_t=\pi'(s)\right]\\
&=E_{\pi'}\left[r_{t+1}+\gamma V_{\pi}(s_{t+1})\big\vert s_t=s\right]\\
&\le E_{\pi'}\left[r_{t+1}+\gamma Q_{\pi}(s_{t+1},\pi'(s_{t+1}))\big\vert s_t=s\right]\\
&=E_{\pi'}\left[r_{t+1}+\gamma E_{\pi'}[r_{t+2}+\gamma V_{\pi}(s_{t+3})\vert s_{t+1}]\big\vert s_t=s\right]\\
&=E_{\pi'}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 V_{\pi}(s_{t+2})\big\vert s_t=s\right]\\
&\le E_{\pi'}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 Q_{\pi}(s_{t+2},\pi'(s_{t+2}))\big\vert s_t=s\right]\\
&\le \cdots\\
&\le E_{\pi'}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+1}+\cdots\big\vert s_t=s\right]=V_{\pi'}(s)
\end{aligned}</script><ul><li>其中，$E_{\pi’}\left[r_{t+1}+\gamma V_{\pi}(s_{t+1})\big\vert s_t=s\right]$ 表示在状态 $s$ 时采用策略 $\pi’$ 生成的动作，未来后继状态仍采用策略 $\pi$ 生成的动作。</li></ul><p>不断迭代，直至收敛后，有</p><script type="math/tex;mode=display">Q_{\pi}(s,\pi'(s))=\max\limits_{a\in \mathcal{A}}Q_{\pi}(s,a)\xlongequal{满足迭代终止条件}Q_{\pi}(s,\pi(s))=V_\pi(s)</script><p>因此，可得贝尔曼最优方程</p><script type="math/tex;mode=display">V_{\pi}(s)=\max\limits_{a\in \mathcal{A}}Q_{\pi}(s,a)</script><p>即最佳策略下的一个状态价值必须等于这个状态下采取最好动作获得的回报期望，此时的状态价值为最佳状态价值</p><p>而对于随机策略，贝尔曼最优方程为</p><script type="math/tex;mode=display">\begin{align}
V_{\pi}(s)&=\max\limits_{\pi}\sum\limits_{a\in \mathcal{A}}\pi(a\vert s)Q_{\pi}(s,a)\tag{BellmanOptimalityEquation}\label{BellmanOptimalityEquation}\\
&=\max\limits_{\pi}\sum\limits_{a\in \mathcal{A}}\pi(a\vert s)\left(\sum\limits_{r'}P(r'\vert s,a)r'+\gamma \sum\limits_{s’}V_{\pi}(s’)P(s’\vert s,a)\right)
\end{align}</script><h5 id="关注几个问题"><a href="#关注几个问题" class="headerlink" title="关注几个问题"></a>关注几个问题</h5><ul><li>如何求解贝尔曼最优方程</li><li>最优策略的存在性</li><li>最优策略的唯一性</li><li>贝尔曼最优方程的解是否具有最优性</li><li>最优策略是确定性策略还是随机性策略</li></ul><h4 id="贝尔曼最优方程求解"><a href="#贝尔曼最优方程求解" class="headerlink" title="贝尔曼最优方程求解"></a>贝尔曼最优方程求解</h4><p><img src="/posts/2520454135/image-20240206103605935.png" alt="image-20240206103605935"></p><p>对于贝尔曼最优方程，已知条件为</p><ul><li>MDP的动态特性：$P(r’\vert s,a)$ 与 $P(s’\vert s,a)$</li><li>对于 $V(s’)$ 我们会设定随机初始值，相当于已知 $Q_{\pi}(s)$</li></ul><p>其向量形式表示为</p><script type="math/tex;mode=display">V=\max\limits_{\pi}(R_{\pi}+\gamma P_{\pi}V)</script><p>令 $f(V)=\max\limits_{\pi}(R_{\pi}+\gamma P_{\pi}V)$ ，则有 $V=f(V)$ ，其中 $f(V)$ 是 $N$ 维向量</p><script type="math/tex;mode=display">\vert f(V)\vert_{N}=\max\limits_{\pi}\sum\limits_{a}\pi(a\vert s)Q(s,a),s\in \mathcal{S}</script><h5 id="引理"><a href="#引理" class="headerlink" title="引理"></a>引理</h5><h6 id="收缩映射"><a href="#收缩映射" class="headerlink" title="收缩映射"></a>收缩映射</h6><p><img src="/posts/2520454135/image-20240215232529968.png" alt="image-20240215232529968"></p><p>若函数 $f$ 满足</p><script type="math/tex;mode=display">\Vert f(x_1)-f(x_2)\Vert\le \gamma \Vert x_1-x_2\Vert ,\gamma \in (0,1)</script><p>则称 $f$ 为 <strong>收缩映射(收缩函数)</strong></p><h6 id="收缩映射定理-不动点定理"><a href="#收缩映射定理-不动点定理" class="headerlink" title="收缩映射定理(不动点定理)"></a>收缩映射定理(不动点定理)</h6><p>不动点：对于随机变量 $x\in X$ ，若满足 $x=f(x)$ ，则称其为函数 $f:X\mapsto X$ 的一个不动点</p><blockquote><p>收缩映射定理：用于求解 $x=f(x)$ 的解</p></blockquote><p>只要函数 $f$ 具有收缩映射性质，则一定存在一个不动点 $x^<em>$ 满足 $x^</em>=f(x^*)$</p><ul><li><p>唯一性：对于一个收缩映射，其不动点是唯一的(最优价值是唯一的)</p><p>存在多个策略可以获取最优价值</p></li><li><p>求解方法——价值迭代</p><p>随机指定初始值 $x^{(0)}$ ，通过收缩映射 $x^{(k+1)}=f(x^{(k)})$ 产生一个随机变量序列 $\{x^{(k)}\}$ ，则有 $x^{(k)}\xrightarrow{k\rightarrow \infty}x^*$ ，且呈指数级别收敛与不动点</p></li></ul><hr><p><strong>eg</strong> ：对于 $f(x)=0.5x$ ，求解其不动点</p><p>$f(x)=0.5x$ 是一个收缩映射</p><script type="math/tex;mode=display">\Vert0.5x_1-0.5x_2 \Vert= 0.5\Vert x_1-x_2\Vert\le \gamma \Vert x_1-x_2\Vert,\gamma \in (0.5,1]</script><ul><li>同理，对于 $x=f(x)=Ax$ 也是一个收缩映射，当 $x\in \R^N,A\in \R^{N\times N},\Vert A\Vert \le \gamma &lt;1$<script type="math/tex;mode=display">\Vert A x_1-Ax_2 \Vert= A\Vert x_1-x_2\Vert\le \gamma \Vert x_1-x_2\Vert,\gamma \in (\Vert A\Vert,1]</script></li></ul><p>相当于求解 $x=f(x)$ ，设定初始值 $x^{(0)}=10$ ，则有</p><script type="math/tex;mode=display">x^{(1)}=f(x^{(0)})=5\\
x^{(2)}=f(x^{(1)})=2.5\\
\vdots\\
x^{(k+1)}=f(x^{(k)})=0</script><p>则有不动点 $x^{(*)}=x^{(k+1)}=0$</p><h5 id="收缩映射定理应用于求解贝尔曼最优方程"><a href="#收缩映射定理应用于求解贝尔曼最优方程" class="headerlink" title="收缩映射定理应用于求解贝尔曼最优方程"></a>收缩映射定理应用于求解贝尔曼最优方程</h5></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/2520454135/" title="强化学习——蘑菇书">https://amostian.github.io/posts/2520454135/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 强化学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/4103897705/" rel="prev" title="Python数据分析"><i class="fa fa-chevron-left"></i> Python数据分析</a></div><div class="post-nav-item"><a href="/posts/2869733016/" rel="next" title="1-强化学习概念与原理">1-强化学习概念与原理 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-0-%E4%BB%8E%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%88%B0%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-text">2.0 从随机变量到马尔科夫过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-text">2.1 马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="nav-text">2.1.1 马尔可夫性质</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-2-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E8%A1%A8%E7%A4%BA"><span class="nav-text">2.1.2 马尔可夫过程的数学表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B"><span class="nav-text">2.2 马尔可夫奖励过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E5%9B%9E%E6%8A%A5%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">2.2.1 回报与价值函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%98%E6%89%A3%E5%9B%A0%E5%AD%90"><span class="nav-text">折扣因子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E5%9B%9E%E6%8A%A5%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">2.2.2 回报的计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-text">2.2.3 价值函数的计算方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="nav-text">蒙特卡洛方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼方程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC"><span class="nav-text">贝尔曼方程推导</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E7%9A%84%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-text">贝尔曼方程的解析解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E7%9A%84%E6%95%B0%E5%80%BC%E8%A7%A3"><span class="nav-text">贝尔曼方程的数值解</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95-1"><span class="nav-text">蒙特卡洛方法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95"><span class="nav-text">动态规划方法</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="nav-text">2.3 马尔科夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-MDP%E4%B8%8EMP-MRP"><span class="nav-text">2.3.1 MDP与MP&#x2F;MRP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MDP%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5"><span class="nav-text">MDP中的策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MDP%E4%B8%8EMRP"><span class="nav-text">MDP与MRP</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-MDP%E4%B8%AD%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">2.3.2 MDP中的价值函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼期望方程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼动作价值期望方程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼状态价值期望方程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E8%87%AA%E4%B8%BE"><span class="nav-text">价值函数的自举</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E5%85%B3%E7%B3%BB"><span class="nav-text">备份图角度理解价值函数关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E4%B8%BE%E6%B3%95%E6%94%B6%E6%95%9B%E6%80%A7%E8%AF%81%E6%98%8E"><span class="nav-text">自举法收敛性证明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E8%A7%A3%E8%AE%A1%E7%AE%97"><span class="nav-text">数值解计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MDP%E4%B8%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-text">MDP与动态规划</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-DP%E8%A7%A3%E5%86%B3%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="nav-text">2.3.3 DP解决预测问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-4-DP%E8%A7%A3%E5%86%B3%E6%8E%A7%E5%88%B6%E9%97%AE%E9%A2%98"><span class="nav-text">2.3.4 DP解决控制问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="nav-text">最优策略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E5%AF%B9%E6%AF%94"><span class="nav-text">策略对比</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%80%E4%BD%B3%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC"><span class="nav-text">最佳状态价值</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5-1"><span class="nav-text">最优策略</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B%E5%AE%9A%E7%90%86%E4%B8%8E%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC"><span class="nav-text">策略改进定理与贝尔曼最优方程推导</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%95%E4%BE%8B%E2%80%94%E2%80%94%E6%B1%82%E8%A7%A3%E7%AD%96%E7%95%A5%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%B1%82%E8%A7%A3%E6%AF%8F%E4%B8%AAQ%E5%80%BC%E7%9A%84%E6%A6%82%E7%8E%87"><span class="nav-text">引例——求解策略相当于求解每个Q值的概率</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC"><span class="nav-text">贝尔曼最优方程推导</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B3%E6%B3%A8%E5%87%A0%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-text">关注几个问题</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3"><span class="nav-text">贝尔曼最优方程求解</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%95%E7%90%86"><span class="nav-text">引理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%94%B6%E7%BC%A9%E6%98%A0%E5%B0%84"><span class="nav-text">收缩映射</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%94%B6%E7%BC%A9%E6%98%A0%E5%B0%84%E5%AE%9A%E7%90%86-%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86"><span class="nav-text">收缩映射定理(不动点定理)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%94%B6%E7%BC%A9%E6%98%A0%E5%B0%84%E5%AE%9A%E7%90%86%E5%BA%94%E7%94%A8%E4%BA%8E%E6%B1%82%E8%A7%A3%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="nav-text">收缩映射定理应用于求解贝尔曼最优方程</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">353</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">58</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">74</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">764.7k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">31:56</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>