<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png"><link rel="mask-icon" href="/images/favicon.png" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:300,300italic,400,400italic,700,700italic|Ma Shan Zheng:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"amostian.github.io",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="7.动手学深度学习-经典卷积神经网络"><meta property="og:url" content="https://amostian.github.io/posts/4253629123/index.html"><meta property="og:site_name" content="AmosTian"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240330175554561.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240330194941385.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240330151233579.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240330202136900.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240330225922114.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240330234600999.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331000938099.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331144347799.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331152307402.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331145231539.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331153128199.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331155920324.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331154715983.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331160706650.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331161110606.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331163922847.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331170409945.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331165405669.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331164216489.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331170512949.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331202929340.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331203215983.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331210413291.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331223128652.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331224010325.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331225531372.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331230238721.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331231040939.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331231120533.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331232159877.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331232319028.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331232836926.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331232930787.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331232959962.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331233104452.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240331233130434.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401100919244.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401154354097.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401154419890.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401155004712.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401155051159.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401161444295.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401162024808.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401162338849.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401163431142.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401165926633.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231009161217700.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011111557855.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011114229248.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011114301347.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011132216648.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231009224313247.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231009224144027.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011232208696.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011232517395.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011233513277.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011233732731.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011233906046.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011233937327.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20231011234125570.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240429204451089.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240429204522779.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240429204550122.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401205303191.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401211122394.png"><meta property="og:image" content="https://amostian.github.io/posts/4253629123/image-20240401210101340.png"><meta property="article:published_time" content="2024-03-31T03:11:55.000Z"><meta property="article:modified_time" content="2024-04-01T13:38:23.000Z"><meta property="article:author" content="AmosTian"><meta property="article:tag" content="AI"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://amostian.github.io/posts/4253629123/image-20240330175554561.png"><link rel="canonical" href="https://amostian.github.io/posts/4253629123/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>7.动手学深度学习-经典卷积神经网络 | AmosTian</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">AmosTian</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">68</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">84</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">236</span></a></li><li class="menu-item menu-item-essay"><a href="/categories/%E9%9A%8F%E7%AC%94/" rel="section"><i class="fa fa-fw fa-pied-piper"></i>随笔</a></li><li class="menu-item menu-item-dynamic-resume"><a href="/dynamic-resume/" rel="section"><i class="fa fa-fw fa-cog"></i>动态简历</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a href="https://github.com/AmosTian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://amostian.github.io/posts/4253629123/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="AmosTian"><meta itemprop="description" content="知道的越多，不知道的越多"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="AmosTian"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">7.动手学深度学习-经典卷积神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间 2024-03-31 11:11:55" itemprop="dateCreated datePublished" datetime="2024-03-31T11:11:55+08:00">2024-03-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间 2024-04-01 21:38:23" itemprop="dateModified" datetime="2024-04-01T21:38:23+08:00">2024-04-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-symbolscount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数 </span><span title="本文字数">13k字 </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>28 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><span id="more"></span><h2 id="7-1-LeNet"><a href="#7-1-LeNet" class="headerlink" title="7.1 LeNet"></a>7.1 LeNet</h2><p>LeNet是早期成功的神经网络，先用卷积层来学习图片空间信息，再用全连接层来转换到类别空间</p><p>LeNet的目标是识别手写数字，取得了与SVM性能相媲美的效果，使得神经网络称为监督学习的主流方法。</p><p>总体来看，LeNet-5由2个部分组成</p><ul><li><strong>卷积编码器</strong>：由两个卷积层组成</li><li><strong>全连接层密集块</strong>：由三个全连接层组成</li></ul><p><img src="/posts/4253629123/image-20240330175554561.png" alt="image-20240330175554561"></p><p>每个卷积块的基本单元</p><ul><li>卷积层</li><li>激活函数 <em>sigmoid</em> 函数</li><li>平均池化层</li></ul><p>每个卷积层使用 $5\times 5$ 的卷积核和一个 <em>sigmoid</em> 函数。卷积层将输入映射为多个输出通道的二维特征，通常会同时增加通道数。</p><p>每个 $2\times 2$ 的池化层会通过空间下采样将维数减少2倍。</p><p>为了将卷积块的局部特征传递给全连接层，必须在小批量中展平每个样本。即将一个四维特征映射转换为一个行张量。一个批量的四维特征映射转换为二维张量：第一维为样本数，第二维为每个样本的平面向量表示</p><ul><li>LeNet有三个全连接层，分别有120、84、10个输出</li></ul><p><img src="/posts/4253629123/image-20240330194941385.png" alt="image-20240330194941385"></p><p>卷积层的每个输出通道相当于识别到图像的一个模式，随着网络层数的堆叠，输出通道数会增加，即识别到的模式越来越多，同时每个卷积层的尺寸会减小</p><p>全连接层会将这些卷积块识别出的局部模式平滑地压缩</p><h3 id="7-1-1-实现"><a href="#7-1-1-实现" class="headerlink" title="7.1.1 实现"></a>7.1.1 实现</h3><p>创建网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对输入进行变形，防止检测目标在图像边缘，防止卷积核刚开始移动目标就消失</span></span><br><span class="line"><span class="comment"># 填充后，目标可以适当向中心调整</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 批量数不变，通道数变为1，将32*32的图片变为28*28</span></span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn._Module(c_i,c_o,padding=p)，</span></span><br><span class="line"><span class="comment">#	第一个参数是输入通道数</span></span><br><span class="line"><span class="comment">#	第二个参数是输出通道数，</span></span><br><span class="line"><span class="comment">#	填充2p个</span></span><br><span class="line"><span class="comment"># 卷积层结束是4维的数据，将其展平为1*.的向量，使用3个全连接层</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试网络输出形状是否正确</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="comment"># 由于net是nn.Sequential类实例，所以每一层可以被迭代</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#	Reshape output shape: 	 torch.Size([1, 1, 28, 28])</span></span><br><span class="line"><span class="comment">#	Conv2d output shape: 	 torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="comment">#	Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])</span></span><br><span class="line"><span class="comment">#	AvgPool2d output shape: 	 torch.Size([1, 6, 14, 14])</span></span><br><span class="line"><span class="comment">#	Conv2d output shape: 	 torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="comment">#	Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])</span></span><br><span class="line"><span class="comment">#	AvgPool2d output shape: 	 torch.Size([1, 16, 5, 5])</span></span><br><span class="line"><span class="comment">#	Flatten output shape: 	 torch.Size([1, 400])</span></span><br><span class="line"><span class="comment">#	Linear output shape: 	 torch.Size([1, 120])</span></span><br><span class="line"><span class="comment">#	Sigmoid output shape: 	 torch.Size([1, 120])</span></span><br><span class="line"><span class="comment">#	Linear output shape: 	 torch.Size([1, 84])</span></span><br><span class="line"><span class="comment">#	Sigmoid output shape: 	 torch.Size([1, 84])</span></span><br><span class="line"><span class="comment">#	Linear output shape: 	 torch.Size([1, 10])</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第一层输出，高宽减半，但通道数增加，相当于信息增多</span></span><br></pre></td></tr></table></figure><p>数据加载，超参数定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>虽然卷积神经网络的参数较全连接网络明显减少，但计算成本依然较高，因为 <strong>每个参数都参与更多的乘法</strong> ，通常使用GPU加快计算</p><p>在验证集上评估模型，由于完整验证集存在内存中，所以在验证集进行正向和反向传播之前，需要将数据逐批量从内存移动到网络所在的GPU显存中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            <span class="comment"># 将第一个网络的参数拿出来，取这个网络参数所在的设备进行运算</span></span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment"># 有多条数据，则逐一移动到目标设备</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="comment"># 若是全连接层或卷积层，用xavier初始化参数</span></span><br><span class="line">        <span class="comment">#     根据输入输出大小，使得输入和输出的方差差不多，减缓梯度爆炸或梯度消失</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>可以将卷积神经网络视作受限的全连接神经网络，LeNet过拟合现象明显小于MLP</p><ul><li><p>MLP结果</p><p><img src="/posts/4253629123/image-20240330151233579.png" alt="image-20240330151233579"></p></li><li><p>LeNet-5结果</p><p><img src="/posts/4253629123/image-20240330202136900.png" alt="image-20240330202136900"></p></li></ul><h2 id="7-2-AlexNet"><a href="#7-2-AlexNet" class="headerlink" title="7.2 AlexNet"></a>7.2 AlexNet</h2><h3 id="7-2-1-机器学习发展"><a href="#7-2-1-机器学习发展" class="headerlink" title="7.2.1 机器学习发展"></a>7.2.1 机器学习发展</h3><p>传统机器学习的步骤：</p><ol><li>获取数据集，早期数据的收集依赖传感器，最大的图像也就1M像素</li><li>特征预处理：根据光学、几何学、其他学科先验知识及偶然发现，手工对特征数据集进行预处理</li><li>通过特征提取算法或其他手动调整的特征提取方式来输入数据得到特征</li><li>将提取的特征代入机器学习算法</li></ol><hr><p><strong>传统的机器学习算法</strong>，会从理论去证明模型的各种性质</p><p>2000年左右，最流行的机器学习算法是核方法</p><p>一种统计机器学习方法：SVM</p><ul><li><p>提取特征</p></li><li><p>选择核函数计算相关性</p><ul><li>线性模型：内积</li><li>非线性关系：核方法，将空间变为线性空间；再用线性模型的方法解决问题</li></ul></li><li><p>解决凸优化问题</p><p>优化问题是机器学习最热门方向</p></li></ul><p>SVM好处：有完整的泛函定理，能计算模型复杂度</p><hr><p><strong>深度学习之前的计算机视觉领域</strong>，数据特征的作用很大。更大、更干净的数据集，稍微改进的特征提取，对模型精度影响远大于学习算法的进步</p><p>图像特征都是被计算出来的，一套新的特征函数、改进结果，会有崭新的结果，如：</p><ul><li>特征描述子：图像特征抽取模型SIFT（尺度不变特征变换）、SURF（加速鲁棒特征）、HOG（定向梯度直方图）</li><li>视觉词袋（聚类）</li></ul><p>2000年计算机视觉关注点在几何学</p><ul><li><p>抽取特征</p></li><li><p>将计算机视觉描述为几何问题</p><p>如：通过相机位置，描述一个3D模型</p><p>相当于对物理世界进行一个建模，如果实际问题符合某个几何模型，则可以很好的解决问题</p></li><li><p>(非)凸优化</p><p>有完整的几何定理</p></li></ul><hr><p><strong>目前，主流的深度学习在于 表征学习</strong> ，即特征本身就应该被学习到，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。</p><p>在视觉中，底层网络可能检测边缘、颜色和纹理等底层图像特征，类似于传统滤波器的特征抽取器；更高层，建立在底层局部特征上，以表示更大的特征；更高层检测整个物体。最终的隐藏层可以学习图像的综合表示</p><p>深度卷积神经网络的突破归因于数据和硬件两个因素。</p><ul><li>数据：包含许多特征的深度模型需要有大量的标签数据，才能显著优与基于传统凸优化的方法。2010年左右的大数据浪潮，使得数据集得以丰富。其中 ImageNet 起到了不小的作用</li><li>硬件：深度学习每轮迭代都需要昂贵的线性代数层传递数据。GPU的可优化高吞吐量的矩阵和向量乘法</li></ul><p>计算机视觉子领域，如：地球科学、气象预测，做了很多物理假设。深度学习比物理假设模型效果好，因为有充分跨学科先验知识，深度学习可以更好的建模</p><h3 id="7-2-2-神经网络与硬件关系"><a href="#7-2-2-神经网络与硬件关系" class="headerlink" title="7.2.2 神经网络与硬件关系"></a>7.2.2 神经网络与硬件关系</h3><p>CPU：每个CPU核都拥有高时钟频率的运行能力和三级Cache缓存，非常适合执行各种指令，具有分支预测、指令流水和程序运行的功能。但通用核心的制造成本高，需要大量的芯片面积、复杂的支持结构（内存结构、内核间的缓存逻辑、高速互联等），因此对于单一的计算任务性价比不高</p><p>GPU：由100-1000个小的处理单元组成，通常被分为更大的组。每个GPU核相对较弱（时钟频率低），但庞大的核心数量使得GPU的计算速度比CPU快几个数量级。</p><ul><li>原因：功耗随时钟频率呈二次方增长。对于一个CPU核心，假设运行速度比GPU快4倍，功耗是单个GPU核的16倍，但同样的功耗下用16个GPU核代替，GPU综合性能是CPU的4倍。另外GPU核更简单。</li></ul><p><img src="/posts/4253629123/image-20240330225922114.png" alt="image-20240330225922114"></p><p>在LeNet网络提出年代，数据规模是10000个，内存10MB，CPU每秒1千万次浮点数运算</p><p>随着GPU的发展，计算能力的增速大于数据规模增速</p><ul><li><p>在90年代，数据与算力是均衡的，相较于传统机器学习，神经网络是一种廉价的模型，虽然计算量比较大，但使用随机梯度下降，对内存的需求量也不大</p></li><li><p>2000年，数据量不大，拟合方法比较合适：</p><p>简单；有理论依据；硬件运算能力能覆盖模型的计算量（适量数据规模下核矩阵可被计算）</p></li><li><p>2010年后，数据规模增长速度小于计算能力增长速度，所以可以构造更深的神经网络，进一步挖掘数据中的信息</p></li></ul><p>数据规模在深度神经网络的复兴中起到很大作用</p><h3 id="7-2-3-AlexNet"><a href="#7-2-3-AlexNet" class="headerlink" title="7.2.3 AlexNet"></a>7.2.3 AlexNet</h3><p>ImageNet数据集是一个物体分类数据集</p><p><img src="/posts/4253629123/image-20240330234600999.png" alt="image-20240330234600999"></p><p>对计算机视觉领域贡献是方法论的改变</p><p><img src="/posts/4253629123/image-20240331000938099.png" alt="image-20240331000938099"></p><ul><li>之前，重点是特征工程，机器学习专家把对问题的理解转换成标准机器学习算法能理解的数值，即专家通过特征提取结合领域先验知识将机器学习算法所需的特征从问题中提取出来</li><li>AlexNet：在输出层是一个 <em>softmax</em> ，之前所有的卷积块都是对特征的编码器，实际上也是对特征的提取。实现端到端的学习：从原始输入数据直接转换为目标输出格式</li></ul><h4 id="AlexNet架构"><a href="#AlexNet架构" class="headerlink" title="AlexNet架构"></a>AlexNet架构</h4><h5 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h5><p>相对于 <em>LeNet-5</em> , AlexNet也是由卷积编码器和全连接密集层组成</p><ul><li><strong>卷积编码器</strong>：由五个卷积层组成</li><li><strong>全连接层密集块</strong>：由三个全连接层组成，两个隐藏层、一个输出层</li></ul><p><img src="/posts/4253629123/image-20240331144347799.png" alt="image-20240331144347799"></p><ul><li>$3\times 3$ 池化层容忍一个像素的向左和向右偏移</li></ul><p>AlexNet 的输入是3通道的 $224\times 224$ 的图像，所以需要更大的卷积核来捕获目标</p><p>第一卷积层输出为96通道的 $\frac{224-11+4}{4}\times \frac{224-11+4}{4}=96\times 54\times 54$</p><p>$3\times 3$ ，步幅为2的池化层后，输出为96通道的 $\frac{54-3+2}{2}=26\times26$ 的输出</p><p><img src="/posts/4253629123/image-20240331152307402.png" alt="image-20240331152307402"></p><p>将大数据降维到多通道的 $26\times 26$ 后，后续处理参考 <em>LeNet-5</em> ，采用 $5\times 5$ 的卷积核，边缘各填充两个像素，通道数变多，$256\times \frac{26+4-5+1}{1}=256\times 26\times 26$ 。最大池化层后，变为 $256\times \frac{26+2-1}{2}=256\times 12\times 12$</p><p>由于通道数远多于 <em>LeNet-5</em> 的16通道，后续还接了3个卷积层，卷积核为 $3\times 3$ ，填充为 $2$ ，所以数据尺寸不变，通道数变为256</p><p>最后一个最大池化层，将数据尺寸变为 $\frac{12+2-3}{2}=5\times 5$ ，这与 <em>LeNet-5</em> 全连接密集层输入数据尺寸相同，通道数变多</p><p><img src="/posts/4253629123/image-20240331145231539.png" alt="image-20240331145231539"></p><p>一张图片，展平后有 $256\times 5\times 5=6400$ 个参数，通过3个全连接密集层逐渐降为 $1000$ 个输出类别</p><h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p><em>AlexNet</em> 将 <em>sigmoid</em> 函数替换为更简单的 $ReLU$ 函数，</p><ul><li><p>ReLU正向计算简单，sigmoid函数需要复杂的求幂运算</p></li><li><p>ReLU在 0 附近的梯度为1，使用不同的参数初始化方法，ReLU都可以使模型有效训练</p><p>sigmoid若模型参数没有正确初始化，在0附近的梯度为0，模型无法有效训练</p></li></ul><h5 id="模型容量与数据预处理"><a href="#模型容量与数据预处理" class="headerlink" title="模型容量与数据预处理"></a>模型容量与数据预处理</h5><p>AlexNet通过dropout法控制全连接层的模型复杂度，LeNet只使用了权重衰减</p><p>数据增强增大了样本容量</p><ul><li>随机截取、翻转</li><li>随机亮度</li><li>随机色温</li></ul><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>AlexNet实质上就是更深更大的LeNet，主要改进</p><ul><li>dropout：模型参数更多、网络更深，所以需要对参数做一些正则</li><li>ReLU：相对于 <em>sigmoid</em> 梯度更大</li><li>最大池化：使得每个卷积块的输出值大，梯度相对比较大</li></ul><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>网络定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>测试网络输出形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)    </span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240331153128199.png" alt="image-20240331153128199"></p><p>读取数据：将AlexNet直接应用于Fashion-MNIST的问题是输入数据尺寸问题，Fashion-MNIST图像的分辨率是 $28 \times 28$像素，所以通过 <code>resize=224</code> 将图像尺寸拉伸到224像素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure><p>模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.01</span>, <span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240331155920324.png" alt="image-20240331155920324"></p><p>分析：AlexNet的模型容量远大于Fashion-MNIST的数据量，随着训练epoch的增加，后期会出现过拟合</p><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>AlexNet第一个卷积层有 $96\times 3\times 11\times 11+96=34944\approx 35K$ 个参数</p><ul><li>LeNet第一个卷积层有 $6\times 1\times 5\times5=150$ 个参数</li></ul><p>第二个卷积层 $256\times 96\times 5\times 5+256=614656\approx614K$ 个参数</p><p><img src="/posts/4253629123/image-20240331154715983.png" alt="image-20240331154715983"></p><p>运算量：</p><p>AlexNet第一层，输出每个元素都参与一次卷积，相当于每计算一个输出元素，需要在3个输入通道上进行一次卷积，即 $3\times 11\times 11$ 次乘法。输出 $96\times54\times 54$ ，所以有 $96\times 54\times 54\times 3\times 11\times 11=101,616,768\approx101M$ 次乘法</p><p>…</p><p>可见AlexNet是更大更深的LeNet，参数数量增加了近10倍，但计算量增加了近260倍</p><h2 id="7-3-VGG"><a href="#7-3-VGG" class="headerlink" title="7.3 VGG"></a>7.3 VGG</h2><p>一个经典的神经网络是一个序列</p><ol><li>带填充以保持分辨率的卷积层</li><li>非线性激活函数</li><li>池化层</li></ol><blockquote><p>AlexNet比LeNet更深更大来得到更高的精度</p></blockquote><h3 id="7-3-1-VGG块"><a href="#7-3-1-VGG块" class="headerlink" title="7.3.1 VGG块"></a>7.3.1 VGG块</h3><p>如何做到更深更大获取更高精度</p><ul><li>更多的全连接层（参数数量很多，很贵）</li><li>更多卷积层如何拼接</li><li>将卷积层组合成块：使用可重复的卷积块来构建深度卷积神经网络</li></ul><p>一个VGG块也由一系列卷积层组成和用于空间下采样的池化层组成</p><p><img src="/posts/4253629123/image-20240331160706650.png" alt="image-20240331160706650"></p><p>VGG发现：用更小的卷积核窗口堆更深更窄的网络比稍大的窗口堆效果好</p><ul><li><p>$3\times 3$ 卷积核的卷积层堆一个VGG块，VGG块可以堆更深的网络</p></li><li><p>$5\times 5$ 卷积核的卷积层堆一个VGG块获得的网络层数更浅，</p></li></ul><h3 id="7-3-2-VGG架构"><a href="#7-3-2-VGG架构" class="headerlink" title="7.3.2 VGG架构"></a>7.3.2 VGG架构</h3><p>多个VGG块后，接全连接密集层</p><p><img src="/posts/4253629123/image-20240331161110606.png" alt="image-20240331161110606"></p><h3 id="7-3-3-实现"><a href="#7-3-3-实现" class="headerlink" title="7.3.3 实现"></a>7.3.3 实现</h3><p><strong>VGG块实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_convs：一个块中卷积层个数</span></span><br><span class="line"><span class="comment"># in_channels：输入通道数</span></span><br><span class="line"><span class="comment"># out_channels：输出通道数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;conv:&quot;</span>,in_channels,<span class="string">&quot;,&quot;</span>,out_channels)</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p>上述的VGG块的中间卷积层，输入通道数与输出通道数相等</p><p>第一个块，有一个卷积层：1个输入通道，64个输出通道</p><ul><li>尺寸：$\frac{224-3+1+2}{1}=224\times 224$ ；$2\times 2$ 最大池化，步长为2，尺寸减半 $112\times 112$</li></ul><p>第二个块，有一个卷积层：64个输入通道，128个输出通道</p><ul><li>尺寸：$\frac{112-3+1+2}{1}=112\times 112$ ；最大池化，尺寸减半 $56\times 56$</li></ul><p>第三个块，有2个卷积层：(128, 256)，(256, 256)</p><ul><li>卷积层，尺寸不变；最大池化，尺寸减半</li></ul><p>第四个块，有2个卷积层：(256, 512)，(512, 512)</p><p>第五个块，有2个卷积层：(512, 512)，(512, 512)</p><p><strong>网络定义</strong></p><blockquote><p>5个块，是因为 $\frac{224}{2^5}=7$ ，所以只能堆5块</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i,(num_convs, out_channels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(conv_arch):</span><br><span class="line">        <span class="built_in">print</span>(i,<span class="string">&quot;:&quot;</span>,in_channels,<span class="string">&quot;,&quot;</span>,out_channels)</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        <span class="comment">#   一张224*224的图片经过5个块后变为7*7</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240331163922847.png" alt="image-20240331163922847"></p><p>相当于有8个卷积层和3个全连接密集层，共11层网络</p><p><strong>训练</strong></p><p>由于VGG-11比AlexNet都深，计算量更大，所以构建更小的网络足以训练 Fashion-MNIST数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">4</span></span><br><span class="line"><span class="comment"># 一个块通道数减少了16倍，输入、输出通道数分别变为1/4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240331170409945.png" alt="image-20240331170409945"></p><h3 id="7-3-4-VGG网络变种"><a href="#7-3-4-VGG网络变种" class="headerlink" title="7.3.4 VGG网络变种"></a>7.3.4 VGG网络变种</h3><p>不同的卷积块个数和卷积核规模可以得到不同复杂度的变种（神经网络有高配版、低配版）。对于VGG，目前用的最多的是VGG-16和VGG-19（医学）</p><p><img src="/posts/4253629123/image-20240331165405669.png" alt="image-20240331165405669"></p><ul><li>LRN（Local Response Normalisation）：局部响应归一化，之后不再使用</li></ul><p><img src="/posts/4253629123/image-20240331164216489.png" alt="image-20240331164216489"></p><p>神经网络层数越多，精确率越高，但代价是计算时间会变长</p><h3 id="7-3-5-VGG是CNN思路错误率最低的模型"><a href="#7-3-5-VGG是CNN思路错误率最低的模型" class="headerlink" title="7.3.5 VGG是CNN思路错误率最低的模型"></a>7.3.5 VGG是CNN思路错误率最低的模型</h3><p><img src="/posts/4253629123/image-20240331170512949.png" alt="image-20240331170512949"></p><p>之后GoogleNet和ResNet都是其他思路</p><h2 id="7-4-NiN"><a href="#7-4-NiN" class="headerlink" title="7.4 NiN"></a>7.4 NiN</h2><p>LetNet、AlexNet、VGG在卷积层之后会将输出展平为二维张量，最后进入全连接层。</p><p>虽然卷积层参数少 $c_o\times c_i\times k^2$ ，但卷积块之后的的第一个全连接层参数很多</p><ul><li>LeNet：$16\times 5\times 5\times 120=48K$</li><li>AlexNet：$256\times 5\times 5\times 4096=26M$</li><li>VGG：$512\times 7\times 7\times 4096=102M$</li></ul><p>参数过多占用很多内存资源，也会占用计算带宽。最重要的是容易发生过拟合</p><p>NiN的思路是在每个像素的所有通道上使用全连接层</p><h3 id="7-4-1-NiN块"><a href="#7-4-1-NiN块" class="headerlink" title="7.4.1 NiN块"></a>7.4.1 NiN块</h3><p><img src="/posts/4253629123/image-20240331202929340.png" alt="image-20240331202929340"></p><p>NiN块不改变通道数，只做同一像素位置的空间结构信息叠加</p><p>即 $1\times 1$ 卷积的输入通道与输出通道都等于块内卷积层的输出通道</p><p>同时，为了避免网络塌陷，在每个卷积层（包括 $1\times 1$ ）后加 <em>ReLU</em> 函数</p><h3 id="7-4-2-网络设计"><a href="#7-4-2-网络设计" class="headerlink" title="7.4.2 网络设计"></a>7.4.2 网络设计</h3><p><img src="/posts/4253629123/image-20240331203215983.png" alt="image-20240331203215983"></p><p>NiN的优势就是显著减少了模型所需要的参数量</p><p>第一个块：</p><ul><li>第一个卷积层：输入通道1，输出通道96，核 $11\times 11$ ，共 $96\times 1\times 11\times 11=11,616$ 个参数</li><li>$1\times 1$ 卷积层有输出通道 $96$ ，输入通道 $96$ ，所以共 $2(96\times 96\times 1\times 1)=18,432$</li><li>共：$11,616+18,432=30,048\approx30K$</li></ul><p>第二个块：</p><ul><li>第一个卷积层： $256\times 96\times 5\times 5=614,400$</li><li>$1\times 1$ 卷积层：$2(256\times 256)=131,072$</li><li>共：$745,472\approx745K$</li></ul><p>第三个块：</p><ul><li>第一个卷积层：$384\times 256\times 3\times 3=8,861,184$</li><li>$1\times 1$ 卷积层：$2(384\times 384)=294,912$</li><li>共：$9,156,096\approx 9M$</li></ul><p>第四个块：</p><ul><li>第一个卷积层：$10\times 384\times 3\times 3=34,560$</li><li>$1\times 1$ 卷积层：$2(10\times 10)=200$</li><li>共：$34,760\approx34K$ 个参数</li></ul><h3 id="7-4-3-实现"><a href="#7-4-3-实现" class="headerlink" title="7.4.3 实现"></a>7.4.3 实现</h3><p><strong>NiN块定义</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure><p><strong>网络实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#	Sequential output shape:	 torch.Size([1, 96, 54, 54])</span></span><br><span class="line"><span class="comment">#	MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])</span></span><br><span class="line"><span class="comment">#	Sequential output shape:	 torch.Size([1, 256, 26, 26])</span></span><br><span class="line"><span class="comment">#	MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])</span></span><br><span class="line"><span class="comment">#	Sequential output shape:	 torch.Size([1, 384, 12, 12])</span></span><br><span class="line"><span class="comment">#	MaxPool2d output shape:	 torch.Size([1, 384, 5, 5])</span></span><br><span class="line"><span class="comment">#	Dropout output shape:	 torch.Size([1, 384, 5, 5])</span></span><br><span class="line"><span class="comment">#	Sequential output shape:	 torch.Size([1, 10, 5, 5])</span></span><br><span class="line"><span class="comment">#	AdaptiveAvgPool2d output shape:	 torch.Size([1, 10, 1, 1])</span></span><br><span class="line"><span class="comment">#	Flatten output shape:	 torch.Size([1, 10])</span></span><br></pre></td></tr></table></figure><p>全局平均汇聚层（<code>AdaptiveAvgPool2d</code>），生成对数几率</p><ul><li>实现二维平均池化，输出形状 <code>(1,1)</code> ，计算这个 $5\times 5$ 的输出的平均值作为这个数据的预测输出</li></ul><p><strong>训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240331210413291.png" alt="image-20240331210413291"></p><h2 id="7-5-GoogleNet"><a href="#7-5-GoogleNet" class="headerlink" title="7.5 GoogleNet"></a>7.5 GoogleNet</h2><p><img src="/posts/4253629123/image-20240331223128652.png" alt="image-20240331223128652"></p><p>LetNet-5是第一个卷积神经网络</p><p>AlexNet是更深更大的卷积神经网络，并发现 $3\times 3$ 组成更深更窄的网络学习效果比 $5\times 5$ 的卷积核更好</p><p>VGG为规范引入 VGG 块，并随着卷积块个数和卷积核规模的不同可以得到不同复杂度的变种</p><p>NiN首次使用 $1\times 1$ 的卷积核去掉全连接层</p><h3 id="7-5-1-Inception块"><a href="#7-5-1-Inception块" class="headerlink" title="7.5.1 Inception块"></a>7.5.1 Inception块</h3><p><img src="/posts/4253629123/image-20240331224010325.png" alt="image-20240331224010325"></p><p><strong>4个路径从不同的角度抽取信息，然后在输出通道上进行合并</strong></p><p><strong>白色</strong></p><ul><li>$1\times 1$ 卷积，变换通道数，控制模型复杂度</li></ul><p><strong>蓝色</strong></p><ul><li>$1\times 1$ 卷积，抽取通道信息</li><li><p>$3\times 3$ / $5\times 5$ 卷积，抽取数据空间信息</p></li><li><p>MaxPooling，抽取空间信息并使抽取结果更加鲁棒，容许像素平移</p></li></ul><p><img src="/posts/4253629123/image-20240331225531372.png" alt="image-20240331225531372"></p><blockquote><p>一个Inception块通道变化：192-&gt;256</p></blockquote><p>大致设计思路：</p><p>$3\times 3$ 的卷积相较 $5\times 5$ 的卷积能更好抽取数据中的空间信息，所以占数据输出通道的一半比重</p><p>$1\times 1$ 的卷积核能抽取通道信息，所以占 $\frac{1}{4}$ 的通道</p><p>剩余的 MaxPooling 和 $5\times 5$ 的卷积核各占剩余一半通道</p><h4 id="Inception块实现"><a href="#Inception块实现" class="headerlink" title="Inception块实现"></a>Inception块实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="Inception好处"><a href="#Inception好处" class="headerlink" title="Inception好处"></a>Inception好处</h4><p>增加特征提取的多样性，参数变少，计算量变低</p><p><img src="/posts/4253629123/image-20240331230238721.png" alt="image-20240331230238721"></p><p>若将输入通道为192的数据变为输出通道为256的数据</p><ul><li>$3\times 3$ 的卷积核：需要 $256\times 192\times 3\times 3=442,368\approx44M$ 个参数</li><li>$5\times 5$ 的卷积核：需要 $256\times 192\times 5\times 5=1,228,800\approx 1.22M$ 个参数</li></ul><p>计算量：</p><ul><li>$3\times 3$ 的卷积核：一个输出元素需要 $192\times 3\times 3$ 次乘法，共有 $256\times 28\times 28$ 个元素，即 $346,816,512\approx346M$ 次乘法</li><li>$5\times 5$ 的卷积核：一个输出元素需要 $192\times 5\times 5$ 次乘法，共 $256\times 28\times 28$ 个输出元素，即 $963,379,200\approx 963M$</li></ul><h3 id="7-5-2-网络架构"><a href="#7-5-2-网络架构" class="headerlink" title="7.5.2 网络架构"></a>7.5.2 网络架构</h3><h4 id="整体"><a href="#整体" class="headerlink" title="整体"></a>整体</h4><p><img src="/posts/4253629123/image-20240331231040939.png" alt="image-20240331231040939"></p><h4 id="段1-amp-段2"><a href="#段1-amp-段2" class="headerlink" title="段1&amp;段2"></a>段1&amp;段2</h4><p>快速将数据尺寸降低，通道数增加。使得后续运算可控</p><p><img src="/posts/4253629123/image-20240331231120533.png" alt="image-20240331231120533"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>将输入变为 $64\times 56\times 56$ 的输出</p><ul><li>卷积层：$\frac{224-7+2+2\times 3}{2}=112$</li><li>最大池化层：$\frac{112-3+2+2}{2}=56$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>将输入 $64\times 56\times 56$ 变为 $192\times 28\times 28$ 的输出</p><ul><li>$1\times1$ 卷积层：从64个角度合并同一像素位置不同通道上的信息，输出 $64$ 个通道</li><li>$3\times 3$ 卷积层：$\frac{28-3+2+2}{2}=28$ ，不改变数据尺寸，通道数变为192</li><li>池化层：尺寸减半，不改变通道数</li></ul><h4 id="段3"><a href="#段3" class="headerlink" title="段3"></a>段3</h4><blockquote><p>中间段要识别更多的局部特征</p><p>所以第一个Inception块用小核保留较多的原始信息，细化局部特征</p><p>第二个块尽可能在一个通道组合更多的局部特征，所以 $5\times 5$ 占比增加</p></blockquote><p><img src="/posts/4253629123/image-20240331232159877.png" alt="image-20240331232159877"></p><p>第三个段串联两个完整的Inception块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h4 id="段4-amp-5与输出层"><a href="#段4-amp-5与输出层" class="headerlink" title="段4&amp;5与输出层"></a>段4&amp;5与输出层</h4><p><img src="/posts/4253629123/image-20240331232319028.png" alt="image-20240331232319028"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>通道数和Inception能的卷积块划分都挺没道理的，只能说有效</p><p>输出层</p><ol><li><p>全局平均汇聚层：将每个输出通道的数据取平均变为1维均值</p></li><li><p>将接全连接层，变为分类目标数</p></li></ol><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><h3 id="7-5-3-变种"><a href="#7-5-3-变种" class="headerlink" title="7.5.3 变种"></a>7.5.3 变种</h3><p><img src="/posts/4253629123/image-20240331232836926.png" alt="image-20240331232836926"></p><h4 id="Inception-V3"><a href="#Inception-V3" class="headerlink" title="Inception V3"></a>Inception V3</h4><blockquote><p>卷积块堆叠也挺没道理的</p></blockquote><p><img src="/posts/4253629123/image-20240331232930787.png" alt="image-20240331232930787"></p><ul><li>将 $5\times 5$ 变为 两个 $3\times 3$</li></ul><p><img src="/posts/4253629123/image-20240331232959962.png" alt="image-20240331232959962"></p><ul><li>将 $3\times 3$ 变为 $1\times7\rightarrow 7\times 1$</li><li>将 $5\times 5$ 变为 $1\times7\rightarrow 7\times 1\rightarrow 1\times7\rightarrow 7\times 1$</li></ul><p><img src="/posts/4253629123/image-20240331233104452.png" alt="image-20240331233104452"></p><ul><li>。。。</li></ul><p><img src="/posts/4253629123/image-20240331233130434.png" alt="image-20240331233130434"></p><p>V3分类准确率高，但占用内存大，耗费时间算中等</p><h2 id="7-6-批量归一化"><a href="#7-6-批量归一化" class="headerlink" title="7.6 批量归一化"></a>7.6 批量归一化</h2><p><img src="/posts/4253629123/image-20240401100919244.png" alt="image-20240401100919244"></p><p>在神经网络中，损失是反向传播的。</p><p>顶部梯度大，收敛速度快；底部梯度值小，收敛速度慢，但底部学习的是底层局部特征，底部值变化会导致顶部值发生相应变化，导致顶部的层需要重新学习多次，从而导致收敛变慢。</p><p>Batch-Normalization解决的问题：学习底部层时，避免顶部层发生变化</p><h3 id="7-6-1-原理"><a href="#7-6-1-原理" class="headerlink" title="7.6.1 原理"></a>7.6.1 原理</h3><p>$\mathbf{x}\in \mathcal{B}$ 来自小批量 $\mathcal{B}$ 的输入，批量归一化第 $j$ 个特征</p><script type="math/tex;mode=display">BN(\mathbf{x}^{(j)}_i)=\gamma_{j}\frac{\mathbf{x}^{(j)}_i-\hat{\mu}_{\mathcal{B}}^{(j)}}{\hat{\sigma}_{\mathcal{B}}^{(j)}}+\beta_j</script><ul><li>其中，$\gamma_j$ ，$\beta_j$ 为第 $j$ 个特征的方差和均值，是待学习的超参数</li></ul><p>固定小批量的均值和方差</p><script type="math/tex;mode=display">\hat{\mu}_{\mathcal{B}}^{(j)}=\frac{1}{\vert \mathcal{B}\vert}\sum_{i=1}^{\vert \mathcal{B}\vert}\mathbf{x}_{i}^{(j)}\\
\left(\hat{\sigma}_{\mathcal{B}}^{(j)}\right)^2=\frac{1}{\vert \mathcal{B}\vert}\sum_{i=1}^{\vert \mathcal{B}\vert}\left[\mathbf{x}_{i}^{(j)}-\hat{\mu}_{\mathcal{B}}^{(j)}\right]^2+\varepsilon</script><ul><li>方差添加 <code>eps</code> ，防止归一化过程中出现除零</li></ul><h4 id="Xavier与批量归一化区别"><a href="#Xavier与批量归一化区别" class="headerlink" title="Xavier与批量归一化区别"></a>Xavier与批量归一化区别</h4><p>Xavier作用是使各层的输入输出的均值和方差基本一致，也可理解为各层的输出均值方差基本一致，手段是通过调整随机初始化参数的分布，作用于训练的开始阶段，第一次反向传播后失效</p><p>批量归一化可以理解为对浅层变化量小的参数值进行补偿，使得中间层的参数值变化不是很剧烈，从而加快收敛。作用于整个训练过程中，每个层每次正向传播的输出都会被归一化</p><h3 id="7-6-2-批量归一化层"><a href="#7-6-2-批量归一化层" class="headerlink" title="7.6.2 批量归一化层"></a>7.6.2 批量归一化层</h3><h4 id="位置"><a href="#位置" class="headerlink" title="位置"></a>位置</h4><p>全连接层和卷积层输出之后，激活函数之前（批量归一化实质上是一个线性变换）</p><ul><li><p>全连接层：作用在特征维，相当于每个特征上归一化</p><p>均值、方差为当前批量的样本在该特征维下的均值、方差，</p></li><li><p>卷积层：作用在通道维，通道维相当于特征维</p><p>$1\times 1$ 的卷积核，作用于每个像素，相当于样本维是所有像素$(批量\times 高\times 宽)$，特征维是通道</p></li></ul><p><strong>输出维度与输入维度相同</strong></p><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><ul><li><p><strong>持续加速深层网络的收敛速度</strong>，但不会提升数据精度</p></li><li><p>尚未理论明确的理论：优化过程中的各种噪声源会加快训练和减少过拟合</p><ul><li>由于批量数据的均值与方差不同，所以相当于模拟数据的随机偏移和缩放对学习的影响</li></ul></li></ul><p>没必要与dropout混合使用</p><p>注意：只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的</p><ul><li>批量大小为1的批量应用批量归一化，减去均值后为0，学习不到</li></ul><h3 id="7-6-3-实现"><a href="#7-6-3-实现" class="headerlink" title="7.6.3 实现"></a>7.6.3 实现</h3><p><strong>定义BN操作</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># moving_mean/moving_var，全体数据的全局均值方差，非批量数据的均值方差</span></span><br><span class="line"><span class="comment"># eps：防止除0</span></span><br><span class="line"><span class="comment"># momentum：更新均值方差的超参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="comment"># moving_mean和moving_var都是Parameter类的参数</span></span><br><span class="line">    <span class="comment">#	返回归一化后的批量输出和批量均值、方差值</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure><p><strong>BN层</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="comment"># num_features：全连接层的输出特征数或卷积层的输出通道数。</span></span><br><span class="line">    <span class="comment"># num_dims：2表示完全连接层，4表示卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 非模型参数的变量初始化为0和1</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var</span></span><br><span class="line">        <span class="comment"># 复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            <span class="comment"># 注意eps的区别，框架不同会变</span></span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><ul><li>对上一层的输出进行归一化，所以BN层的输出和输入数相等，<code>num_features</code> 为上一层输出通道数 / 特征数</li></ul><p><strong>网络定义</strong> ：使用批量归一化层的 <em>LeNet-5</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p><strong>训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><h3 id="7-6-4-Pytorch实现"><a href="#7-6-4-Pytorch实现" class="headerlink" title="7.6.4 Pytorch实现"></a>7.6.4 Pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">16</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">120</span>), nn.BatchNorm1d(<span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.BatchNorm1d(<span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><h2 id="7-7-残差网络ResNet"><a href="#7-7-残差网络ResNet" class="headerlink" title="7.7 残差网络ResNet"></a>7.7 残差网络ResNet</h2><p>对VGG（块）、GoogleNet（并行连结）改进，</p><p>ResNet对上一层网络的扩充而不是改变</p><h3 id="7-7-1-函数类"><a href="#7-7-1-函数类" class="headerlink" title="7.7.1 函数类"></a>7.7.1 函数类</h3><blockquote><p>随着网络假设，如何理解：新添加的层如何提升神经网络性能；或如何靠近真实的函数</p></blockquote><p>假设神经网络的所有函数服从一个架构 $\mathcal{F}$ ，用 $f$ 表示某个参数确定的神经网络，参数即 $\mathbf{W}$ 。而 $f^<em>$ 为数据集真实的函数，也就是 $f$ 要去拟合的函数。如果 $f^</em>$ 也属于 $\mathcal{F}$ ，则通过参数调整可以很容易找到，但不属于 $\mathcal{F}$ 时，就需要增大模型容量，使 $\mathcal{F}’$ 中能找到一个最接近 $f^<em>$ 的一个近似函数 $f^</em>_{\mathcal{F}’}$ 。</p><p>而加大模型容量，在深度学习中可以使加深网络层数，我们希望的是 $f^<em>_{\mathcal{F}’}$ 比 $f^</em>_{\mathcal{F}}$ 更接近 $f^*$ 。</p><p>但若 $\mathcal{F}\not\subseteq \mathcal{F}’$ ，则无法保证新的网络更接近，即更复杂的非嵌套函数类并不能是真的向 $f^*$ 靠近</p><p><img src="/posts/4253629123/image-20240401154354097.png" alt="image-20240401154354097"></p><p>只有当 $\mathcal{F}\subseteq \mathcal{F}’$ 时，即较复杂函数能包含较小的函数类时，才能保证提高性能。</p><p><img src="/posts/4253629123/image-20240401154419890.png" alt="image-20240401154419890"></p><p>若将新添加的层训练成恒等映射 $f(\mathbf{x})=\mathbf{x}$ ，则新模型和原模型同样有效。同时新模型可能训练出更优的解来拟合训练数据集。这也是何恺明等人提出的残差网络的思想。</p><h3 id="7-7-2-残差块"><a href="#7-7-2-残差块" class="headerlink" title="7.7.2 残差块"></a>7.7.2 残差块</h3><p><img src="/posts/4253629123/image-20240401155004712.png" alt="image-20240401155004712"></p><p>理想状态下，我们希望卷积块能拟合出理想的恒等映射。</p><p><img src="/posts/4253629123/image-20240401155051159.png" alt="image-20240401155051159"></p><p>但实际上，残差映射 $f(\mathbf{x})-\mathbf{x}$ 更容易优化，<strong>梯度反向传播时，乘法变加法</strong></p><p>同时，对于 $f(\mathbf{x})=\alpha\left[f(\mathbf{x})-\mathbf{x}\right]+\beta\mathbf{x}$ ，只需将 $\alpha=0$ ，则 $f(\mathbf{x})$ 成为恒等映射</p><p><strong>梯度乘变梯度加</strong></p><p>对于真实标签为 $y$ 的预测 $f(x)$ ，其损失函数 $\ell(y,f(x))$ ，损失函数的梯度 $\frac{\partial \ell\left(f(x)\right)}{\partial w}$ ，使用梯度下降法</p><script type="math/tex;mode=display">w=w-\eta\frac{\partial \ell\left(f(x)\right)}{\partial w}</script><p>在梯度反向传播中，我们不希望损失函数的梯度 $\frac{\partial \left(f(x)\right)}{\partial w}$ 变小</p><p>若采用更深的网络预测，即 $g\left(f(x)\right)$ ，其损失函数为 $\ell\left(y,g(f(x))\right)$</p><script type="math/tex;mode=display">\frac{\partial \ell\left(y,g(f(x))\right)}{\partial w}=\frac{\partial \ell\left(y,g(f(x))\right)}{\partial \ell\left(f(x)\right)}\frac{\partial \ell\left(f(x)\right)}{\partial w}</script><p>所以直接采用更深的神经网络会导致梯度变得更小。</p><p>而ResNet的做法 $f(x)+g(f(x))$ 的梯度为 $\frac{\partial \left(f(x)\right)}{\partial w}+\frac{\partial \ell\left(y,g(f(x))\right)}{\partial w}$ ，即使残差映射的梯度再小也没有关系。输入仍可以通过加法传导到下一层</p><p><strong>另一种角度理解</strong></p><p>结合泰勒公式，任意函数在 $x=0$ 处的泰勒展开为 $f(x)=f(0)+f’(0)x+\frac{f’’(0)x^2}{2!}+\cdots$</p><p>即 $f(x)=线性项+非线性项$ ，而 残差映射 学习的就是输入的非线性部分</p><h3 id="7-7-3-实现"><a href="#7-7-3-实现" class="headerlink" title="7.7.3 实现"></a>7.7.3 实现</h3><p>ResNet-34 / ResNet-50 用的多</p><h4 id="残差块设计"><a href="#残差块设计" class="headerlink" title="残差块设计"></a>残差块设计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            <span class="comment"># 注意步长，目的是保持数据尺寸相等</span></span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure><p>实现两种残差块</p><ul><li>经过 $3\times 3$ 的卷积核后，数据尺寸变为 $\frac{n+s-1}{s}$ ，所以 $1\times 1$ 的卷积核需要调整步长</li></ul><p><img src="/posts/4253629123/image-20240401161444295.png" alt="image-20240401161444295"></p><ul><li><p>第一种：输入和输出尺寸相等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br><span class="line"><span class="comment">#	torch.Size([4, 3, 6, 6])</span></span><br></pre></td></tr></table></figure></li><li><p>第二种：增加输出通道数，减少尺寸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通常，通道数会加倍，高宽减半</span></span><br><span class="line">blk = Residual(<span class="number">3</span>,<span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">blk(X).shape</span><br><span class="line"><span class="comment">#	torch.Size([4, 6, 3, 3])</span></span><br></pre></td></tr></table></figure></li></ul><p>残差块可以任意组合设置</p><p><img src="/posts/4253629123/image-20240401162024808.png" alt="image-20240401162024808"></p><h4 id="ResNet架构"><a href="#ResNet架构" class="headerlink" title="ResNet架构"></a>ResNet架构</h4><p>与GoogleNet相同，都是5个段</p><p><strong>段1</strong> 和VGG、GoogleNet一致，都是 $7\times 7$ 的卷积核</p><p><img src="/posts/4253629123/image-20240401162338849.png" alt="image-20240401162338849"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h5 id="ResNet段"><a href="#ResNet段" class="headerlink" title="ResNet段"></a>ResNet段</h5><p>一个ResNet段是多个残差块的组合</p><p>段内：第一个残差块增加通道，尺寸减半；第二个残差块通道数不变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个resnet段</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            <span class="comment"># 每个段第一个对数据高宽减半</span></span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 每个段非第一个块对特征组合</span></span><br><span class="line">            <span class="comment">#   特殊的是第一个段的第一个块无需对数据尺寸减半，因为前面是一个最大池化层，已经减半</span></span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><h5 id="段2"><a href="#段2" class="headerlink" title="段2"></a>段2</h5><p>由于之前已经使用了 $7\times7$ ，步幅为2的卷积层和步幅为2的最大汇聚层，所以第一个ResNet段无须减小高和宽</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h5 id="段3-amp-4-amp-5"><a href="#段3-amp-4-amp-5" class="headerlink" title="段3&amp;4&amp;5"></a>段3&amp;4&amp;5</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h5 id="全局平均池化与全连接输出层"><a href="#全局平均池化与全连接输出层" class="headerlink" title="全局平均池化与全连接输出层"></a>全局平均池化与全连接输出层</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240401163431142.png" alt="image-20240401163431142"></p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240401165926633.png" alt="image-20240401165926633"></p><p>速度不快，但用大网络拟合 Fashion-MNIST数据集会出现明显的过拟合</p><h2 id="7-8-DenseNet"><a href="#7-8-DenseNet" class="headerlink" title="7.8 DenseNet"></a>7.8 DenseNet</h2><p>DenseNet是ResNet的扩展</p><h2 id="7-9-非标准卷积"><a href="#7-9-非标准卷积" class="headerlink" title="7.9 非标准卷积"></a>7.9 非标准卷积</h2><h3 id="7-9-1-空洞卷积"><a href="#7-9-1-空洞卷积" class="headerlink" title="7.9.1 空洞卷积"></a>7.9.1 空洞卷积</h3><h4 id="如何增加输出单元的感受野"><a href="#如何增加输出单元的感受野" class="headerlink" title="如何增加输出单元的感受野"></a>如何增加输出单元的感受野</h4><blockquote><p>感受野和卷积长度（卷积核大小）直接相关</p></blockquote><ul><li><p>增加卷积核大小——增加参数数量</p></li><li><p>增加层数：两层 $3\times 3$ 近似一层 $5\times 5$ 卷积的效果</p><p>增加复杂度</p></li><li><p>卷积前进行汇聚——产生信息损失</p><p>汇聚层不但可以有效减少神经元的数量，还可以使网络对一些小的局部形态改变保持不变性，并拥有更大的感受野</p><p>第2层卷积层神经元感受野有K个神经元，第3层卷积层神经元感受野有$2K-1$ 个神经元，依次类推，随着卷积层数深度增加，神经元感受野也就越大，越容易提取到高级特征</p><p><img src="/posts/4253629123/image-20231009161217700.png" alt="image-20231009161217700"></p></li></ul><p><strong>空洞卷积</strong> 不增加参数数量，同时增加输出单元感受野——膨胀卷积</p><p>在卷积核每两个元素之间插入 $D-1$ 个空洞，卷积核的有效大小为</p><script type="math/tex;mode=display">K'=K+(K-1)\times (D-1)</script><ul><li>$D$ 为膨胀率，当 $D=1$ 时卷积核为普通的卷积核</li></ul><p>通过给卷积核插入空洞（改变步长）来增加感受野</p><p><img src="/posts/4253629123/image-20231011111557855.png" alt="image-20231011111557855"></p><h3 id="7-9-2-“反”卷积"><a href="#7-9-2-“反”卷积" class="headerlink" title="7.9.2 “反”卷积"></a>7.9.2 “反”卷积</h3><h4 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h4><blockquote><p>低维特征映射到高维特征映射——转置卷积，反卷积</p></blockquote><p>假设一个高维向量 $X\in \R^D$ 和一个低维向量 $Z\in \R^M$ ，且 $M&lt;D$ ，</p><script type="math/tex;mode=display">Z=WX,W\in \R^{M\times D}</script><p>若用仿射变换来实现高维到低维的映射</p><script type="math/tex;mode=display">X=W^TZ</script><p>两个映射形式上是转置关系</p><ul><li>在全连接网络中，忽略激活函数，前向计算和反向传播就是一种转置关系<script type="math/tex;mode=display">Z^{(l+1)}=W^{(l+1)}Z^{(l)}\\
\delta^{(l)}=(W^{(l+1)})^T\delta^{(l+1)}</script></li></ul><p>对于二维向量</p><p><img src="/posts/4253629123/image-20231011114229248.png" alt="image-20231011114229248"></p><ul><li>$C$ 为稀疏矩阵，其非零元素来自于卷积核 $W$ 中的元素</li></ul><p><img src="/posts/4253629123/image-20231011114301347.png" alt="image-20231011114301347"></p><hr><p>对于一个 $M$ 维的向量 $Z$ 和大小为 $K$ 的卷积核，如果希望通过卷积操作来映射到更高维的向量，需要对 $Z$ 两端进行补零 $P=K-1$ ，可以得到 $M+K-1$ 维向量</p><p>对于 $M\times N$ 维向量 $X$ ，通过 $U\times V$ 为卷积核 $W$ ，在 $M$ 边分别补 $U-1$ 个零，$N$ 边分别补 $V-1$ 个零，可以得到转置卷积 $(M+U-1)\times (N+V-1)$ 维输出</p><p><img src="/posts/4253629123/image-20231011132216648.png" alt="image-20231011132216648"></p><h4 id="微步卷积"><a href="#微步卷积" class="headerlink" title="微步卷积"></a>微步卷积</h4><p>步长变小——微步卷积</p><p>整体思路：通过增加卷积的步长 $S&gt;1$ ，实现对特征的下采样操作，大幅降低特征映射维数(神经元数量)</p><p>同样，减小步长 $S&lt;1$ ，能使特征映射的维数(神经元数量)增加</p><p><img src="/posts/4253629123/image-20231009224313247.png" alt="image-20231009224313247"></p><p>若希望转置卷积的步长变为 $\frac{1}{S}$ ，则需要在输入特征之间插入 $S-1$ 个零来使移动速度变慢</p><ul><li><p>以一维转置卷积为例，对一个 $M$ 维的向量 $Z$ 和大小为 $K$ 的卷积核，通过对向量 $Z$ 进行两端补零 $P=K-1$ ，并且在每两个向量元素之间插入 $D$ 个零，然后进行步长为 $1$ 的卷积，得到 $(D+1)\times (M-1)+K$ 维向量</p></li><li><p>对于 $M\times N$ 的输入向量 $X$ 和 $U\times V$ 的卷积核，对向量 $X$ 分别补 $U-1$ 与 $V-1$ 个零，并且在每两个元素之间插入 $D_1$ 与 $D_2$ 个零，得到 $[(D_1+1)\times (M-1)+U]\times [(D_2+1)\times (N-1)+V]$ 维的输出向量 $Z$</p></li></ul><p>若原先步长 $S=2$ ，若步长减少 $\frac{1}{2}$ 输出的特征映射会增加一倍</p><p><img src="/posts/4253629123/image-20231009224144027.png" alt="image-20231009224144027"></p><p>左图：输入 $X\in \R^{5\times 5}$ ，步长 $S=2$</p><p>右图：步长 $S=1$ ，填充零</p><ul><li>$[(1+1)\times (2-1)+3]\times[(1+1)\times (2-1)+3]=5\times 5$</li></ul><h2 id="7-10-应用"><a href="#7-10-应用" class="headerlink" title="7.10 应用"></a>7.10 应用</h2><h3 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h3><p>决策网络</p><p><img src="/posts/4253629123/image-20231011232208696.png" alt="image-20231011232208696"></p><h3 id="目标检测RCN"><a href="#目标检测RCN" class="headerlink" title="目标检测RCN"></a>目标检测RCN</h3><p>区域卷积网络</p><h3 id="图像分割RCNN"><a href="#图像分割RCNN" class="headerlink" title="图像分割RCNN"></a>图像分割RCNN</h3><p><img src="/posts/4253629123/image-20231011232517395.png" alt="image-20231011232517395"></p><ul><li>像素级图像分割</li></ul><h3 id="光学字符识别OCR"><a href="#光学字符识别OCR" class="headerlink" title="光学字符识别OCR"></a>光学字符识别OCR</h3><h3 id="应用于文本"><a href="#应用于文本" class="headerlink" title="应用于文本"></a>应用于文本</h3><h5 id="n-gram特征与卷积"><a href="#n-gram特征与卷积" class="headerlink" title="n-gram特征与卷积"></a>n-gram特征与卷积</h5><p><img src="/posts/4253629123/image-20231011233513277.png" alt="image-20231011233513277"></p><ul><li>一元特征</li><li>二元特征：单词顺序</li><li>三元特征</li></ul><p>可以理解为滑动窗口</p><h4 id="词到词向量"><a href="#词到词向量" class="headerlink" title="词到词向量"></a>词到词向量</h4><p>通过Lookup Table 将一个单词变为一个词向量</p><ul><li><img src="/posts/4253629123/image-20231011233732731.png" alt="image-20231011233732731"></li></ul><p><img src="/posts/4253629123/image-20231011233906046.png" alt="image-20231011233906046"></p><p><img src="/posts/4253629123/image-20231011233937327.png" alt="image-20231011233937327"></p><p><img src="/posts/4253629123/image-20231011234125570.png" alt="image-20231011234125570"></p><h2 id="模型微调"><a href="#模型微调" class="headerlink" title="模型微调"></a>模型微调</h2><p>数据集标注代价很高</p><p><img src="/posts/4253629123/image-20240429204451089.png" alt="image-20240429204451089"></p><p>Fashion-MNIST训练集大约有6万张图片10个类别、ImageNet大约有1000万张图片1000个类别。我们通常任务的数据集规模在二者之间。</p><p>若想完成一个识别汽车的程序，一般情况类别数在100种左右，我们会为每个类别的汽车从不同角度拍摄500张图片，然后在收集到的数据集上训练一个分类模型。</p><p>这个数据集规模远小于ImageNet，适合ImageNet的复杂模型可能在这个数据集上过拟合。此外，由于训练样本数量有限，只使用这个数据集也可能造成欠拟合。</p><p>上述问题的解决方案是 <strong>迁移学习</strong> ，将从 <em>源数据集</em> 学到的知识迁移到 <em>目标数据集</em> 。尽管ImageNet数据集中大多数图片都与汽车无关，但此数据集上训练到的模型可能提取到更通用的底层图像特征，结合这些边缘、纹理、形状和对象的组合，可能更有效地识别汽车。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>一个神经网络分为 <strong>特征抽取</strong> 和 <strong>分类器</strong> 两部分，特征抽取将原始输入变为更容易线性分割的特征，分类器根据每个类别各自特征的权重组合实现分类。即 <strong>神经网络</strong> 的作用是使特征提取变得可学习</p><p><img src="/posts/4253629123/image-20240429204522779.png" alt="image-20240429204522779"></p><p>认为模型参数包含从源数据集中学到的知识（底层特征的组合方式），这些知识同样适用于目标模型</p><p><img src="/posts/4253629123/image-20240429204550122.png" alt="image-20240429204550122"></p><p>当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力</p><p><strong>注意选择数据集适配的预训练模型</strong> ，如癌症图片识别最好使用在医学数据集上的预训练模型（同领域数据集）</p><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol><li><p>在源数据集上预训练那神经网络模型，即 <em>源模型</em></p></li><li><p>创建新的神经网络模型，即 <em>目标模型</em> 。这个目标模型将复制源模型上所有的模块及其参数。</p></li><li><p>向目标模型添加输出层，其输出数为目标数据集中的类别数，随机初始化输出层的参数</p></li><li><p>在目标数据集上训练目标模型。</p><p>输出层从头开始训练，所有其他层的参数根据源模型参数微调</p></li></ol><p><img src="/posts/4253629123/image-20240401205303191.png" alt="image-20240401205303191"></p><h3 id="微调的具体方法"><a href="#微调的具体方法" class="headerlink" title="微调的具体方法"></a>微调的具体方法</h3><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>若源数据集远复杂于目标数据集，则在目标数据集上使用更强的正则化进行训练</p><ul><li>使用更小的学习率：底层特征不需要多调整，主要学习接近输出层的顶层特征，所以底层学习率很小，顶层学习率会高一点。但微调的学习率远小于完整训练的学习率</li><li>使用更少的数据迭代，少迭几次ecoch，一般2、3轮即可</li></ul><h4 id="重用分类器权重"><a href="#重用分类器权重" class="headerlink" title="重用分类器权重"></a>重用分类器权重</h4><p>源数据集可能也有目标数据集中的部分标签值</p><p>使用预训练模型分类器中对应的权重初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取全连接层的权重</span></span><br><span class="line">weight = pretrained_net.fc.weight</span><br><span class="line"><span class="comment"># torch.split(tensor,split_size_or_sections,dim)</span></span><br><span class="line"><span class="comment">#     将tensor，沿dim指定的维度，以section指定的大小为一块</span></span><br><span class="line"><span class="comment"># 即输入到分类器中的512个线性可分割的特征中，全连接输出层的934行对应的权重组合为hotdog类的分类器</span></span><br><span class="line"><span class="comment"># 令finetune_net.fc.weight=hotdog_w即可</span></span><br><span class="line">hotdog_w = torch.split(weight.data, <span class="number">1</span>, dim=<span class="number">0</span>)[<span class="number">934</span>]</span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.in_features, <span class="number">2</span>)</span><br><span class="line">finetune_net.fc.weight=hotdog_w</span><br></pre></td></tr></table></figure><h4 id="固定一些层"><a href="#固定一些层" class="headerlink" title="固定一些层"></a>固定一些层</h4><p>高层特征与目标数据集相关；</p><p>底层特征更加通用：固定不优化或微调，模型复杂度变低</p><p><img src="/posts/4253629123/image-20240401211122394.png" alt="image-20240401211122394"></p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p>热狗识别：在小型数据集上微调ResNet，源模型在ImageNet上做了预训练。小数据集包含上千张含热狗和不含热狗的图像，将通过微调模型来识别图像中是否包含热狗</p><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;hotdog&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;hotdog.zip&#x27;</span>,</span><br><span class="line">                         <span class="string">&#x27;fba480ffa8aa7e0febbb511d181409f899b9baa5&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data_dir = d2l.download_extract(<span class="string">&#x27;hotdog&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图片读取至内存</span></span><br><span class="line">train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>))</span><br><span class="line">test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>))</span><br></pre></td></tr></table></figure><ul><li>共1400张 “正类” 和 “负类” 图片，其中1000张用于训练，其余用于测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hotdogs = [train_imgs[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">not_hotdogs = [train_imgs[-i - <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">d2l.show_images(hotdogs + not_hotdogs, <span class="number">2</span>, <span class="number">8</span>, scale=<span class="number">1.4</span>);</span><br></pre></td></tr></table></figure><p><img src="/posts/4253629123/image-20240401210101340.png" alt="image-20240401210101340"></p><p>发现图片尺寸不一致，而ResNet预训练模型的输入是 $224\times 224$ ，需要处理</p><p>使用数据增广，将图像的高度和宽度都缩放到 $256$ 像素，然后裁剪中间的 $224\times 224$ 区域作为输入。此外，对于RGB三个通道，分别标准化每个通道</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为原始模型在ImageNet数据集上进行了数据增广，所以微调之前也需要将数据增广</span></span><br><span class="line"><span class="comment">#   使用RGB通道的均值和标准差，以标准化每个通道</span></span><br><span class="line"><span class="comment">#   每个通道的均值方差，[R通道均值,G通道均值,B通道均值],[R通道均方差,G通道方差,B通道方差]</span></span><br><span class="line">normalize = torchvision.transforms.Normalize(</span><br><span class="line">    [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据格式与原始模型的数据集对齐</span></span><br><span class="line">train_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br><span class="line"></span><br><span class="line">test_augs = torchvision.transforms.Compose([</span><br><span class="line">    <span class="comment"># 短边变为256，长宽比不变</span></span><br><span class="line">    torchvision.transforms.Resize([<span class="number">256</span>, <span class="number">256</span>]),</span><br><span class="line">    <span class="comment"># 中心截取224*224</span></span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br></pre></td></tr></table></figure><h4 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将模型结构取回，并且将在ImageNet上预训练的参数也取回</span></span><br><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>在ResNet的全局平均汇聚层后，全连接层的输出转换为ImageNet数据集的1000个类别的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目标模型初始化，fc层前的都用预训练模型初始化</span></span><br><span class="line">finetune_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 根据需要修改输出类别，并随机初始化参数</span></span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.in_features, <span class="number">2</span>)</span><br><span class="line">nn.init.xavier_uniform_(finetune_net.fc.weight);</span><br></pre></td></tr></table></figure><p>我们构建一个新的神经网络作为目标模型。 它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）。</p><h4 id="模型微调-1"><a href="#模型微调-1" class="headerlink" title="模型微调"></a>模型微调</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_fine_tuning</span>(<span class="params">net, learning_rate, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span>,</span></span><br><span class="line"><span class="params">                      param_group=<span class="literal">True</span></span>):</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>), transform=train_augs),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>), transform=test_augs),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    devices = d2l.try_all_gpus()</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    <span class="comment"># 最后一层用比较大的学习率，输出层之前的层学习率保持不变</span></span><br><span class="line">    <span class="comment">#  因为前面预训练的层在大数据集上已经预训练好了，只需要微调参数，我们希望输出层能更快的收敛</span></span><br><span class="line">    <span class="keyword">if</span> param_group:</span><br><span class="line">        params_1x = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()</span><br><span class="line">             <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;fc.weight&quot;</span>, <span class="string">&quot;fc.bias&quot;</span>]]</span><br><span class="line">        trainer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: params_1x&#125;,</span><br><span class="line">                                   &#123;<span class="string">&#x27;params&#x27;</span>: net.fc.parameters(),</span><br><span class="line">                                    <span class="string">&#x27;lr&#x27;</span>: learning_rate * <span class="number">10</span>&#125;],</span><br><span class="line">                                lr=learning_rate, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,</span><br><span class="line">                                  weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,</span><br><span class="line">                   devices)</span><br><span class="line">    </span><br><span class="line">train_fine_tuning(finetune_net, <span class="number">5e-5</span>)</span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------<i class="fa fa-hand-peace-o"></i>本文结束-------------</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者 </strong>AmosTian</li><li class="post-copyright-link"><strong>本文链接 </strong><a href="https://amostian.github.io/posts/4253629123/" title="7.动手学深度学习-经典卷积神经网络">https://amostian.github.io/posts/4253629123/</a></li><li class="post-copyright-license"><strong>版权声明 </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AI/" rel="tag"><i class="fa fa-tags"></i> AI</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tags"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3273175593/" rel="prev" title="6.动手学深度学习-卷积神经网络"><i class="fa fa-chevron-left"></i> 6.动手学深度学习-卷积神经网络</a></div><div class="post-nav-item"><a href="/posts/1556323108/" rel="next" title="8.动手学深度学习-循环神经网络">8.动手学深度学习-循环神经网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-LeNet"><span class="nav-text">7.1 LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-1-%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.1.1 实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-2-AlexNet"><span class="nav-text">7.2 AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95"><span class="nav-text">7.2.1 机器学习发展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%A1%AC%E4%BB%B6%E5%85%B3%E7%B3%BB"><span class="nav-text">7.2.2 神经网络与硬件关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-3-AlexNet"><span class="nav-text">7.2.3 AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AlexNet%E6%9E%B6%E6%9E%84"><span class="nav-text">AlexNet架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="nav-text">模型设计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">模型容量与数据预处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-text">实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="nav-text">模型复杂度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-3-VGG"><span class="nav-text">7.3 VGG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-1-VGG%E5%9D%97"><span class="nav-text">7.3.1 VGG块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-2-VGG%E6%9E%B6%E6%9E%84"><span class="nav-text">7.3.2 VGG架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-3-%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.3.3 实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-4-VGG%E7%BD%91%E7%BB%9C%E5%8F%98%E7%A7%8D"><span class="nav-text">7.3.4 VGG网络变种</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-5-VGG%E6%98%AFCNN%E6%80%9D%E8%B7%AF%E9%94%99%E8%AF%AF%E7%8E%87%E6%9C%80%E4%BD%8E%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-text">7.3.5 VGG是CNN思路错误率最低的模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-4-NiN"><span class="nav-text">7.4 NiN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-1-NiN%E5%9D%97"><span class="nav-text">7.4.1 NiN块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-2-%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1"><span class="nav-text">7.4.2 网络设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-3-%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.4.3 实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-5-GoogleNet"><span class="nav-text">7.5 GoogleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-1-Inception%E5%9D%97"><span class="nav-text">7.5.1 Inception块</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inception%E5%9D%97%E5%AE%9E%E7%8E%B0"><span class="nav-text">Inception块实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inception%E5%A5%BD%E5%A4%84"><span class="nav-text">Inception好处</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-2-%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-text">7.5.2 网络架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E4%BD%93"><span class="nav-text">整体</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%B51-amp-%E6%AE%B52"><span class="nav-text">段1&amp;段2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%B53"><span class="nav-text">段3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%B54-amp-5%E4%B8%8E%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-text">段4&amp;5与输出层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-text">模型训练</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-3-%E5%8F%98%E7%A7%8D"><span class="nav-text">7.5.3 变种</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inception-V3"><span class="nav-text">Inception V3</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-6-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">7.6 批量归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-1-%E5%8E%9F%E7%90%86"><span class="nav-text">7.6.1 原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Xavier%E4%B8%8E%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%8C%BA%E5%88%AB"><span class="nav-text">Xavier与批量归一化区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-2-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="nav-text">7.6.2 批量归一化层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE"><span class="nav-text">位置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%9C%E7%94%A8"><span class="nav-text">作用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-3-%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.6.3 实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-4-Pytorch%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.6.4 Pytorch实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-7-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9CResNet"><span class="nav-text">7.7 残差网络ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-1-%E5%87%BD%E6%95%B0%E7%B1%BB"><span class="nav-text">7.7.1 函数类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-2-%E6%AE%8B%E5%B7%AE%E5%9D%97"><span class="nav-text">7.7.2 残差块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-3-%E5%AE%9E%E7%8E%B0"><span class="nav-text">7.7.3 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97%E8%AE%BE%E8%AE%A1"><span class="nav-text">残差块设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet%E6%9E%B6%E6%9E%84"><span class="nav-text">ResNet架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ResNet%E6%AE%B5"><span class="nav-text">ResNet段</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AE%B52"><span class="nav-text">段2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AE%B53-amp-4-amp-5"><span class="nav-text">段3&amp;4&amp;5</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E4%B8%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-text">全局平均池化与全连接输出层</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-text">训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-8-DenseNet"><span class="nav-text">7.8 DenseNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-9-%E9%9D%9E%E6%A0%87%E5%87%86%E5%8D%B7%E7%A7%AF"><span class="nav-text">7.9 非标准卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-9-1-%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF"><span class="nav-text">7.9.1 空洞卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%A2%9E%E5%8A%A0%E8%BE%93%E5%87%BA%E5%8D%95%E5%85%83%E7%9A%84%E6%84%9F%E5%8F%97%E9%87%8E"><span class="nav-text">如何增加输出单元的感受野</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-9-2-%E2%80%9C%E5%8F%8D%E2%80%9D%E5%8D%B7%E7%A7%AF"><span class="nav-text">7.9.2 “反”卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="nav-text">转置卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BE%AE%E6%AD%A5%E5%8D%B7%E7%A7%AF"><span class="nav-text">微步卷积</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-10-%E5%BA%94%E7%94%A8"><span class="nav-text">7.10 应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AlphaGo"><span class="nav-text">AlphaGo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BRCN"><span class="nav-text">目标检测RCN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2RCNN"><span class="nav-text">图像分割RCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%ABOCR"><span class="nav-text">光学字符识别OCR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC"><span class="nav-text">应用于文本</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#n-gram%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8D%B7%E7%A7%AF"><span class="nav-text">n-gram特征与卷积</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%8D%E5%88%B0%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-text">词到词向量</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83"><span class="nav-text">模型微调</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4"><span class="nav-text">步骤</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E7%9A%84%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95"><span class="nav-text">微调的具体方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%8D%E7%94%A8%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D"><span class="nav-text">重用分类器权重</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BA%E5%AE%9A%E4%B8%80%E4%BA%9B%E5%B1%82"><span class="nav-text">固定一些层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-1"><span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-1"><span class="nav-text">模型微调</span></a></li></ol></li></ol></li></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="AmosTian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">AmosTian</p><div class="site-description" itemprop="description">知道的越多，不知道的越多</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">236</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">68</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">84</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/AmosTian" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AmosTian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://blog.csdn.net/qq_40479037?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_40479037?type&#x3D;blog" rel="noopener" target="_blank"><i class="fa fa-fw fa-crosshairs"></i>CSDN</a> </span><span class="links-of-author-item"><a href="mailto:17636679561@163.com" title="E-Mail → mailto:17636679561@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("01/27/2022 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-grav"></i> </span><span class="author" itemprop="copyrightHolder">AmosTian</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数 </span><span title="站点总字数">1252.1k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">50:25</span></div></div></footer></div><script color="0,0,0" opacity="0.5" zindex="-1" count="150" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script async src="/js/cursor/fireworks.js"></script><script src="/js/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,model:{jsonPath:"live2d-widget-model-hijiki"},display:{position:"right",width:150,height:300},mobile:{show:!1},log:!1})</script></body></html>